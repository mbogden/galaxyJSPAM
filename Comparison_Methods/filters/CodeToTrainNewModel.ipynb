{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os, sys, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gooddir = './training_image_set_2/goodImgs'\n",
    "baddir = './training_image_set_2/badImgs'\n",
    "goodImgs = os.listdir(gooddir)\n",
    "badImgs = os.listdir(baddir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodImgs.sort()\n",
    "badImgs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "trainingGood = []\n",
    "testingGood = []\n",
    "trainingBad = []\n",
    "testingBad = []\n",
    "for f in goodImgs:\n",
    "    if i % 2 == 0:\n",
    "        trainingGood.append(f)\n",
    "    elif i % 2 != 0:\n",
    "        testingGood.append(f)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "trainingBad = []\n",
    "testingBad = []\n",
    "for f in badImgs:\n",
    "    if j % 2 == 0:\n",
    "        trainingBad.append(f)\n",
    "    elif j % 2 != 0:\n",
    "        testingBad.append(f)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_image(img_path):\n",
    "    #path = dirs+'/'+img_path\n",
    "    img = image.load_img(img_path, target_size=(128, 128), color_mode = \"grayscale\")\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingGoodArr = np.concatenate([grab_image('./training_image_set_2/goodImgs/'+f) for f in trainingGood])\n",
    "testingGoodArr = np.concatenate([grab_image('./training_image_set_2/goodImgs/'+f) for f in testingGood])\n",
    "trainingBadArr = np.concatenate([grab_image('./training_image_set_2/badImgs/'+f) for f in trainingBad])\n",
    "testingBadArr = np.concatenate([grab_image('./training_image_set_2/badImgs/'+f) for f in testingBad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = np.concatenate((trainingGoodArr, trainingBadArr))\n",
    "testing = np.concatenate((testingGoodArr, testingBadArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "goodCSVData = ['0']\n",
    "badCSVData = ['1']\n",
    "with open('labels2.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    for i in trainingGood: #goodtrain\n",
    "        writer.writerows(goodCSVData)\n",
    "    for i in trainingBad: #badtrain\n",
    "        writer.writerows(badCSVData)\n",
    "    for i in testingGood:\n",
    "        writer.writerows(goodCSVData)\n",
    "    for i in testingBad:\n",
    "        writer.writerows(badCSVData)\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "with open('labels2.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter='\\n')\n",
    "    for row in readCSV:\n",
    "        labels.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = keras.utils.to_categorical(labels)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "m = 0\n",
    "for f in trainingGood:\n",
    "    n += 1\n",
    "for f in trainingBad:\n",
    "    n += 1\n",
    "for f in testingGood:\n",
    "    m += 1\n",
    "for f in testingBad:\n",
    "    m += 1\n",
    "print(n)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 128, 128, 1)\n",
      "(136, 128, 128, 1)\n",
      "(137, 2)\n",
      "(136, 2)\n"
     ]
    }
   ],
   "source": [
    "train_labels = Y[ : n]\n",
    "test_labels = Y[n :]\n",
    "print(training.shape)\n",
    "print(testing.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /nfshome/apps/python-3.6.7/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 126, 126, 64)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 61, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 61, 61, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 59, 59, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 29, 29, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 53824)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 107650    \n",
      "=================================================================\n",
      "Total params: 182,146\n",
      "Trainable params: 182,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3,3),\n",
    "                              activation='relu',\n",
    "                              input_shape=[training.shape[1],\n",
    "                                           training.shape[2],\n",
    "                                           training.shape[3]]))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(128,128,1)))\n",
    "model.add(keras.layers.ELU())\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(128,128,1)))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr = 0.0001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 109 samples, validate on 28 samples\n",
      "Epoch 1/10\n",
      "109/109 [==============================] - 29s 265ms/step - loss: 0.0162 - acc: 1.0000 - val_loss: 1.7791 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "109/109 [==============================] - 10s 94ms/step - loss: 0.0118 - acc: 1.0000 - val_loss: 1.8469 - val_acc: 0.4643\n",
      "Epoch 3/10\n",
      "109/109 [==============================] - 7s 67ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 1.4226 - val_acc: 0.5714\n",
      "Epoch 4/10\n",
      "109/109 [==============================] - 8s 72ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 1.4496 - val_acc: 0.5714\n",
      "Epoch 5/10\n",
      "109/109 [==============================] - 6s 59ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 1.3583 - val_acc: 0.5714\n",
      "Epoch 6/10\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.2343 - val_acc: 0.6071\n",
      "Epoch 7/10\n",
      "109/109 [==============================] - 8s 71ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.2100 - val_acc: 0.6429\n",
      "Epoch 8/10\n",
      "109/109 [==============================] - 7s 68ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.1912 - val_acc: 0.6429\n",
      "Epoch 9/10\n",
      "109/109 [==============================] - 8s 75ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.1289 - val_acc: 0.6786\n",
      "Epoch 10/10\n",
      "109/109 [==============================] - 8s 73ms/step - loss: 9.6667e-04 - acc: 1.0000 - val_loss: 1.1032 - val_acc: 0.6786\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "epochs = 10\n",
    "history = model.fit(training, train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 5s 38ms/step\n",
      "Test loss: 0.4292726008331074\n",
      "Test accuracy: 0.9044117647058824\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testing, test_labels, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "#model_json = model.to_json()\n",
    "#with open(\"model3.json\", \"w\") as json_file:\n",
    "#    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "#model.save_weights(\"model3.h5\")\n",
    "#print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "#json_file = open('model3.json', 'r')\n",
    "#loaded_model_json = json_file.read()\n",
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#loaded_model.load_weights(\"model3.h5\")\n",
    "#print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "#loaded_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#              optimizer=keras.optimizers.Adam(),\n",
    "#              metrics=['accuracy'])\n",
    "#score = loaded_model.evaluate(data_test, data_test_labels, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = os.listdir('./training_image_set_2/goodImgs')\n",
    "#for f in a:\n",
    "#    i = grab_image('./training_image_set_2/goodImgs/'+f)\n",
    "#    print('File name:', f, ' Prediction:', model.predict(i))\n",
    "    \n",
    "#print('/n/n/n/n')\n",
    "\n",
    "#b = os.listdir('./training_image_set_2/badImgs')\n",
    "#for f in b:\n",
    "#    i = grab_image('./training_image_set_2/badImgs/'+f)\n",
    "#    print('File name:', f, ' Prediction:', model.predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
