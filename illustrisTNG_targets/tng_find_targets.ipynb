{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41903b7-c553-4b32-8059-c5040dbd0304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFile: tng-find-targets.py\\nAuthor: Matthew Ogden\\nEmail: ogdenm12@gmail.com\\nGithub: mbogden\\nCreated: 2023-Nov-09\\n\\nDescription: \\n    This code is designed to interact with the IllustrisTNG Simulation Data. \\n    It's goal is to identify close interactions/mergers between two galaxies,\\n    then saves catalog data relating to the encounter in a csv file.\\n\\nReferences:  \\n- [Add IllustrisTNG ref]\\n- Sections of this code were enhanced with the assistance of ChatGPT made by OpenAI.\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "File: tng-find-targets.py\n",
    "Author: Matthew Ogden\n",
    "Email: ogdenm12@gmail.com\n",
    "Github: mbogden\n",
    "Created: 2023-Nov-09\n",
    "\n",
    "Description: \n",
    "    This code is designed to interact with the IllustrisTNG Simulation Data. \n",
    "    It's goal is to identify close interactions/mergers between two galaxies,\n",
    "    then saves catalog data relating to the encounter in a csv file.\n",
    "\n",
    "References:  \n",
    "- [Add IllustrisTNG ref]\n",
    "- Sections of this code were enhanced with the assistance of ChatGPT made by OpenAI.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186b202-41e4-4e51-b426-9452ff296c0c",
   "metadata": {},
   "source": [
    "# Finding Galaxy Mergers within the IllustrisTNG Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d3e73-43f8-40d5-a66e-4b60c38ec7b5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffaf7935-2431-4286-ba29-0ddb848dfedd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Done\n"
     ]
    }
   ],
   "source": [
    "# ================================ IMPORTS ================================ #\n",
    "import os, argparse, h5py\n",
    "import numpy as np, pandas as pd, scipy.signal\n",
    "import matplotlib.pyplot as plt \n",
    "import illustris_python as il\n",
    "\n",
    "print(\"Imports Done\")\n",
    "\n",
    "# Global variables\n",
    "SIM_DIR = '../sims.TNG/TNG50-1/output/'\n",
    "\n",
    "# A useful fucntion I often use for indented printing\n",
    "def tabprint( printme, start = '\\t - ', end = '\\n' ):\n",
    "    print( start + str(printme), end = end )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cc9a5-46ed-44a4-9f2a-f818722fd23d",
   "metadata": {},
   "source": [
    "---\n",
    "## Command Line Arguments\n",
    "\n",
    "This is written in JupyterLab, and will be compiled and ran in python for faster execution.  This will define the possible input command line arguements.\n",
    "\n",
    "\n",
    "WARNING:  I have not been consistent with implementing and following arguments.  Code still in indevlopment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aad8ad-d829-43e3-81cb-950629bab0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: Defined\n"
     ]
    }
   ],
   "source": [
    "# This argument decides if code is in python or jupyter.\n",
    "buildEnv = False\n",
    "\n",
    "# Define argument parser function \n",
    "def initParser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument( '-s', '--simDir', default = '../sims.TNG/TNG50-1/output/',  type=str, \\\n",
    "                        help=\"Base directory for a single simulation on the IllustrisTNG servers.\")   \n",
    "    \n",
    "    parser.add_argument( '-n', '--simName', default = 'TNG50-1',  type=str, \\\n",
    "                        help=\"Name for the simulation being worked on.\")\n",
    "    \n",
    "    parser.add_argument( '-o', '--overwrite', default = False,  type=bool, \\\n",
    "                        help=\"Overwrite output files?  If false, will check if output file exists before beginning time-consuming tasks.\")\n",
    "    \n",
    "    parser.add_argument( '-t', '--trim', default = -1,  type=int, \\\n",
    "                        help=\"Default number of subhalos to consider, sorted by highest mass first.\")\n",
    "    \n",
    "    parser.add_argument( '-f', '--function', default = 'None', type=str, \\\n",
    "                        help=\"Default function program will be executing.\")\n",
    "    \n",
    "    parser.add_argument( '-d', '--dataDir', default = 'data', type=str, \\\n",
    "                        help=\"Default location to store misc data files.\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = initParser()\n",
    "print(\"Args: Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28ddf0-65da-4a9f-950a-20619ed0afe7",
   "metadata": {},
   "source": [
    "## To Python? Or to JupyterLab? \n",
    "This will establish if this is being run in a JupyterLab environment or from Command Line in Python. \n",
    "\n",
    "NOTE:  If you're running this in Jupyter, modify the `cmdStr` below to whatever variables you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3bad9c-5048-4aaa-b254-fdb542bc0b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Building Environment\n",
      "CMD Line: \n",
      "\t$: python3 targets-working.py --trim 10 --dataDir data\n",
      "Args: Read\n",
      "Namespace(simDir='../sims.TNG/TNG50-1/output/', simName='TNG50-1', overwrite=False, trim=10, function='None', dataDir='data')\n"
     ]
    }
   ],
   "source": [
    "# Am I in a jupyter notebook?\n",
    "try:\n",
    "    \n",
    "    # This command is NOT available in a python script\n",
    "    get_ipython().__class__.__name__\n",
    "    buildEnv = True\n",
    "    print (\"In Building Environment\")\n",
    "    \n",
    "    # Command Line Arguments\n",
    "    cmdStr  = 'python3 targets-working.py'\n",
    "    cmdStr += ' --trim 10'\n",
    "    cmdStr += ' --dataDir data'\n",
    "    \n",
    "    # Read string as if command line\n",
    "    print( \"CMD Line: \\n\\t$:\", cmdStr)\n",
    "    \n",
    "    # This function doesn't like the 'python3 file.py' part.\n",
    "    args = parser.parse_args(cmdStr.split()[2:])\n",
    "\n",
    "# Or am I in a python script?\n",
    "except:\n",
    "    \n",
    "    # Read CMD arguments\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "print( \"Args: Read\")\n",
    "print( args )\n",
    "\n",
    "# Setup data directory if not found\n",
    "os.makedirs(args.dataDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd168c99-22b6-427d-9600-7c6ea4fb4bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this locational valid?\n",
      "Simulation data: True - ../sims.TNG/TNG50-1/output/\n"
     ]
    }
   ],
   "source": [
    "if buildEnv: \n",
    "    # Location of one simulation\n",
    "    print(\"Is this locational valid?\")\n",
    "    print( f\"Simulation data: {os.path.exists( args.simDir )} - {args.simDir}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28feb0bf-fed5-43f1-999a-854079cc1c20",
   "metadata": {},
   "source": [
    "---\n",
    "# Halos and SubHalos\n",
    "Within the simulation, Halos are the largest set of objects that are gravitationally bound to each other, I like to think of them as galaxy clusters.  Subhalos are also gravitationally bound objects but more dense, and I suspect has to do with potential energy.   I like to think of them as galaxies, globular clusters, blobs of gas, etc.  (That's my reasoning and I'm sticking to it)\n",
    "\n",
    "\n",
    "For more information, pleas visit the IllustrisTNG Data Specification Page.  https://www.tng-project.org/data/docs/specifications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4caf19b4-feb1-4b2d-bc83-364f0c45f542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful function for constructing/deconstructing subhalo ids.\n",
    "\n",
    "def generate_subhalo_id_raw(snap_num, subfind_id):\n",
    "    # Convert input to integers in case they are passed as strings\n",
    "    snap_num = int(snap_num)\n",
    "    subfind_id = int(subfind_id)\n",
    "    # Calculate the SubhaloIDRaw\n",
    "    subhalo_id_raw = snap_num * 10**12 + subfind_id\n",
    "    return subhalo_id_raw\n",
    "\n",
    "def deconstruct_subhalo_id_raw(subhalo_id_raw):\n",
    "    # Convert input to integer in case it is passed as a string\n",
    "    subhalo_id_raw = int(subhalo_id_raw)\n",
    "    # Extract SnapNum and SubfindID from SubhaloIDRaw\n",
    "    snap_num = subhalo_id_raw // 10**12\n",
    "    subfind_id = subhalo_id_raw % 10**12\n",
    "    return (snap_num, subfind_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654a989-b038-469e-98de-fc5f693396a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Mass Filter\n",
    "\n",
    "So I am looking for larger galaxies that visualize well.  I will be choosing galaxies that are between masses of 1/10th and x10 the Milky Way galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b53a6f1-2773-4834-92a7-9a50cdd9b5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mass Mask: data/TNG50-1-67-mask-mass-10.npy\n",
      "(6244619,) bool\n"
     ]
    }
   ],
   "source": [
    "n_subhalo = -1\n",
    "\n",
    "def getMassFilter( args, snapNum, mScale = 10 ):\n",
    "    \n",
    "    # This is the first time I pull data for every single subhalo.  Let's save the value for a later time.\n",
    "    global n_subhalo\n",
    "    \n",
    "    # Define where file will be saved\n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-mass-{mScale}.npy'\n",
    "    \n",
    "    # Read from file if it exits\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Mass Mask: {mLoc}\")\n",
    "        mass_mask = np.load( mLoc )\n",
    "        n_subhalo = mass_mask.shape[0]\n",
    "        return mass_mask\n",
    "    \n",
    "    # define mass limits\n",
    "    milky_way_mass = 150.0  # in (10^10 M_⊙) \n",
    "    upper_mass = milky_way_mass * mScale\n",
    "    lower_mass = milky_way_mass / mScale\n",
    "    \n",
    "    # Pull masses for all subhalos in snapshot\n",
    "    fields = ['SubhaloMass']\n",
    "    print(\"Pulling Masses for all Subhalos\")\n",
    "    print(\"WARNING: May take a while \")\n",
    "    SubhaloMass = il.groupcat.loadSubhalos( args.simDir, snapNum, fields=fields)\n",
    "    \n",
    "    # This is the first occasion where I wi\n",
    "    \n",
    "    # Find galaxies between upper and lower mass\n",
    "    mask_mass = ( SubhaloMass[:] <= upper_mass ) & ( SubhaloMass[:] >= lower_mass )\n",
    "    \n",
    "    # Save mass\n",
    "    np.save( mLoc, mask_mass )\n",
    "    \n",
    "    # Needed elsewhere\n",
    "    n_subhalo = mask_mass.shape[0]\n",
    "    \n",
    "    return mask_mass\n",
    "    \n",
    "if buildEnv and True:\n",
    "    args.overwrite = False\n",
    "    mask_mass = getMassFilter( args, 67 )\n",
    "    print( mask_mass.shape, mask_mass.dtype )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea4bb7-779c-4776-9b84-9dfd74e6c2ab",
   "metadata": {},
   "source": [
    "___\n",
    "## Centrals and Satellites\n",
    "Halo's often have a central galaxy that's the largest, with smaller subhalos orbiting it called satellites.  For convenience, let's create a mask of these central galaxies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca282d15-13f8-4c83-bb26-7c66ef93bd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Central Galaxy file: data/TNG50-1-67-mask-central.npy\n",
      "Central Galaxies: (6244619,) [ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "def expand_mask_from_list( true_list ):    \n",
    "    mask = np.full( n_subhalo, False, dtype=bool )    \n",
    "    mask[true_list] = True    \n",
    "    return mask\n",
    "    \n",
    "\n",
    "def getCentralFilter( args, snapNum = 99 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-central.npy'\n",
    "\n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Central Galaxy file: {mLoc}\")\n",
    "        mask_central = np.load( mLoc )\n",
    "        return mask_central\n",
    "\n",
    "    print(f\"Getting Central SubHalo IDs for sim/snapshot: {args.simName} / {snapNum}\")\n",
    "\n",
    "    # The GroupFirstSub is the subhalo id for the largest subhalo in a halo.  \n",
    "    GroupFirstSub = il.groupcat.loadHalos( args.simDir, snapNum, fields=['GroupFirstSub'])\n",
    "\n",
    "    # Filter out groups that contain no subhalos.\n",
    "    w = np.where(GroupFirstSub >= 0) # value of -1 indicates no subhalo in this group\n",
    "    central_ids = GroupFirstSub[w]\n",
    "    \n",
    "    # Expand into a full array with a value for every subhalo\n",
    "    mask_central = expand_mask_from_list( central_ids )\n",
    "    \n",
    "    # Save mass\n",
    "    np.save( mLoc, mask_central )\n",
    "    \n",
    "    return mask_central\n",
    "\n",
    "if buildEnv and True: \n",
    "\n",
    "    args.overwrite = False\n",
    "    mask_central = getCentralFilter( args, snapNum = 67 )\n",
    "    print('Central Galaxies:', mask_central.shape, mask_central[:10] )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ba957-ce5c-42e0-8e05-3b998a5e1c77",
   "metadata": {},
   "source": [
    "# (r) Galaxy Morphologies (Deep Learning)\n",
    "\n",
    "Because our method relies on disks of galaxies, it might be useful for us to find mergers betweeen two disk galaxies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27267df1-15b8-4654-bfc3-b233a1821373",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file: subcatalogs/TNG50-1-morphologies_deeplearn.hdf5\n",
      "Top-level headers and sizes:\n",
      "\tGroup: Header, Number of items: 0\n",
      "\tGroup: Snapshot_25, Number of items: 4\n",
      "\tGroup: Snapshot_29, Number of items: 4\n",
      "\tGroup: Snapshot_33, Number of items: 4\n",
      "\tGroup: Snapshot_40, Number of items: 4\n",
      "\tGroup: Snapshot_50, Number of items: 4\n",
      "\tGroup: Snapshot_67, Number of items: 4\n",
      "Reading Disk Morphology Mask: data/TNG50-1-67-mask-disk-morphology.npy\n",
      "Disk Galaxies: (6244619,) [False  True  True  True  True False  True False  True  True]\n"
     ]
    }
   ],
   "source": [
    "# A function to print the upper level of an HDF5 file.\n",
    "def print_HDF5_info( file_path ):\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        print( f\"HDF5 file: {file_path}\" )\n",
    "        print(\"Top-level headers and sizes:\")\n",
    "\n",
    "        # Iterate over items in the root of the file\n",
    "        for key in file.keys():\n",
    "            # Get the object (could be a group or dataset)\n",
    "            item = file[key]\n",
    "\n",
    "            # Check if the item is a group or dataset and print its size\n",
    "            if isinstance(item, h5py.Group):\n",
    "                print(f\"\\tGroup: {key}, Number of items: {len(item)}\")\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                print(f\"\\tDataset: {key}, Shape: {item.shape}\")\n",
    "    # Close file\n",
    "\n",
    "def getDiskMorphologyFilter( args, snapNum = 99 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-disk-morphology.npy'\n",
    "\n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Disk Morphology Mask: {mLoc}\")\n",
    "        mask_disk = np.load( mLoc )\n",
    "        return mask_disk\n",
    "    \n",
    "    # Check if morphology file exists.  \n",
    "    hdf5Loc = f'{args.dataDir}/TNG50-1-morphologies_deeplearn.hdf5'\n",
    "    if not os.path.exists( hdf5Loc ):\n",
    "        print(\"File missing: \", hdf5Loc )\n",
    "        raise AssertionError\n",
    "    \n",
    "    # Read the deeplearning morphology file\n",
    "    with h5py.File(f'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5', 'r') as file:\n",
    "        \n",
    "        header = f'Snapshot_{snapNum}'\n",
    "        \n",
    "        # Verify snapshot header is in file\n",
    "        if header not in file.keys():\n",
    "            print(f\"Bad HDF5 header: {header} / {file.keys()}\" )\n",
    "            return None       \n",
    "        \n",
    "        subhaloIDs      = np.array( file[header]['SubhaloID'] )\n",
    "        subhaloDiskProb = np.array( file[header]['P_Disk'] )\n",
    "        \n",
    "        # Iterate through and grab subhalos with a greater chance of being a disk galaxy\n",
    "        disk_list = []\n",
    "        for i in range( subhaloIDs.shape[0] ):\n",
    "            if subhaloDiskProb[i] > 0.5:\n",
    "                disk_list.append( subhaloIDs[i] )\n",
    "        \n",
    "    # Done reading file.\n",
    "    \n",
    "    # create mask \n",
    "    mask_disk = expand_mask_from_list( np.array( disk_list ) )\n",
    "    \n",
    "    # Save mass\n",
    "    print(f\"Saving Disk Morphology Mask: {mLoc}\")\n",
    "    np.save( mLoc, mask_disk )\n",
    "    \n",
    "    return mask_central    \n",
    "if buildEnv and True:\n",
    "    print_HDF5_info( 'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5' )\n",
    "    args.overwrite=False\n",
    "    mask_disk = getDiskMorphologyFilter( args, 67 )  \n",
    "    \n",
    "    print('Disk Galaxies:', mask_disk.shape, mask_disk[:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df6eee-f1ae-461b-93a1-18bb8fd7cb1c",
   "metadata": {},
   "source": [
    "## (y) Merger History\n",
    "\n",
    "Because manually detecting major mergers in the merger tree is messy (trust me, I tried), I'll be using someone else's subcatalogs to detect major mergers between galaxies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea33f1ae-0c63-4884-9707-dff96eb333db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Upcoming Major Merger Mask: data/TNG50-1-67-mask-major-merger-13.npy\n",
      "Disk Galaxies: (6244619,) [ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getMajorMergerMask( args, snapNum = 67, snapCutoff=13 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-major-merger-{snapCutoff}.npy'\n",
    "    \n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Upcoming Major Merger Mask: {mLoc}\")\n",
    "        mask_merger = np.load( mLoc )\n",
    "        return mask_merger\n",
    "    \n",
    "    file_loc = f'subcatalogs/MergerHistory_0{snapNum}.hdf5'\n",
    "    \n",
    "    print( f\"Merger History Loc: {file_loc}\" )\n",
    "    print( f\"File found: { os.path.exists( file_loc ) }\" )\n",
    "    \n",
    "    # Return None if no file found for snap num.\n",
    "    if not os.path.exists( file_loc ):\n",
    "        print(f\"WARNING:  Could not find file: {file_loc}\")\n",
    "        raise ValueError(f\"Subcatalog File Missing: {file_loc}\")\n",
    "        return None\n",
    "    \n",
    "    # Read Merger History file.    \n",
    "    with h5py.File(file_loc, 'r') as file:\n",
    "            \n",
    "        # Get the object (could be a group or dataset)\n",
    "        dataset = file['SnapNumNextMajorMerger']\n",
    "        \n",
    "        # Create a boolean mask for values that are non-negative and below the upper limit\n",
    "        mask_merger = (dataset[:] >= 0) & (dataset[:] <= (snapNum + snapCutoff) )\n",
    "        \n",
    "    # Saving\n",
    "    print(f\"Saving Major Merger Mask: {mLoc}\")\n",
    "    np.save( mLoc, mask_merger )\n",
    "       \n",
    "    return mask_merger\n",
    "            \n",
    "    # Find merger\n",
    "    \n",
    "# Get merger tree catalog\n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    # print_HDF5_info( f'subcatalog/MergerHistory_0{snapNum}.hdf5' )\n",
    "    \n",
    "    mask_merger = getMajorMergerMask( args, 67, 13 )\n",
    "    \n",
    "    print('Disk Galaxies:', mask_merger.shape, mask_merger[:10] )\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559f8a3-7e74-4c77-b46d-6a8feeb7e565",
   "metadata": {},
   "source": [
    "## (t) Galaxy Morphologies (Kinematic) and Bar Properties\n",
    "We would like to know additional details about the distributions of mass bewteen the bulge, discs, and halos.  This subcatalog appears to have that info. \n",
    "\n",
    "NOTE: Ignoring for now.  But potentially a future consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea39b1ee-dee1-4bb1-9dff-b13ba3117586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if buildEnv and False:\n",
    "    print_HDF5_info( 'subcatalogs/morphs_kinematic_bars.hdf5' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd322f-6dca-456a-8ddc-5c672d5add91",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Combine Masks Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b4e295-6293-4539-b2f2-9211ffd4980d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mass Mask: data/TNG50-1-67-mask-mass-10.npy\n",
      "Reading Central Galaxy file: data/TNG50-1-67-mask-central.npy\n",
      "Reading Disk Morphology Mask: data/TNG50-1-67-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: data/TNG50-1-67-mask-major-merger-13.npy\n",
      "(6244619,)\n",
      "Remaining Subhalos: 108\n",
      "GOIs: (108,)\n",
      "Reading Mass Mask: data/TNG50-1-67-mask-mass-10.npy\n",
      "Reading Central Galaxy file: data/TNG50-1-67-mask-central.npy\n",
      "Reading Disk Morphology Mask: data/TNG50-1-67-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: data/TNG50-1-67-mask-major-merger-13.npy\n"
     ]
    }
   ],
   "source": [
    "def combine_masks(mask_list):\n",
    "    \n",
    "    # Verify that all masks have the same shape\n",
    "    if not all(mask.shape == mask_list[0].shape for mask in mask_list):\n",
    "        raise ValueError(\"ERROR: Combine Masks: All masks must have the same shape\")\n",
    "\n",
    "    # Initialize the combined mask with the first mask\n",
    "    combined_mask = mask_list[0].copy()\n",
    "\n",
    "    # Perform logical AND operation with each subsequent mask\n",
    "    for mask in mask_list[1:]:\n",
    "        combined_mask &= mask\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "def generate_mask( args, snapNum, mass = True, massScale = 10, central = True, disk = True, major = True, majorCutoff = 13 ):\n",
    "    \n",
    "    # Create list of mask to find goi\n",
    "    mask_list = []\n",
    "    if mass:     mask_list.append( getMassFilter           ( args, snapNum, mScale = massScale ) )\n",
    "    if central:  mask_list.append( getCentralFilter        ( args, snapNum ) )\n",
    "    if disk:     mask_list.append( getDiskMorphologyFilter ( args, snapNum ) )\n",
    "    if major:    mask_list.append( getMajorMergerMask      ( args, snapNum, majorCutoff ) )\n",
    "\n",
    "    try:\n",
    "        # Get \n",
    "        combined_mask = combine_masks( mask_list )\n",
    "        goi_ids = np.where( combined_mask )        \n",
    "        return goi_ids[0]\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    # Create list of mask to find goi\n",
    "    mask_list = []\n",
    "    mask_list.append( getMassFilter( args, 67 ) )\n",
    "    mask_list.append( getCentralFilter( args, 67 ) )\n",
    "    mask_list.append( getDiskMorphologyFilter( args, 67 ) )\n",
    "    mask_list.append( getMajorMergerMask( args, 67 ) )\n",
    "\n",
    "    try:\n",
    "        # Get \n",
    "        combined_mask = combine_masks( mask_list )\n",
    "        print(combined_mask.shape)\n",
    "        print( f\"Remaining Subhalos: {np.sum( combined_mask )}\" )\n",
    "        goi_ids = np.where( combined_mask )\n",
    "        print( f\"GOIs: {goi_ids[0].shape}\" )\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Initial GOIs of interest\n",
    "    goi_ids = generate_mask( args, 67 )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719a15d-e5a7-4e4d-b238-0424c4002e11",
   "metadata": {},
   "source": [
    "--- \n",
    "## Find GOI's in Future Merger Trees\n",
    "\n",
    "Since requesting a merger tree only returns it's tree for the current moment and backwards in time, I need to jump several snapshots forward, and identify which galaxies it belongs to there.  It's a long tedious process but I'll figure it out.\n",
    "\n",
    "WARNING: Something is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c1d2632-d8e0-4a1c-9f7a-e02ebd186840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Merger-of-Interest List: data/TNG50-1-moi-list-v1-67-75.txt\n",
      "0 [67000000216072 75000000145472]\n",
      "1 [67000000184179 75000000219314]\n",
      "2 [67000000218606 75000000244170]\n",
      "3 [67000000244815 75000000278684]\n",
      "4 [67000000242560 75000000285683]\n",
      "5 [67000000282211 75000000312712]\n",
      "6 [67000000308503 75000000326949]\n",
      "7 [67000000299414 75000000329789]\n",
      "8 [67000000302085 75000000330982]\n",
      "9 [67000000297999 75000000342071]\n",
      "10 [67000000325904 75000000347840]\n",
      "11 [67000000330081 75000000355558]\n",
      "12 [67000000329197 75000000364583]\n",
      "13 [67000000376070 75000000374226]\n",
      "14 [67000000350284 75000000376596]\n",
      "15 [67000000353206 75000000381221]\n",
      "16 [67000000351283 75000000382639]\n",
      "17 [67000000359921 75000000386246]\n",
      "18 [67000000356635 75000000388219]\n",
      "19 [67000000363215 75000000388850]\n",
      "20 [67000000367142 75000000390264]\n",
      "21 [67000000385766 75000000390924]\n",
      "22 [67000000363906 75000000396140]\n",
      "23 [67000000373825 75000000401636]\n",
      "24 [67000000369178 75000000402715]\n",
      "25 [67000000378394 75000000410491]\n",
      "26 [67000000386589 75000000414419]\n",
      "27 [67000000421199 75000000424943]\n",
      "28 [67000000404128 75000000428068]\n",
      "29 [67000000393423 75000000428450]\n",
      "30 [67000000399865 75000000433460]\n",
      "31 [67000000396987 75000000434317]\n",
      "32 [67000000396305 75000000435282]\n",
      "33 [67000000406810 75000000437050]\n",
      "34 [67000000412331 75000000438563]\n",
      "35 [67000000414327 75000000439488]\n",
      "36 [67000000423574 75000000444726]\n",
      "37 [67000000424348 75000000448516]\n",
      "38 [67000000422759 75000000449103]\n",
      "39 [67000000417693 75000000450459]\n",
      "40 [67000000426538 75000000451662]\n",
      "41 [67000000433484 75000000454995]\n",
      "42 [67000000435040 75000000455393]\n",
      "43 [67000000427901 75000000456398]\n",
      "44 [67000000440536 75000000458620]\n",
      "45 [67000000438729 75000000459629]\n",
      "46 [67000000436238 75000000462291]\n",
      "47 [67000000443678 75000000467221]\n",
      "48 [67000000449562 75000000469603]\n",
      "49 [67000000446735 75000000470774]\n",
      "50 [67000000464803 75000000475459]\n",
      "51 [67000000448785 75000000481240]\n",
      "52 [67000000463802 75000000483794]\n",
      "53 [67000000459245 75000000488565]\n",
      "54 [67000000462087 75000000490145]\n",
      "55 [67000000465124 75000000490512]\n",
      "56 [67000000462756 75000000490658]\n",
      "57 [67000000467140 75000000492614]\n",
      "58 [67000000471346 75000000495892]\n",
      "59 [67000000480204 75000000503173]\n",
      "60 [67000000456491 75000000503478]\n",
      "61 [67000000476656 75000000507204]\n",
      "62 [67000000484567 75000000508211]\n",
      "63 [67000000477343 75000000509342]\n",
      "64 [67000000480833 75000000510138]\n",
      "65 [67000000489023 75000000510789]\n",
      "66 [67000000482532 75000000515580]\n",
      "67 [67000000488251 75000000517754]\n",
      "68 [67000000486798 75000000518363]\n",
      "69 [67000000499549 75000000519189]\n",
      "70 [67000000486254 75000000519379]\n",
      "71 [67000000491964 75000000521482]\n",
      "72 [67000000495543 75000000524740]\n",
      "73 [67000000500228 75000000525346]\n",
      "74 [67000000501261 75000000527582]\n",
      "75 [67000000500545 75000000527679]\n",
      "76 [67000000495789 75000000527793]\n",
      "77 [67000000521654 75000000528239]\n",
      "78 [67000000506009 75000000528469]\n",
      "79 [67000000508235 75000000529750]\n",
      "80 [67000000506399 75000000531566]\n",
      "81 [67000000506693 75000000533424]\n",
      "82 [67000000511452 75000000533911]\n",
      "83 [67000000498214 75000000534048]\n",
      "84 [67000000506299 75000000535169]\n",
      "85 [67000000505694 75000000535679]\n",
      "86 [67000000510751 75000000537217]\n",
      "87 [67000000508867 75000000539518]\n",
      "88 [67000000519492 75000000540448]\n",
      "89 [67000000513317 75000000540627]\n",
      "90 [67000000522703 75000000542380]\n",
      "91 [67000000514475 75000000544993]\n",
      "92 [67000000519328 75000000545396]\n",
      "93 [67000000519227 75000000546673]\n",
      "94 [67000000516766 75000000548156]\n",
      "95 [67000000520068 75000000550730]\n",
      "96 [67000000528728 75000000553797]\n",
      "97 [67000000526774 75000000555154]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_subhalo_id_raw(subfind_id, snap_num, ):\n",
    "    # Convert input to integers in case they are passed as strings\n",
    "    snap_num = int(snap_num)\n",
    "    subfind_id = int(subfind_id)\n",
    "    # Calculate the SubhaloIDRaw\n",
    "    subhalo_id_raw = snap_num * 10**12 + subfind_id\n",
    "    return subhalo_id_raw\n",
    "\n",
    "\n",
    "def reverse_SubhaloIDRaw( SubhaloIDRaw ):\n",
    "    snapNum = SubhaloIDRaw // 10**12\n",
    "    subhaloID = SubhaloIDRaw % 10**12\n",
    "    return snapNum, subhaloID\n",
    "\n",
    "def find_gois_in_tree( tree_id, snapNum, goi_list, args ):\n",
    "    \n",
    "    # Load only SubhaloIDRaw for effecient retrieval time\n",
    "    tree_subhaloIDRaw = il.sublink.loadTree( args.simDir, snapNum, tree_id, fields=['SubhaloIDRaw'] )\n",
    "\n",
    "    # See if any of my GOIs are in list\n",
    "    goi_mask = np.isin( goi_list, tree_subhaloIDRaw )\n",
    "    n_matches = np.sum(goi_mask)\n",
    "\n",
    "\n",
    "    # If none found\n",
    "    if n_matches == 0:\n",
    "        return []\n",
    "\n",
    "    # Else we have results\n",
    "    # print( f'Tree - Matches: {tree_id} - {n_matches}\\n' )\n",
    "\n",
    "    # Get the index locations where the mask is True\n",
    "    goi_loc = np.where(goi_mask)[0]\n",
    "    goi_ids_in_tree = goi_list[ goi_loc ]\n",
    "\n",
    "    goi_tree_list = []\n",
    "\n",
    "    for goi in goi_ids_in_tree:\n",
    "        goi_tree_list.append( ( goi, generate_subhalo_id_raw( tree_id, snapNum ) ) )\n",
    "\n",
    "    return goi_tree_list\n",
    "\n",
    "def getMOI_v1( args, start_snapNum, stop_snapNum ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-moi-list-v1-{start_snapNum}-{stop_snapNum}.txt'\n",
    "    \n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Merger-of-Interest List: {mLoc}\")\n",
    "        moi_list = np.loadtxt( mLoc, dtype=int )\n",
    "        return moi_list\n",
    "    \n",
    "    goi_ids = generate_mask( args, start_snapNum )\n",
    "    goi_ids_raw = np.array([ generate_subhalo_id_raw( goi, start_snapNum ) for goi in goi_ids ])\n",
    "    print( f\"Search for Galaxies of Interest: {goi_ids_raw.shape}\")\n",
    "    \n",
    "    merger_ids = generate_mask( args, stop_snapNum, mass = True, massScale = 12, central = True, disk = False, major=False )\n",
    "    print( f\"Searching within Merger Trees: {merger_ids.shape}\" )\n",
    "    \n",
    "    moi_list = []\n",
    "    for i, mid in enumerate(merger_ids):  \n",
    "        tabprint( f\" {i} / {merger_ids.shape[0]} - {mid}\", end='\\r' )\n",
    "        moi_list.extend( find_gois_in_tree( mid, stop_snapNum, goi_ids_raw, args ) )\n",
    "    \n",
    "    print( f\"\\nFound GOI / Tree Matches: {len( moi_list) }\")\n",
    "    \n",
    "    # Save list for future reference\n",
    "    moi_list = np.array( moi_list, dtype=int )\n",
    "    np.savetxt( mLoc, moi_list, fmt='%i', header='merger-goi tree-goi' )\n",
    "    return moi_list\n",
    "\n",
    "if buildEnv and True:  \n",
    "    \n",
    "    args.overwrite = False\n",
    "    \n",
    "    moi_list = getMOI_v1( args, 67, 75 )\n",
    "    \n",
    "    for i in range( moi_list.shape[0] ):\n",
    "        print( i, moi_list[i] )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70597fcc-01dc-4155-ad52-e929b3d5821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define print fucntion for a row\n",
    "def printRow( tree, i, fields ):\n",
    "    # if i == -1:\n",
    "    #     print(\"Invalid index\")\n",
    "    #     return\n",
    "    \n",
    "    print( \" - \".join( [ f\"{key}:{tree[key][i]}\" for key in fields ]) )\n",
    "    \n",
    "def createVisLink( subhaloIDRaw, projection = 'face', simulation='TNG50-1' ):\n",
    "        tmp = reverse_SubhaloIDRaw( subhaloIDRaw )    \n",
    "        \n",
    "        if projection == 'face':\n",
    "            link = f\"https://www.tng-project.org/api/{simulation}/snapshots/{tmp[0]}/subhalos/{tmp[1]}/vis.png?partType=stars&partField=stellarComp-jwst_f200w-jwst_f115w-jwst_f070w&size=1&method=histo&rotation=face-on&plotStyle=edged\"\n",
    "        # Else, project x,y plane\n",
    "        else:\n",
    "            link = f\"https://www.tng-project.org/api/{simulation}/snapshots/{tmp[0]}/subhalos/{tmp[1]}/vis.png?partType=stars&partField=stellarComp-jwst_f200w-jwst_f115w-jwst_f070w&size=1&method=histo&nPixels=256%2C256&axes=0%2C1&plotStyle=edged\"\n",
    "        return link\n",
    "    \n",
    "def analyze_MOI( args, gois ):\n",
    "    \n",
    "    mGOI = gois[0]\n",
    "    tGOI = gois[1]\n",
    "    \n",
    "    print( 'Final Viz: ', createVisLink( tGOI ) )\n",
    "    \n",
    "    tabprint( f'Merger GOI: {mGOI}' )\n",
    "    tabprint( f'Tree   GOI: {tGOI}' )    \n",
    "    \n",
    "    # If matches found, load more info\n",
    "    fields = ['SubhaloID','NextProgenitorID','MainLeafProgenitorID','FirstProgenitorID','SubhaloGrNr',\\\n",
    "              'SubhaloIDRaw','SubhaloMass', 'RootDescendantID', 'SnapNum', 'DescendantID',\\\n",
    "              'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', ]\n",
    "    \n",
    "    tree_snapNum, tree_subhaloID = reverse_SubhaloIDRaw( tGOI )\n",
    "    goi_snapNum, goi_subhaloID = reverse_SubhaloIDRaw( mGOI )\n",
    "    \n",
    "    \n",
    "    # Load Tree with desired info\n",
    "    tree = il.sublink.loadTree( args.simDir, tree_snapNum, tree_subhaloID, fields=fields)\n",
    "    \n",
    "    # Create a dictionary to map Subhalo IDs to their index in the list\n",
    "    subhalo_index = {subhalo_id: index for index, subhalo_id in enumerate(tree['SubhaloID'])}  \n",
    "    ci = 0  # Starting index of requested subhalo/snapshot\n",
    "    \n",
    "    # Print some start info for familiarization\n",
    "    if False: \n",
    "        print(\"Printing basic info for familization\")\n",
    "        print(\"\\nChild Info\")\n",
    "        printRow( tree, ci, fields )\n",
    "\n",
    "        print(\"\\nPrimary Info\")\n",
    "        pi = subhalo_index.get( tree['FirstProgenitorID'][ci], -1 )\n",
    "        printRow( tree, pi, fields )\n",
    "\n",
    "        print(\"\\nSecondary Info\")\n",
    "        si = subhalo_index.get( tree['NextProgenitorID'][pi], -1 )\n",
    "        if si == -1:\n",
    "            print(\"No Secondary Parent\")\n",
    "        else:\n",
    "            printRow( tree, si, fields )\n",
    "    \n",
    "    \n",
    "    # Grab ids and velocity arrays of the primary galaxies throughout time. \n",
    "    pVel = np.ones( (tree_snapNum+1, 3) ) * np.nan    # Velocities\n",
    "    pIDRaw = np.zeros( ( tree_snapNum+1), dtype=int)  # SubhaloIDRaw\n",
    "        \n",
    "    while ci != -1:\n",
    "        \n",
    "        i, tmp = reverse_SubhaloIDRaw( tree['SubhaloIDRaw'][ci] )\n",
    "        \n",
    "        # Grab array values\n",
    "        pVel[i,:] = tree['SubhaloVel'][ci][:]\n",
    "        pIDRaw[i] = tree['SubhaloIDRaw'][ci]\n",
    "        \n",
    "        # Update to primary parent in previous snapshot\n",
    "        ci = subhalo_index.get( tree['FirstProgenitorID'][ci], -1 )\n",
    "\n",
    "    # Calculate the change in velocity (Δv)\n",
    "    dVel = np.diff(pVel, axis=0)\n",
    "    \n",
    "    # Calculate magnitude of acceleration at each time step (assumption Δt=1)\n",
    "    pAcc = np.sqrt( np.sum( dVel**2, axis=-1 ) )\n",
    "    snapshots = [ reverse_SubhaloIDRaw( pid )[0] for pid in pIDRaw ]\n",
    "    \n",
    "    plt.xlim( 50, 75 )\n",
    "    plt.plot( snapshots[1:], pAcc )\n",
    "    \n",
    "    # Grab peaks in acceleration after snapshot\n",
    "    cSnapshot = 55 \n",
    "    \n",
    "    # Find peaks and their prominences\n",
    "    peaks, properties = scipy.signal.find_peaks(pAcc[cSnapshot:], prominence=True)\n",
    "    peak_snapshots = peaks + cSnapshot + 1\n",
    "    prominences = properties['prominences']\n",
    "    \n",
    "    # Print or use the sorted peaks and their prominences\n",
    "    idList = []\n",
    "    print(\"\")\n",
    "    for i, sn in enumerate(peak_snapshots):\n",
    "        if prominences[i] < 5: continue\n",
    "        plt.axvline(x=peak_snapshots[i], color='r', linestyle='--', label=f'{peak_snapshots[i]} - {prominences[i]:.2f}')\n",
    "        print(f\"Peak at index {peak_snapshots[i]} with prominence {prominences[i]}: {pIDRaw[peak_snapshots[i]]} - {pIDRaw[peak_snapshots[i]+1]}\")\n",
    "        \n",
    "        \n",
    "        for j in range( peak_snapshots[i]-1, tree_snapNum, 1):\n",
    "            link = createVisLink( pIDRaw[j] ) \n",
    "            tabprint( link )\n",
    "        \n",
    "    data = {}\n",
    "    \n",
    "    plt.axvline( x=75, color='k', linestyle='--' )\n",
    "    \n",
    "    plt.xlabel('Snap Shots')\n",
    "    plt.ylabel('Acceleration Magnitude')\n",
    "    plt.title(f\"Fly-by Detection for SubhaloIDRaw: {tGOI}\")\n",
    "    plt.legend()\n",
    "    \n",
    "    return tree\n",
    "    \n",
    "\n",
    "if buildEnv and False:  \n",
    "    moi_list = getMOI_v1( args, 67, 75 )\n",
    "    \n",
    "    tmp_tree = analyze_MOI( args, moi_list[51] )\n",
    "    \n",
    "    print( tmp_tree.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8becaaf2-a6f3-4726-955d-fcef85ce1032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Merger-of-Interest List: data/TNG50-1-moi-list-v1-67-75.txt\n",
      "Index(['moi_SubhaloIDRaw', 'snapnum', 'p_acceleration', 'p_SubhaloIDRaw',\n",
      "       's_SubhaloIDRaw', 'p_SubhaloMass', 's_SubhaloMass', 'p_SubhaloPos',\n",
      "       's_SubhaloPos', 'p_SubhaloVel', 's_SubhaloVel', 'p_SubhaloSpin',\n",
      "       's_SubhaloSpin', 'p_SubhaloHalfmassRad', 's_SubhaloHalfmassRad',\n",
      "       'xy_projection', 'p_face_projection', 's_face_projection'],\n",
      "      dtype='object')\n",
      "[67000000216072 67000000184179 67000000218606 67000000325904\n",
      " 67000000329197 67000000350284 67000000351283 67000000356635\n",
      " 67000000385766 67000000363906 67000000386589 67000000399865\n",
      " 67000000424348 67000000426538 67000000436238 67000000449562\n",
      " 67000000448785 67000000459245 67000000456491 67000000476656\n",
      " 67000000484567 67000000482532 67000000486798 67000000499549\n",
      " 67000000486254 67000000500228 67000000521654 67000000506009\n",
      " 67000000508235 67000000506693 67000000498214 67000000519492\n",
      " 67000000522703 67000000516766]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_moi_info( args, gois ):\n",
    "    \n",
    "    mGOI = gois[0]\n",
    "    tGOI = gois[1]\n",
    "        \n",
    "    # tabprint( f'Merger GOI: {mGOI}' )\n",
    "    # tabprint( f'Tree   GOI: {tGOI}' )\n",
    "    \n",
    "    # If matches found, load more info\n",
    "    fields = ['SubhaloID','NextProgenitorID','MainLeafProgenitorID','FirstProgenitorID','SubhaloGrNr',\\\n",
    "              'SubhaloIDRaw','SubhaloMass', 'RootDescendantID', 'SnapNum', 'DescendantID',\\\n",
    "              'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', ]\n",
    "    \n",
    "    tree_snapNum, tree_subhaloID = reverse_SubhaloIDRaw( tGOI )\n",
    "    goi_snapNum, goi_subhaloID = reverse_SubhaloIDRaw( mGOI )    \n",
    "    \n",
    "    # Load Tree with desired fields\n",
    "    tree = il.sublink.loadTree( args.simDir, tree_snapNum, tree_subhaloID, fields=fields)\n",
    "    \n",
    "    # Create a dictionary to map Subhalo IDs to their index in the list\n",
    "    subhalo_index = {subhalo_id: index for index, subhalo_id in enumerate(tree['SubhaloID'])}  \n",
    "    ci = 0  # Starting index of requested subhalo/snapshot\n",
    "    \n",
    "    # Grab ids and velocity arrays of the primary galaxies throughout time. \n",
    "    pVel = np.ones( (tree_snapNum+1, 3) ) * np.nan    # Velocities\n",
    "    pIDRaw = np.zeros( ( tree_snapNum+1), dtype=int)  # SubhaloIDRaw\n",
    "        \n",
    "    while ci != -1:\n",
    "        \n",
    "        i, tmp = reverse_SubhaloIDRaw( tree['SubhaloIDRaw'][ci] )\n",
    "        \n",
    "        # Grab array values\n",
    "        pVel[i,:] = tree['SubhaloVel'][ci][:]\n",
    "        pIDRaw[i] = tree['SubhaloIDRaw'][ci]\n",
    "        \n",
    "        # Update to primary parent in previous snapshot\n",
    "        ci = subhalo_index.get( tree['FirstProgenitorID'][ci], -1 )\n",
    "\n",
    "    # Calculate the change in velocity (Δv)\n",
    "    dVel = np.diff(pVel, axis=0)\n",
    "    \n",
    "    # Calculate magnitude of acceleration at each time step (assumption Δt=1)\n",
    "    pAcc = np.sqrt( np.sum( dVel**2, axis=-1 ) )\n",
    "    snapshots = [ reverse_SubhaloIDRaw( pid )[0] for pid in pIDRaw ]\n",
    "    \n",
    "    snaploc = 1000000000000\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for snapFind in range( 55, 75 ):\n",
    "        #print(\"#####   %d   #####\" % snapFind)\n",
    "        \n",
    "        snapnum_mask = (tree['SubhaloIDRaw'] // snaploc) % snaploc == snapFind\n",
    "        snapnum_index = np.where( snapnum_mask )\n",
    "\n",
    "        # Gather masses\n",
    "        snapnum_masses = tree['SubhaloMass'][snapnum_index]\n",
    "\n",
    "        # Find n highest masses\n",
    "        n = 2\n",
    "        top5_index = np.argsort(snapnum_masses)[-n:][::-1]\n",
    "        \n",
    "        if len(top5_index) <= 1: continue\n",
    "       \n",
    "        pid = snapnum_index[0][top5_index[0]]\n",
    "        sid = snapnum_index[0][top5_index[1]]\n",
    "        \n",
    "        keys =  [ 'SubhaloIDRaw', 'SubhaloMass', 'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', ]\n",
    "        \n",
    "        data[snapFind] = {}\n",
    "        data[snapFind]['p_acceleration'] = pAcc[snapFind]\n",
    "        \n",
    "        for k in keys:\n",
    "            for c, ii in [ ('p',pid), ('s',sid) ]:\n",
    "                #print( k, c, ii )\n",
    "                data[snapFind]['%s_%s'%(c,k)] = tree[k][ii]\n",
    "                \n",
    "        data[snapFind]['xy_projection'] = createVisLink( tree['SubhaloIDRaw'][pid], projection = 'xy' )\n",
    "        data[snapFind]['p_face_projection'] = createVisLink( tree['SubhaloIDRaw'][pid], projection = 'face' )\n",
    "        data[snapFind]['s_face_projection'] = createVisLink( tree['SubhaloIDRaw'][sid], projection = 'face' )\n",
    "    \n",
    "    return data\n",
    "        \n",
    "\n",
    "def save_moi_info( args, moi_list, start_snapnum, stop_snapnum ):\n",
    "    \n",
    "    fLoc = f'{args.dataDir}/{args.simName}-moi-info-{start_snapnum}-{stop_snapnum}.csv'\n",
    "    \n",
    "    # If file exists, read and return.\n",
    "    if os.path.exists( fLoc ) and args.overwrite == False:\n",
    "        df = pd.read_csv( fLoc )\n",
    "        return df\n",
    "    \n",
    "    # Else, create file by getting info via merger trees.\n",
    "    data = {}\n",
    "    n = len( moi_list )\n",
    "    for i in range( n ):\n",
    "        print( i, ' / ', n, end='\\r'  )\n",
    "        data[moi_list[i][0]] = get_moi_info( args, moi_list[i] )\n",
    "    print('')\n",
    "    # Convert the nested dictionary to a list of records\n",
    "    records = [{'moi_SubhaloIDRaw': subhalo_id, 'snapnum': snapnum, **props}\n",
    "               for subhalo_id, snaps in data.items()\n",
    "               for snapnum, props in snaps.items()]\n",
    "\n",
    "    df = pd.json_normalize(records, sep='_')\n",
    "    print( df )\n",
    "\n",
    "    df.to_csv( fLoc , index=False )\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "if buildEnv and True:  \n",
    "    \n",
    "    start_sn = 67\n",
    "    stop_sn = 75\n",
    "    \n",
    "    moi_list = getMOI_v1( args, start_sn, stop_sn )\n",
    "    df = save_moi_info( args, moi_list, start_sn, stop_sn )\n",
    "    \n",
    "    df_filtered = df\n",
    "    \n",
    "    # Let's do some filtering for good targets.    \n",
    "    print( df.columns )\n",
    "    moi_list = df['moi_SubhaloIDRaw'].unique()\n",
    "    \n",
    "    for moi in moi_list:\n",
    "        #print( moi )\n",
    "        \n",
    "        m_condition = df['moi_SubhaloIDRaw'] == moi\n",
    "        p_mass = df.loc[m_condition, 'p_SubhaloMass']\n",
    "        s_mass = df.loc[m_condition, 's_SubhaloMass']\n",
    "        mass_ratio = s_mass / ( p_mass + s_mass )\n",
    "        ratio_cutoff = 0.1\n",
    "        ratio_condition = mass_ratio > ratio_cutoff\n",
    "        \n",
    "        if not np.any( ratio_condition ):\n",
    "            df_filtered = df_filtered[ df_filtered['moi_SubhaloIDRaw'] != moi ]\n",
    "        \n",
    "    print( df_filtered['moi_SubhaloIDRaw'].unique() )\n",
    "        \n",
    "    # First, let's grab all the GOIs to loop through.\n",
    "\n",
    "    df_filtered.to_csv('TNG50-1-moi-final-v1-67-75-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfa05c34-da82-43f7-8624-5b895ba19856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: Halo\n",
      "Group: Halo/TracerLength\n",
      "Dataset: Halo/TracerLength/bhs, Shape: (10247012,), Data type: int32\n",
      "Dataset: Halo/TracerLength/gas, Shape: (10247012,), Data type: int32\n",
      "Dataset: Halo/TracerLength/stars, Shape: (10247012,), Data type: int32\n",
      "Group: Halo/TracerOffset\n",
      "Dataset: Halo/TracerOffset/bhs, Shape: (10247012,), Data type: int64\n",
      "Dataset: Halo/TracerOffset/gas, Shape: (10247012,), Data type: int64\n",
      "Dataset: Halo/TracerOffset/stars, Shape: (10247012,), Data type: int64\n",
      "Dataset: ParentIDs, Shape: (3292847014,), Data type: uint64\n",
      "Group: Subhalo\n",
      "Group: Subhalo/TracerLength\n",
      "Dataset: Subhalo/TracerLength/bhs, Shape: (5688113,), Data type: int32\n",
      "Dataset: Subhalo/TracerLength/gas, Shape: (5688113,), Data type: int32\n",
      "Dataset: Subhalo/TracerLength/stars, Shape: (5688113,), Data type: int32\n",
      "Group: Subhalo/TracerOffset\n",
      "Dataset: Subhalo/TracerOffset/bhs, Shape: (5688113,), Data type: int64\n",
      "Dataset: Subhalo/TracerOffset/gas, Shape: (5688113,), Data type: int64\n",
      "Dataset: Subhalo/TracerOffset/stars, Shape: (5688113,), Data type: int64\n",
      "Dataset: TracerIDs, Shape: (3292847014,), Data type: uint64\n",
      "\n",
      "Dataset: EverInSubboxFlag, Shape: (6244619,), Data type: bool\n",
      "Dataset: FullBoxSnapNum, Shape: (3600,), Data type: int16\n",
      "Dataset: SnapNumMapApprox, Shape: (3600,), Data type: float32\n",
      "Dataset: SubboxScaleFac, Shape: (3600,), Data type: float32\n",
      "Dataset: SubboxSnapNum, Shape: (100,), Data type: int16\n",
      "Dataset: SubhaloIDs, Shape: (8880,), Data type: int64\n",
      "Dataset: SubhaloMBID, Shape: (8880, 3600), Data type: uint64\n",
      "Dataset: SubhaloMaxSBSnap, Shape: (8880,), Data type: int32\n",
      "Dataset: SubhaloMinEdgeDist, Shape: (8880, 7), Data type: float32\n",
      "Dataset: SubhaloMinSBSnap, Shape: (8880,), Data type: int32\n",
      "Dataset: SubhaloPos, Shape: (8880, 3600, 3), Data type: float32\n",
      "Dataset: SubhaloPosExtrap, Shape: (8880, 3600), Data type: int16\n",
      "Dataset: minEdgeDistRedshifts, Shape: (7,), Data type: float32\n"
     ]
    }
   ],
   "source": [
    "# Read HDF5 file\n",
    "\n",
    "import h5py\n",
    "\n",
    "def print_structure_and_size(file_name):\n",
    "    with h5py.File(file_name, 'r') as file:\n",
    "        def print_info(name, node):\n",
    "            if isinstance(node, h5py.Dataset):\n",
    "                print(f\"Dataset: {name}, Shape: {node.shape}, Data type: {node.dtype}\")\n",
    "            else:\n",
    "                print(f\"Group: {name}\")\n",
    "\n",
    "        file.visititems(print_info)\n",
    "\n",
    "file_loc = '../sims.TNG/TNG50-1/postprocessing/tracer_tracks/tr_all_groups_99_meta.hdf5'\n",
    "print_structure_and_size(file_loc)\n",
    "print('')\n",
    "print_structure_and_size( '../sims.TNG/TNG50-1/postprocessing/SubboxSubhaloList/subbox1_67.hdf5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d663c6-532b-4e1e-8c36-a1b4761793fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ef8e8-439e-43c0-86c2-fab91539969a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ceaa09-5fe6-44fe-bb94-8f6ee13a055a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed1355-0543-4e53-a85a-42db91aaca3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67afbc-6ad3-4135-ae0f-2449e7cfdd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7274c8-ba83-4783-9d2e-e7bf2dba5e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4351392-771e-4658-a0f7-f1a35f389192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4577a4-5f41-4201-b390-8eb017bd53a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c4f30-3ce1-4b9e-9cac-42b2add83b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7af34-b7e9-40b4-9328-85f53a95c34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f2690f-e26c-42c9-a958-b69525146983",
   "metadata": {},
   "source": [
    "# DEPRECATED\n",
    "\n",
    "Everything below this line is old and for reference purposes only..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4d665-9b02-4e64-9450-b981ef28f187",
   "metadata": {},
   "source": [
    "# Current Seletion Plan.  \n",
    "Since there are millions of galaxies, a hundred snapshops, and many millions of possible mergers, I'm going to try and do a selection screening for ideal merger events.  Here are the basic steps I'm considering\n",
    "\n",
    "0) (DONE) Select Snapshot with available subcatalog info.\n",
    "    - Status: Using Snapshot 67\n",
    "    - Thus far, the Morphology Subcatalog only has data for a few snapshots, therefore we will focus on these snapshots.\n",
    "    - If this proves to be too limited, we can build our own deep learning model, training on the X:Kinematic and Y:Galaxy DL Morphologies.\n",
    "    \n",
    "0) (Done) Use \"Merger History\" subcatalog to find galaxies about to undergo a major merger.\n",
    "    - Status: Found subhaloID's for 67 about to undergo a major merger within 7 snapshots.  \n",
    "        - Hencefort called MOI - (Mergers of Interest) \n",
    "    - Use \"SnapNumNextMajorMerger\" and filter to X snapshots in future.\n",
    "    - Return SubhaloID's\n",
    "    - Collect all snapshots into a single list\n",
    "    \n",
    "0) (Working) Use Merger Tree Catalog to get additional info\n",
    "    - Status:  \n",
    "        - Looking into subhalo 0 in snapshot 75 for moi's.  Found 68\n",
    "        - Working on pulling data of interest for those moi's. \n",
    "    - NOTE: Requesting a Merger Tree with a SubhaloID and Snapshot, will return it's merger tree UP TO THAT SNAPSHOT and sadly not into the future.\n",
    "        - Therefore we will randomly (sorted by biggest mass first) look at merger trees in the final snapshot.\n",
    "        - We will then search those mergers trees to see if they contain any of our galaxies of interest.  \n",
    "        - Since they're giant lists, this shouldn't take tooooooo long...  hopefully...  (fingers crossed)\n",
    "    - Optional \n",
    "        - Filter based on Mass threshold\n",
    "            - 1/4 the Milky Way and above\n",
    "        - Filter based on Central Galaxy.\n",
    "    - When galaxy of interest is found.\n",
    "        - Grab Halo ID.\n",
    "        - Go forward in time to retrieve ID of secondary galaxy\n",
    "        - Go backwards in time to collect velocity info\n",
    "        \n",
    "0) Use \"Morphological Deep Learning\" Subcatalog to get morphology type of both parents.\n",
    "    - Using ID of primary and secondary\n",
    "    \n",
    "0) Use Subhalo Catalog and Kinematic subcatalog to retrieve orbital parameters of both galaxies\n",
    "    \n",
    "6) Using Visualization tool, search for Halo ID and Snapnum. \n",
    "\n",
    "0) Feed the orbital parameters we find in IllustrisTNG, into SPAM.  Does it create an image (tidal features) similar to Illustrng visual? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
