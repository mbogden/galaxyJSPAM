{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f4c900c-d6ae-4081-880e-fdc9715a11ca",
   "metadata": {},
   "source": [
    "# Creating a Fitness Function\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03f068d-45f2-4a68-bce4-2edc564259fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 15:59:23.962272: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 15:59:24.122022: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules: Imported\n",
      "Tensorflow version:  2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Import Python Mobules\n",
    "import os, json, time, argparse, cv2\n",
    "import numpy as np\n",
    "from sys import argv\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "# Reduce warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Basic information 1\n",
    "print(\"Modules: Imported\")\n",
    "\n",
    "# Print tensorflow version\n",
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3094ac1d-3d4d-4647-991f-ae179f2aff5f",
   "metadata": {},
   "source": [
    "## Define Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926e6482-eb54-4425-aab2-90add1cdd29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: Defined\n"
     ]
    }
   ],
   "source": [
    "# Command line arguments\n",
    "\n",
    "showPlots = False\n",
    "buildEnv = False\n",
    "Yext = None\n",
    "Xext = None\n",
    "\n",
    "def initParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument( '-runName', )\n",
    "    parser.add_argument( \"-verbose\",  default = 0,  type=int, )\n",
    "    parser.add_argument( \"-save_model\", default='True', type=str )\n",
    "    parser.add_argument( \"-model_loc\", default='None',  type=str )\n",
    "    parser.add_argument( \"-data_loc\", default='None',  type=str )\n",
    "    parser.add_argument( \"-val_loc\", default='None',  type=str )\n",
    "\n",
    "    # Training Variables\n",
    "    parser.add_argument( \"-start\",    default = 0,  type=int, )\n",
    "    parser.add_argument( \"-stop\",     default = 1,  type=int, )\n",
    "    parser.add_argument( \"-num_epochs\",    default=1,       type=int )\n",
    "    parser.add_argument( \"-learning_rate\", default=0.0001,  type=float )\n",
    "    parser.add_argument( \"-batch_size\",    default=16,      type=int )\n",
    "    parser.add_argument( \"-keras_tuner\",   default='False', type=str )\n",
    "\n",
    "    # Data Generator\n",
    "    parser.add_argument( \"-data_gen\", default='True',  type=str )\n",
    "    parser.add_argument( \"-noise\",    default='True', type=str )\n",
    "    parser.add_argument( \"-warp_image\",  default='True',  type=str )\n",
    "    parser.add_argument( \"-channel_offset\",  default='True', type=str )\n",
    "\n",
    "    # Core Model types\n",
    "    parser.add_argument( \"-model\",   default = 'efficientNetB0', type=str)\n",
    "    parser.add_argument( \"-pool\",    default = 'None',           type=str )\n",
    "    parser.add_argument( \"-weights\", default = 'imagenet',       type=str )\n",
    "\n",
    "    # Final layers\n",
    "    parser.add_argument( \"-f_depth\", default = 8,  type=int )\n",
    "    parser.add_argument( \"-f_width\", default = 32, type=int )\n",
    "    parser.add_argument( \"-f_activation\", default = 'relu', type=str )\n",
    "    parser.add_argument( \"-output_activation\", default = 'sigmoid' )\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = initParser()\n",
    "print(\"Args: Defined\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2de7caa4-5824-4708-a8d6-8c90f06f28af",
   "metadata": {},
   "source": [
    "## Establish if Building Environment or Script\n",
    "Notice if code can see it's in a Jupyter Notebook Environment, it can create an artificial string of command line arguments.  This is convenient for testing the code in a Jupyter Notebook directly and changing variables on the fly.  Then when the Notebook is compiled as a python script, all changes can be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca70a14-2865-4c72-888e-41dd5cdc5291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Building Environment\n",
      "CMD:   -runName dl-s2-test -start 0 -stop 1 -num_epochs 1 -batch_size 32 -verbose 1 -data_loc data/dl-s2/dl-s2-10 -keras_tuner True\n",
      "Args: Read\n",
      "Namespace(runName='dl-s2-test', verbose=1, save_model=True, model_loc='None', data_loc='data/dl-s2/dl-s2-10', val_loc='None', start=0, stop=1, num_epochs=1, learning_rate=0.0001, batch_size=32, keras_tuner='True', data_gen=True, noise=True, warp_image=True, channel_offset=True, model='efficientNetB0', pool='None', weights='imagenet', f_depth=8, f_width=32, f_activation='relu', output_activation='sigmoid')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Am I in a jupyter notebook?\n",
    "try:\n",
    "    get_ipython().__class__.__name__\n",
    "    buildEnv = True\n",
    "    showPlots = False\n",
    "    print (\"In Building Environment\")\n",
    "    \n",
    "    # Build Command Line Arguments\n",
    "    cmdStr = ''\n",
    "    cmdStr += ' -runName dl-s2-test'\n",
    "    cmdStr += ' -start 0'\n",
    "    cmdStr += ' -stop 1'\n",
    "    cmdStr += ' -num_epochs 1'\n",
    "    cmdStr += ' -batch_size 32'\n",
    "    cmdStr += ' -verbose 1'\n",
    "    # cmdStr += ' -save_model True'\n",
    "    cmdStr += ' -data_loc data/dl-s2/dl-s2-10'\n",
    "    cmdStr += ' -keras_tuner True'\n",
    "    # cmdStr += ' -val_loc data/dl-s1/' \n",
    "    # cmdStr += ' -model efficientNetB1' \n",
    "    # cmdStr += ' -model_loc models/dl-s2-model-0.h5' \n",
    "    \n",
    "    # Read string as if command line\n",
    "    print( \"CMD: \", cmdStr)\n",
    "    args = parser.parse_args(cmdStr.split())\n",
    "\n",
    "\n",
    "# Or am I in a python script?\n",
    "except:\n",
    "    \n",
    "    # Read CMD arguments\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Check for valid runName\n",
    "    print( 'runName: ', args.runName )\n",
    "    if args.runName == None:\n",
    "        print( \"WARNING:  runName required\" )\n",
    "        exit()\n",
    "\n",
    "# Preprocess boolean cmd args\n",
    "if args.save_model == 'True': args.save_model = True \n",
    "else:  args.save_model = False\n",
    "\n",
    "if args.data_gen == 'True': args.data_gen = True \n",
    "else:  args.data_gen = False\n",
    "\n",
    "if args.noise == 'True': args.noise = True \n",
    "else:  args.noise = False\n",
    "\n",
    "if args.warp_image == 'True': args.warp_image = True \n",
    "else:  args.warp_image = False\n",
    "\n",
    "if args.channel_offset == 'True': args.channel_offset = True \n",
    "else:  args.channel_offset = False\n",
    "\n",
    "if 'test' in args.runName:\n",
    "    buildEnv = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print( \"Args: Read\")\n",
    "print( args )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcef34e1-81d8-4725-89ca-90ae59168496",
   "metadata": {},
   "source": [
    "## Define Device Strategy for Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6134c9cc-fe0f-4a7e-bd6c-e42e73dbcbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 15:59:26.235265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:26.269081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:26.269120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:26.271329: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 15:59:26.274120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:26.274158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:26.274169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:27.385562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:27.385609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:27.385614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-19 15:59:27.385632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 15:59:27.385661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read current devices\n",
    "devices = tf.config.get_visible_devices()\n",
    "\n",
    "# If no GPU found, use CPU\n",
    "if len(devices) == 1:\n",
    "    strategy = tf.distribute.OneDeviceStrategy('CPU') # Use local GPU\n",
    "\n",
    "# Standard single GPU on backus\n",
    "else:\n",
    "    #tf.config.experimental.set_memory_growth(devices[1],True)\n",
    "    strategy = tf.distribute.OneDeviceStrategy('GPU:0') # Use local GPU\n",
    "\n",
    "print( 'Devices:', devices )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3de0e070-9e4b-4712-b207-f6d5098623e8",
   "metadata": {},
   "source": [
    "## Define Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23cd6530-1c67-4072-a7f5-c3f92e82ee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  (5280, 1)\n",
      "type:  <class 'numpy.ndarray'>\n",
      "type ele:  <class 'numpy.float64'>\n",
      "data:  (5280, 128, 128, 3)\n",
      "{'score': 0.9459459459459459, 'model_data': '-9.93853,-4.5805,3.27377,-0.50008,-2.45565,-1.07799,23.33004,24.69427,3.49825,5.32056,309.6923,36.8125,41.78471,51.42857,0.3,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.94594,0,0,0.18901,-22.47999,9.02372,0.0,1.0,0.0,0.0,0.0,0.0', 'run_id': 'run_0000', 'target_id': '587722984435351614'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 140\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mwith\u001b[39;00m strategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdl-s2\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m args\u001b[39m.\u001b[39mdata_loc:\n\u001b[0;32m--> 140\u001b[0m         X, Y, Xval, Yval \u001b[39m=\u001b[39m loadData3(args)\n\u001b[1;32m    142\u001b[0m     \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mtid \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    143\u001b[0m         X, Y, Xval, Yval \u001b[39m=\u001b[39m loadData( args )\n",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m, in \u001b[0;36mloadData3\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    105\u001b[0m trainData   \u001b[39m=\u001b[39m data[\u001b[39m~\u001b[39mm]\n\u001b[1;32m    106\u001b[0m trainScores \u001b[39m=\u001b[39m scores[\u001b[39m~\u001b[39mm]\n\u001b[0;32m--> 107\u001b[0m trainInfo   \u001b[39m=\u001b[39m [ i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m i[\u001b[39m'\u001b[39m\u001b[39mrun_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39min\u001b[39;00m runNames[\u001b[39m~\u001b[39mmr] ]\n\u001b[1;32m    109\u001b[0m \u001b[39mprint\u001b[39m( \u001b[39m'\u001b[39m\u001b[39mtestRuns: \u001b[39m\u001b[39m'\u001b[39m,  np\u001b[39m.\u001b[39munique( [ k[\u001b[39m'\u001b[39m\u001b[39mrun_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m testInfo ] )\u001b[39m.\u001b[39mshape )\n\u001b[1;32m    110\u001b[0m \u001b[39mprint\u001b[39m( \u001b[39m'\u001b[39m\u001b[39mtrainRuns: \u001b[39m\u001b[39m'\u001b[39m,  np\u001b[39m.\u001b[39munique( [ k[\u001b[39m'\u001b[39m\u001b[39mrun_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m trainInfo ] )\u001b[39m.\u001b[39mshape )\n",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    105\u001b[0m trainData   \u001b[39m=\u001b[39m data[\u001b[39m~\u001b[39mm]\n\u001b[1;32m    106\u001b[0m trainScores \u001b[39m=\u001b[39m scores[\u001b[39m~\u001b[39mm]\n\u001b[0;32m--> 107\u001b[0m trainInfo   \u001b[39m=\u001b[39m [ i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m i[\u001b[39m'\u001b[39m\u001b[39mrun_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39min\u001b[39;00m runNames[\u001b[39m~\u001b[39mmr] ]\n\u001b[1;32m    109\u001b[0m \u001b[39mprint\u001b[39m( \u001b[39m'\u001b[39m\u001b[39mtestRuns: \u001b[39m\u001b[39m'\u001b[39m,  np\u001b[39m.\u001b[39munique( [ k[\u001b[39m'\u001b[39m\u001b[39mrun_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m testInfo ] )\u001b[39m.\u001b[39mshape )\n\u001b[1;32m    110\u001b[0m \u001b[39mprint\u001b[39m( \u001b[39m'\u001b[39m\u001b[39mtrainRuns: \u001b[39m\u001b[39m'\u001b[39m,  np\u001b[39m.\u001b[39munique( [ k[\u001b[39m'\u001b[39m\u001b[39mrun_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m trainInfo ] )\u001b[39m.\u001b[39mshape )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sliceKey( dictIn ):\n",
    "    dictOut =  dictIn\n",
    "    for k in dictOut:\n",
    "        if type( dictOut[k] ) == type( 'string' ):\n",
    "            if 'slice' in dictOut[k]:\n",
    "                #print( k, dictOut[k] )\n",
    "                tmp = dictOut[k].split('(')[1].split(')')[0].split(',')\n",
    "                s = int( tmp[0] )\n",
    "                e = int( tmp[1] )\n",
    "                dictOut[k] = slice( s, e )\n",
    "                #print( dictOut[k] )\n",
    "    \n",
    "    return dictOut \n",
    "\n",
    "def correlation(y_true, y_pred):\n",
    "    y = y_pred.numpy()\n",
    "    corr = np.corrcoef( y_true[:,0], y[:,0] )[0,1]\n",
    "    return corr\n",
    "\n",
    "    \n",
    "# Load zipped images and data\n",
    "def loadDLS1( args ):\n",
    "\n",
    "    if args.val_loc[-1] != '/':  args.val_loc += '/'\n",
    "    \n",
    "    # Load Key for data\n",
    "    with open( args.val_loc + 'data-key.json' ) as keyFile:\n",
    "        key = json.load( keyFile )\n",
    "    \n",
    "        \n",
    "    data = np.load( args.val_loc + 'data-all.npy')\n",
    "    scores = data[:,key['score']].reshape( data.shape[0], 1 )\n",
    "    img  = np.load( args.val_loc + 'img-all.npz' )['arr_0']\n",
    "    \n",
    "    # Rescale image values based on core model being used.\n",
    "    if 'efficientNet' in args.model :\n",
    "        img *= 255\n",
    "    \n",
    "    if buildEnv:\n",
    "        print( 'img:  ', img.shape, np.amin(img), np.amax(img) )\n",
    "        print( 'data: ', data.shape )\n",
    "        print( 'scores: ', scores.shape )\n",
    "    \n",
    "    # If in building environment, only use 1/10th of data\n",
    "    if buildEnv:\n",
    "        img = img[::10]\n",
    "        data = data[::10]\n",
    "        scores = scores[::10]\n",
    "    \n",
    "    return img, scores\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def loadData3( args ):\n",
    "    # Load data info\n",
    "    with open( args.data_loc + '.json' ) as keyFile:\n",
    "        info = json.load( keyFile )        \n",
    "    scores = np.array( [ k['score'] for k in info['info'] ] )\n",
    "    scores = scores.reshape((-1,1))\n",
    "\n",
    "    print( 'scores: ', scores.shape )\n",
    "    print( 'type: ', type(scores) )\n",
    "    print( 'type ele: ', type(scores[0,0]) )\n",
    "\n",
    "    \n",
    "    # Load images\n",
    "    data = np.load( args.data_loc + '.npz' )['arr_0'].astype( np.float32 )\n",
    "        \n",
    "    if buildEnv:\n",
    "        print( 'data: ', data.shape ) \n",
    "\n",
    "\n",
    "    if args.val_loc == None or args.val_loc == 'None':\n",
    "\n",
    "        n = data.shape[0]\n",
    "        # Grab every 5th run for testing\n",
    "\n",
    "        # Print first info\n",
    "        print( info['info'][0] )\n",
    "\n",
    "        # get list of total run models\n",
    "        runNames = np.unique( [ k['run_id'] for k in info['info'] ] )\n",
    "\n",
    "        # Create list of False for all runs, make every 5th True\n",
    "        mr = np.full( len(runNames), False )\n",
    "        mr[::5] = True\n",
    "\n",
    "        # Create list of False for all data points\n",
    "        m = np.full( n, False )\n",
    "\n",
    "        # Create mask where every 5th run is True\n",
    "        for i in range( n ):\n",
    "            if info['info'][i]['run_id'] in runNames[mr]:\n",
    "                m[i] = True\n",
    "\n",
    "        # Extract test and train data\n",
    "        m = np.full( n, False )\n",
    "        m[::5] = True\n",
    "\n",
    "        testData   = data[m]\n",
    "        testScores = scores[m]\n",
    "        testInfo   = [ i for i in info['info'] if i['run_id'] in runNames[mr] ]\n",
    "        \n",
    "        trainData   = data[~m]\n",
    "        trainScores = scores[~m]\n",
    "        trainInfo   = [ i for i in info['info'] if i['run_id'] in runNames[~mr] ]\n",
    "\n",
    "        print( 'testRuns: ',  np.unique( [ k['run_id'] for k in testInfo ] ).shape )\n",
    "        print( 'trainRuns: ',  np.unique( [ k['run_id'] for k in trainInfo ] ).shape )\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Set data as training data\n",
    "        trainData = data\n",
    "        trainScores = scores\n",
    "\n",
    "        # Read validation data\n",
    "        if 'dl-s1' in args.val_loc:\n",
    "            testData, testScores = loadDLS1( args )\n",
    "        else:\n",
    "            print(\"NOT IMPLEMENTED!\")\n",
    "\n",
    "\n",
    "    # Shuffle training data\n",
    "    p = np.random.permutation( trainData.shape[0] )\n",
    "    trainData = trainData[p]\n",
    "    trainScores = trainScores[p]\n",
    "    \n",
    "    if buildEnv:\n",
    "        print( \"test:  \", testData.shape )\n",
    "        print( \"train: \", trainData.shape )    \n",
    "    \n",
    "    return trainData, trainScores, testData, testScores\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    if 'dl-s2' in args.data_loc:\n",
    "        X, Y, Xval, Yval = loadData3(args)\n",
    "        \n",
    "    elif args.tid != 'all':\n",
    "        X, Y, Xval, Yval = loadData( args )\n",
    "    else:\n",
    "        X, Y, Xval, Yval = loadData2( args )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb57e727",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "888feae6-e8c9-4839-8c82-13633be4c5e4",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70c601-ffe6-4238-bd92-34a5ccd7b8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 4, 4, 1280)       4049571   \n",
      "                                                                 \n",
      " Flatten_Layer (Flatten)     (None, 20480)             0         \n",
      "                                                                 \n",
      " relu_0 (Dense)              (None, 32)                655392    \n",
      "                                                                 \n",
      " relu_1 (Dense)              (None, 32)                1056      \n",
      "                                                                 \n",
      " relu_2 (Dense)              (None, 32)                1056      \n",
      "                                                                 \n",
      " relu_3 (Dense)              (None, 32)                1056      \n",
      "                                                                 \n",
      " relu_4 (Dense)              (None, 32)                1056      \n",
      "                                                                 \n",
      " relu_5 (Dense)              (None, 32)                1056      \n",
      "                                                                 \n",
      " relu_6 (Dense)              (None, 32)                1056      \n",
      "                                                                 \n",
      " relu_7 (Dense)              (None, 32)                1056      \n",
      "                                                                 \n",
      " Output_Sigmoid (Dense)      (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,712,388\n",
      "Trainable params: 4,670,365\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def buildModel( args, X, Y ):\n",
    "    \n",
    "    # Preprocess some input arguments\n",
    "    if args.pool == 'None': args.pool = None\n",
    "    if args.weights == 'None': args.weights = None\n",
    "    \n",
    "    # Build input layer+\n",
    "    x = keras.layers.Input(shape=X.shape[1:], name='Input_Layer')\n",
    "\n",
    "    # Fix bug saving EffecientNet models with tensorflow >= 2.10.0\n",
    "    # Obtained from https://github.com/keras-team/keras/issues/17199\n",
    "    import math\n",
    "    IMAGENET_STDDEV_RGB = [0.229, 0.224, 0.225]\n",
    "    IMAGENET_STDDEV_RGB = [1/math.sqrt(i) for i in IMAGENET_STDDEV_RGB]\n",
    "    x = keras.layers.Rescaling(IMAGENET_STDDEV_RGB)(x)\n",
    "    \n",
    "    y = x    \n",
    "    \n",
    "    # What type of model\n",
    "    if 'efficientNet' in args.model:\n",
    "    \n",
    "        if   args.model == 'efficientNetB0':  core_model = tf.keras.applications.EfficientNetB0\n",
    "        elif args.model == 'efficientNetB1':  core_model = tf.keras.applications.EfficientNetB1\n",
    "        elif args.model == 'efficientNetB2':  core_model = tf.keras.applications.EfficientNetB2\n",
    "        elif args.model == 'efficientNetB3':  core_model = tf.keras.applications.EfficientNetB3\n",
    "        elif args.model == 'efficientNetB4':  core_model = tf.keras.applications.EfficientNetB4\n",
    "        elif args.model == 'efficientNetB5':  core_model = tf.keras.applications.EfficientNetB5\n",
    "        elif args.model == 'efficientNetB6':  core_model = tf.keras.applications.EfficientNetB6\n",
    "        elif args.model == 'efficientNetB7':  core_model = tf.keras.applications.EfficientNetB7\n",
    "            \n",
    "        core_model =  core_model(\n",
    "                include_top=False,\n",
    "                weights=args.weights,\n",
    "                input_shape=y.shape[1:],\n",
    "                pooling=args.pool,\n",
    "            )\n",
    "    \n",
    "    elif args.model == 'resnet':\n",
    "        # Build resnet layer without top layer\n",
    "        core_model = keras.applications.ResNet50V2(\n",
    "            include_top = False,\n",
    "            weights = args.weights,\n",
    "            input_shape = y.shape[1:], \n",
    "        )\n",
    "    else:\n",
    "        print (\"NO MODEL TYPE SELECTED\")\n",
    "        return None\n",
    "    \n",
    "    # Add core model\n",
    "    y = core_model(y)\n",
    "    \n",
    "    # Flatten for final layers\n",
    "    y = keras.layers.Flatten(name='Flatten_Layer')(y)\n",
    "        \n",
    "    for i in range( args.f_depth ):\n",
    "        \n",
    "        if args.f_activation == None:\n",
    "            y = keras.layers.Dense( args.f_width, activation= keras.activations.relu, name='relu_%d'%i )(y)\n",
    "            \n",
    "        if args.f_activation == 'relu':\n",
    "            y = keras.layers.Dense( args.f_width, activation= keras.activations.relu, name='relu_%d'%i )(y)\n",
    "            \n",
    "        if args.f_activation == 'tanh':\n",
    "            y = keras.layers.Dense( args.f_width, activation= keras.activations.tanh, name='tanh_%d'%i )(y)\n",
    "    \n",
    "    # Final layer.\n",
    "    if args.output_activation == None or args.output_activation == 'None' or args.output_activation == 'linear':\n",
    "        y = keras.layers.Dense( Y.shape[1], name='Output_Linear' )(y)\n",
    "        \n",
    "    elif args.output_activation == 'softmax':\n",
    "        y = keras.layers.Dense( Y.shape[1] , activation='softmax', name='Output_Softmax' )(y)\n",
    "        \n",
    "    elif args.output_activation == 'sigmoid':\n",
    "        y = keras.layers.Dense( Y.shape[1] , activation='sigmoid', name='Output_Sigmoid' )(y)\n",
    "\n",
    "\n",
    "    # Compile\n",
    "    model = keras.Model( x, y )\n",
    "    model.compile( \n",
    "        optimizer=keras.optimizers.Adam( learning_rate = args.learning_rate ),\n",
    "        loss=keras.losses.mean_squared_error,\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "# end building model\n",
    "    \n",
    "with strategy.scope(): \n",
    "\n",
    "    # If given model location\n",
    "    if args.model_loc != \"None\":\n",
    "        model = keras.models.load_model( args.model_loc )\n",
    "        model.summary(expand_nested=False)\n",
    "\n",
    "    else:\n",
    "        # If not reading previous model, make fresh model\n",
    "        model = buildModel( args, X, Y )\n",
    "        model.summary(expand_nested=False)\n",
    "\n",
    "# keras.utils.plot_model(model,show_shapes=True,expand_nested=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_affine( img_in, ):\n",
    "    img = np.copy(img_in)\n",
    "\n",
    "    # Grab 4 random values to warp image\n",
    "    r = random_float_array = np.random.uniform( 0.8, 1.2, size=(2,2))\n",
    "\n",
    "    # starting locations of triangle corners\n",
    "    s = img.shape\n",
    "    ls = np.array( [[s[0]/2, s[1]/2], [ s[0], s[1]/2], [s[0]/2, s[1] ]] ).astype(np.float32)  # center, top, right\n",
    "\n",
    "    # destination locations of triage corners\n",
    "    ld = np.copy( ls )\n",
    "    ld[1] *= r[0]\n",
    "    ld[2] *= r[1]   \n",
    "\n",
    "    # Build warp matrix and apply warp\n",
    "    warp_mat = cv2.getAffineTransform( ls, ld )\n",
    "    wImg = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    return wImg\n",
    "\n",
    "if buildEnv and showPlots:\n",
    "    import cv2, random\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create 10 warped images\n",
    "    img = X[0]\n",
    "    imgList = [ img ]\n",
    "    for i in range(9):\n",
    "        imgList.append( warp_affine( img ) )\n",
    "\n",
    "    # Set plot size\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "    # Plot 10 images\n",
    "    fig, axs = plt.subplots(2,5)\n",
    "    for i in range(10):\n",
    "        axs[i//5,i%5].imshow( imgList[i].astype( np.uint8 ), cmap='gray' )\n",
    "        axs[i//5,i%5].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img_in):\n",
    "    img = np.copy(img_in)\n",
    "    var = np.random.uniform( 0, 5 )\n",
    "    dev = var*np.random.uniform( 0, var, size=(3))\n",
    "    noise = np.zeros( img.shape )\n",
    "    for i in range(3):\n",
    "        noise[:,:,i] = np.random.normal(0, dev[i], (img.shape[0], img.shape[1]))\n",
    "    img += noise\n",
    "    img = np.clip( img, 0., 255. )\n",
    "    return img\n",
    "\n",
    "\n",
    "if buildEnv and showPlots:\n",
    "    import random\n",
    "    import numpy as np\n",
    "\n",
    "    # Load a single image as our example\n",
    "    from keras.preprocessing import image\n",
    "    img = X[10,:,:]\n",
    "\n",
    "    # Add noise to image\n",
    "    nImg = add_noise( img )\n",
    "\n",
    "    # Plot both images\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    axs[0].imshow( img.astype( np.uint8 ), cmap='gray' )\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow( nImg.astype( np.uint8 ), cmap='gray' )\n",
    "    axs[1].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc74a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def offset_img( img_in, std_dev=2 ):\n",
    "    img = np.copy(img_in)\n",
    "\n",
    "    # starting locations of triangle corners\n",
    "    s = img.shape\n",
    "    ls = np.array( [[s[0]/2, s[1]/2], [ s[0], s[1]/2], [s[0]/2, s[1] ]] ).astype(np.float32)  # center, top, right\n",
    "\n",
    "    # Build warp matrix and apply warp on each image\n",
    "    for i in range(3):\n",
    "\n",
    "        # destination locations of triage corners\n",
    "        ld = np.copy( ls )\n",
    "\n",
    "        # Pixel offset of each channel\n",
    "        r = np.random.normal( 0, std_dev, size=(2))\n",
    "        for i in range(3):\n",
    "            ld[i] += r\n",
    "\n",
    "        warp_mat = cv2.getAffineTransform( ls, ld )\n",
    "        img[:,:,i] = cv2.warpAffine(img[:,:,i], warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "if buildEnv and showPlots:\n",
    "    import cv2, random\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load a single image as our example\n",
    "    img = X[0,:,:]\n",
    "\n",
    "    \n",
    "    # Create 10 warped images\n",
    "    imgList = [ img ]\n",
    "    for i in range(3):\n",
    "        imgList.append( offset_img( img, std_dev = 10 ) )\n",
    "\n",
    "    # Set plot size\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "    # Plot 4 images\n",
    "    fig, axs = plt.subplots(2,2)\n",
    "    for i in range(4):\n",
    "        axs[i//2,i%2].imshow( imgList[i].astype( np.uint8 ) )\n",
    "        axs[i//2,i%2].axis('off')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "538d5006-f7ee-4238-8555-991c52462321",
   "metadata": {},
   "source": [
    "## Prepare Data Generator and Initialize Results File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60d75a-782d-49fe-89ab-8da7fc28e4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 14:24:56.496833: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_6371\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:4\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2023-05-19 14:24:56.557966: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 903ms/step\n",
      "Starting Predictions:  [0.5017071  0.50073457 0.50045645 0.5038528 ]\n",
      "Using Data Generator with Noise:\n",
      "Ready to Train:\n",
      "runName: dl-s2-test\n",
      "start: 0\n",
      "goal: 1\n",
      "steps: 1\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope(): \n",
    "\n",
    "    # Build custom image modification function\n",
    "    def mod_image( img ):\n",
    "        if args.noise:  img = add_noise( img )\n",
    "        if args.warp_image:  img = warp_affine( img )\n",
    "        if args.channel_offset:  img = offset_img( img )\n",
    "        return img\n",
    "        \n",
    "    # Quick prediction to test functionality\n",
    "    if buildEnv: \n",
    "        print( \"Starting Predictions: \", model.predict( X[:4] )[:,0] )\n",
    "        \n",
    "    if args.data_gen: \n",
    "        print( \"Using Data Generator with Noise:\" )\n",
    "\n",
    "        data_generator = keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range=180,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            preprocessing_function=mod_image,\n",
    "        )\n",
    "    else: \n",
    "        print( \"Data As Is:\" )\n",
    "        data_generator = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "    dg_trainer = data_generator.flow( \n",
    "            X, Y, \n",
    "            batch_size = args.batch_size\n",
    "        )\n",
    "    \n",
    "    allLoss = []\n",
    "    valLoss = []\n",
    "\n",
    "    trainCorr = []\n",
    "    testCorr = []\n",
    "    extCorr = []\n",
    "    \n",
    "    print( 'Ready to Train:' )\n",
    "    print( 'runName: %s' % args.runName )\n",
    "    print( 'start: %d' % args.start )\n",
    "    print( 'goal: %d' % args.stop )\n",
    "    print( 'steps: %d' % args.num_epochs )\n",
    "\n",
    "    with open( 'results/%s.txt' % args.runName, 'w' ) as f: \n",
    "\n",
    "        print( 'runName: %s' % args.runName, file=f )\n",
    "        print( 'start: %d' % args.start, file=f )\n",
    "        print( 'goal: %d' % args.stop, file=f )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "addb7788-68a4-4463-8cd3-59a29749b115",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2935e3-e619-4814-81ff-6128b6ebb89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.keras_tuner == \"False\" or args.keras_tuner == False:\n",
    "\n",
    "    with strategy.scope():\n",
    "        \n",
    "        # Initialize iter\n",
    "        i = args.start   \n",
    "        \n",
    "        while i < args.stop:\n",
    "\n",
    "            history = model.fit(\n",
    "            dg_trainer,\n",
    "                validation_data = ( Xval, Yval ),\n",
    "                epochs     = args.num_epochs,\n",
    "                verbose    = args.verbose,\n",
    "            )\n",
    "\n",
    "            i += args.num_epochs\n",
    "            timeid = int( time.time() )\n",
    "            \n",
    "            # Save loss\n",
    "            allLoss = np.append( allLoss, history.history['loss'] )\n",
    "            valLoss = np.append( valLoss, history.history['val_loss'] )\n",
    "\n",
    "            # Calculate Correlations\n",
    "            pTrainScores = model.predict( X )\n",
    "            pTestScores = model.predict( Xval )\n",
    "\n",
    "            trainCorr = np.corrcoef(  Y[:,0], pTrainScores[:,0] )[0,1]\n",
    "            testCorr = np.corrcoef(  Yval[:,0], pTestScores[:,0] )[0,1]\n",
    "            \n",
    "            # Print Progress\n",
    "            print( 'Progress: %d - %d' % ( i, args.stop ) )\n",
    "\n",
    "            # Print Loss\n",
    "            print( \"Training Loss:\",*[\"%.8f\"%(x) for x in history.history['loss']])    \n",
    "            print( \"Test Loss:\",*[\"%.8f\"%(x) for x in history.history['val_loss']])\n",
    "\n",
    "            # external loss\n",
    "            if type(Xext) != type(None): \n",
    "\n",
    "                # Get external loss\n",
    "                extLoss = model.evaluate( Xext, Yext, verbose=0 )\n",
    "                print( \"External Loss:\", extLoss )\n",
    "\n",
    "            # Print correlations\n",
    "            print( \"Train Correlation:\", trainCorr )\n",
    "            print( \"Test Correlation:\", testCorr )\n",
    "            \n",
    "            # If also using external validation\n",
    "            if type(Xext) != type(None): \n",
    "\n",
    "                # Get external correlation\n",
    "                pExtScores = model.predict( Xext )\n",
    "                extCorr = np.corrcoef(  Yext[:], pExtScores[:,0] )[0,1]\n",
    "                print( \"External Correlation:\", extCorr )\n",
    "\n",
    "\n",
    "            with open( 'results/%s.txt' % args.runName, 'a' ) as f: \n",
    "\n",
    "                print( 'Progress: %d - %d' % ( i, timeid ), file=f )\n",
    "                print( \"Training accuracy:\",*[\"%.8f\"%(x) for x in history.history['loss']], file=f)    \n",
    "                print( \"Test accuracy:\",*[\"%.8f\"%(x) for x in history.history['val_loss']],file=f)\n",
    "                print( \"Train Correlation:\", trainCorr, file=f )\n",
    "                print( \"Test Correlation:\", testCorr, file=f )\n",
    "                if type(Xext) != type(None): \n",
    "    \n",
    "                    print( \"External Correlation:\", extCorr, file=f )\n",
    "            # End writing to results file\n",
    "            \n",
    "            # Save model\n",
    "            if args.save_model: \n",
    "                model.save( 'models/%s-%s-%s.h5' % (args.runName, str(timeid), str(i)), save_format='h5' )\n",
    "                \n",
    "        # End while Training\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf9e6a40-241c-4dfd-8eb3-3c777404ac6b",
   "metadata": {},
   "source": [
    "## View Ongoing Loss and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf0c5b-62d0-459b-81ee-8cd93444b901",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buildEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m buildEnv \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mkeras_tuner \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m      2\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m( history\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mkeys() )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'buildEnv' is not defined"
     ]
    }
   ],
   "source": [
    "if buildEnv and args.keras_tuner == \"False\": \n",
    "    import matplotlib.pyplot as plt\n",
    "    print( history.history.keys() )\n",
    "    plt.plot( allLoss )\n",
    "    plt.plot( valLoss )\n",
    "    plt.title (\"Loss Thus Far\")\n",
    "    plt.legend(['Training','Validation'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel ('MSE Loss')\n",
    "    #plt.ylim([0,1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf4d87-25aa-4e7c-ac87-ed5aea23aec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buildEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m buildEnv \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mkeras_tuner \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m      3\u001b[0m     pTestScores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict( Xval )\n\u001b[1;32m      4\u001b[0m     pTrainScores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict( X )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'buildEnv' is not defined"
     ]
    }
   ],
   "source": [
    "if buildEnv and args.keras_tuner == \"False\": \n",
    "    \n",
    "    pTestScores = model.predict( Xval )\n",
    "    pTrainScores = model.predict( X )\n",
    "\n",
    "    print( Y.shape, Yval.shape )\n",
    "    print( pTestScores.shape, pTrainScores.shape )\n",
    "\n",
    "    trainCorr = np.corrcoef(  Y[:,0], pTrainScores[:,0] )[0,1]\n",
    "    testCorr = np.corrcoef(  Yval[:,0], pTestScores[:,0] )[0,1]\n",
    "    print( trainCorr, testCorr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd5e3b-4ddb-4bcf-aabe-f371f4219b00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'showPlots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m showPlots \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mkeras_tuner \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m      3\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m4\u001b[39m,\u001b[39m4\u001b[39m), dpi\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m)\n\u001b[1;32m      4\u001b[0m     plt\u001b[39m.\u001b[39mscatter( Y[:,\u001b[39m0\u001b[39m], pTrainScores, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining data\u001b[39m\u001b[39m'\u001b[39m ) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'showPlots' is not defined"
     ]
    }
   ],
   "source": [
    "if showPlots and args.keras_tuner == \"False\": \n",
    "\n",
    "    plt.figure(figsize=(4,4), dpi=300)\n",
    "    plt.scatter( Y[:,0], pTrainScores, label='Training data' ) \n",
    "    plt.scatter( Yval[:,0], pTestScores, label='Validation data' )\n",
    "    \n",
    "    plt.xlabel('Human Fitness Scores')\n",
    "    plt.ylabel('Predictons')\n",
    "    plt.title(\"%s\\nValidation Correlation: %.4f\"% (args.runName, testCorr))\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.gca().set_aspect('equal')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0ed485d-3cdb-4cd2-ab36-fab28a6cdaf1",
   "metadata": {},
   "source": [
    "# Continue Training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba8a83-5862-41b4-bf1c-6450e8903d74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mwith\u001b[39;00m strategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m buildEnv \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mkeras_tuner \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m      5\u001b[0m         scale \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strategy' is not defined"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    if buildEnv and args.keras_tuner == \"False\": \n",
    "    \n",
    "        scale = 1.0\n",
    "        args.num_epochs = 10\n",
    "        print( 'LR Before: %f' % args.learning_rate )\n",
    "        args.learning_rate *= scale\n",
    "        print( \"LR After: % \", args.learning_rate )\n",
    "        keras.backend.set_value( \n",
    "            model.optimizer.learning_rate, \n",
    "            float( args.learning_rate ) \n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "           dg_trainer,\n",
    "            validation_data = ( Xval, Yval ),\n",
    "            epochs     = args.num_epochs,\n",
    "            verbose    = args.verbose,\n",
    "        )\n",
    "\n",
    "        # Save loss\n",
    "        allLoss = np.append( allLoss, history.history['loss'] )\n",
    "        valLoss = np.append( valLoss, history.history['val_loss'] ) \n",
    "\n",
    "        if args.save_model: \n",
    "            model.save( 'models/%s-%s-%s.h5' % (args.runName, str(timeid), str(i)), save_format='h5' )\n",
    "\n",
    "        with open( 'results/%s.txt' % args.runName, 'a' ) as f: \n",
    "\n",
    "            print( 'Progress: %d - %d' % ( i, timeid ), file=f )\n",
    "            print( \"Validation accuracy:\",*[\"%.8f\"%(x) for x in history.history['loss']], file=f)    \n",
    "            print( \"Test accuracy:\",*[\"%.8f\"%(x) for x in history.history['val_loss']],file=f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9476d2fd-3e69-497c-820a-a1ddff26efc7",
   "metadata": {},
   "source": [
    "# Save Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0100ab-c680-4c01-8027-1179465aea25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buildEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m buildEnv \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39msave_model \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mkeras_tuner \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m      2\u001b[0m     model\u001b[39m.\u001b[39msave( \u001b[39m'\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mrunName , save_format\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mh5\u001b[39m\u001b[39m'\u001b[39m )\n\u001b[1;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m( \u001b[39m'\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mrunName , \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m ) \u001b[39mas\u001b[39;00m f: \n",
      "\u001b[0;31mNameError\u001b[0m: name 'buildEnv' is not defined"
     ]
    }
   ],
   "source": [
    "   \n",
    "if buildEnv and args.save_model and args.keras_tuner == \"False\": \n",
    "    model.save( 'models/%s.h5' % args.runName , save_format='h5' )\n",
    "\n",
    "    with open( 'results/%s.txt' % args.runName , 'w' ) as f: \n",
    "\n",
    "        print( 'Progress: %d - %d' % ( i, timeid ), file=f )\n",
    "        print( \"Validation accuracy:\",*[\"%.8f\"%(x) for x in allLoss], file=f)    \n",
    "        print( \"Test accuracy:\",*[\"%.8f\"%(x) for x in valLoss],file=f)\n",
    "\n",
    "        \n",
    "print(\"Model and Results Saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e14f3751-8b90-4fa3-bb5c-c1af30269b87",
   "metadata": {},
   "source": [
    "## Keras Tuner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e77062-dc16-401f-9933-cbe4f0ccac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Tuner: \n",
      "INFO:tensorflow:Reloading Tuner from tuner/untitled_project/tuner0.json\n",
      "Tuner Searching: \n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1.322e-05         |8.1323e-05        |lr\n",
      "avg               |max               |pool\n",
      "3                 |1                 |f_depth\n",
      "40                |24                |f_width\n",
      "relu              |relu              |f_activation\n",
      "sigmoid           |linear            |output_activation\n",
      "efficientNetB1    |efficientNetB1    |model\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "2                 |2                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Parsing Tuner Args\n",
      "Tuner Model:  <keras.engine.functional.Functional object at 0x7fdbccade740>\n",
      "Epoch 1/2\n",
      "115/132 [=========================>....] - ETA: 5s - loss: 0.1027"
     ]
    }
   ],
   "source": [
    "def build( hp ):\n",
    "    \n",
    "    cmdStr = ''\n",
    "    cmdStr += ' -runName %s' % args.runName\n",
    "    cmdStr += ' -start %d' % args.start\n",
    "    cmdStr += ' -stop %d' % args.stop\n",
    "    cmdStr += ' -num_epochs %d' % args.num_epochs\n",
    "    \n",
    "    cmdStr += ' -learning_rate %f' % hp.Float(\"lr\", default=0.0001, min_value=1e-6, max_value=.01, sampling=\"log\")\n",
    "    cmdStr += ' -pool %s'          % hp.Choice(\"pool\", [ 'None', 'avg', 'max' ], default=None,  )\n",
    "    cmdStr += ' -f_depth %d'       % hp.Int( 'f_depth', default=3, min_value=1, max_value=8, step=1 )\n",
    "    cmdStr += ' -output_activation %s' % hp.Choice( 'output_activation', default='linear', \\\n",
    "                                                   values=[ 'linear', 'sigmoid', 'softmax', 'None' ] )\n",
    "\n",
    "    \n",
    "    print (\"Parsing Tuner Args\")\n",
    "    tArgs = parser.parse_args(cmdStr.split())\n",
    "\n",
    "    tModel = buildModel( tArgs, X, Y )\n",
    "    print ('Tuner Model: ', tModel )\n",
    "    \n",
    "    return tModel\n",
    "    \n",
    "if args.keras_tuner == 'True':\n",
    "    \n",
    "    import keras_tuner as kt\n",
    "\n",
    "    with strategy.scope():\n",
    "\n",
    "        print (\"Building Tuner: \")\n",
    "        hp = kt.HyperParameters()\n",
    "\n",
    "        tuner = kt.Hyperband(\n",
    "             build,\n",
    "             objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "             max_epochs=args.stop,\n",
    "             factor=5,\n",
    "             hyperband_iterations=5,\n",
    "             directory='tuner',\n",
    "             project_name=args.runName)\n",
    "\n",
    "        print (\"Tuner Searching: \", )\n",
    "        tuner.search( dg_trainer, epochs=args.num_epochs, validation_data=(Xval, Yval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414ee8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
