{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41903b7-c553-4b32-8059-c5040dbd0304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFile: tng_find_targets.ipynb\\nAuthor: Matthew Ogden\\nEmail: ogdenm12@gmail.com\\nGithub: mbogden\\nCreated: 2023-Nov-09\\n\\nDescription: \\n    This code is designed to interact with the IllustrisTNG Simulation Data and related catalogs. \\n    It's goal is to identify close interactions/mergers between two galaxies.\\n\\nReferences:  \\n- TNG50 Simulation Data\\n    - Nelson, D. et al. (2015). The Illustris Simulation: Public Data Release. Astronomy and Computing, 13, 12–37. https://doi.org/10.1016/j.ascom.2015.09.003\\n    - Nelson, D. et al. (2019). First results from the TNG50 simulation: Galactic outflows driven by supernovae and black hole feedback. Monthly Notices of the Royal Astronomical Society, 490(3), 3234–3261. https://doi.org/10.1093/mnras/stz2306\\n    - Pillepich, A. et al. (2019). First results from the TNG50 simulation: The evolution of stellar and gaseous discs across cosmic time. Monthly Notices of the Royal Astronomical Society, 490(3), 3196–3233. https://doi.org/10.1093/mnras/stz2338\\n    \\n- Subhalo Morphology (Deep Learning)\\n    - Huertas-Company, M.et al. (2019). The Hubble Sequence at $z\\\\sim0$ in the IllustrisTNG simulation with deep learning. Monthly Notices of the Royal Astronomical Society, 489(2), 1859–1879. https://doi.org/10.1093/mnras/stz2191\\n    - Varma, S., Huertas-Company, M., Pillepich, A., Nelson, D., Rodriguez-Gomez, V., Dekel, A., Faber, S. M., Iglesias-Navarro, P., Koo, D. C., & Primack, J. (2021). The building up of observed stellar scaling relations of massive galaxies and the connection to black hole growth in the TNG50 simulation. Monthly Notices of the Royal Astronomical Society, 509(2), 2654–2673. https://doi.org/10.1093/mnras/stab3149\\n    \\nAcknowledgements:    \\n- Sections of this code were written with the assistance of ChatGPT made by OpenAI.\\n\\n- The IllustrisTNG simulations were undertaken with compute time awarded by \\n    the Gauss Centre for Supercomputing (GCS) under GCS Large-Scale Projects GCS-ILLU \\n    and GCS-DWAR on the GCS share of the supercomputer Hazel Hen \\n    at the High Performance Computing Center Stuttgart (HLRS), \\n    as well as on the machines of the Max Planck Computing and Data Facility (MPCDF) in Garching, Germany.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "File: tng_find_targets.ipynb\n",
    "Author: Matthew Ogden\n",
    "Email: ogdenm12@gmail.com\n",
    "Github: mbogden\n",
    "Created: 2023-Nov-09\n",
    "\n",
    "Description: \n",
    "    This code is designed to interact with the IllustrisTNG Simulation Data and related catalogs. \n",
    "    It's goal is to identify close interactions/mergers between two galaxies.\n",
    "\n",
    "References:  \n",
    "- TNG50 Simulation Data\n",
    "    - Nelson, D. et al. (2015). The Illustris Simulation: Public Data Release. Astronomy and Computing, 13, 12–37. https://doi.org/10.1016/j.ascom.2015.09.003\n",
    "    - Nelson, D. et al. (2019). First results from the TNG50 simulation: Galactic outflows driven by supernovae and black hole feedback. Monthly Notices of the Royal Astronomical Society, 490(3), 3234–3261. https://doi.org/10.1093/mnras/stz2306\n",
    "    - Pillepich, A. et al. (2019). First results from the TNG50 simulation: The evolution of stellar and gaseous discs across cosmic time. Monthly Notices of the Royal Astronomical Society, 490(3), 3196–3233. https://doi.org/10.1093/mnras/stz2338\n",
    "    \n",
    "- Subhalo Morphology (Deep Learning)\n",
    "    - Huertas-Company, M.et al. (2019). The Hubble Sequence at $z\\sim0$ in the IllustrisTNG simulation with deep learning. Monthly Notices of the Royal Astronomical Society, 489(2), 1859–1879. https://doi.org/10.1093/mnras/stz2191\n",
    "    - Varma, S., Huertas-Company, M., Pillepich, A., Nelson, D., Rodriguez-Gomez, V., Dekel, A., Faber, S. M., Iglesias-Navarro, P., Koo, D. C., & Primack, J. (2021). The building up of observed stellar scaling relations of massive galaxies and the connection to black hole growth in the TNG50 simulation. Monthly Notices of the Royal Astronomical Society, 509(2), 2654–2673. https://doi.org/10.1093/mnras/stab3149\n",
    "    \n",
    "Acknowledgements:    \n",
    "- Sections of this code were written with the assistance of ChatGPT made by OpenAI.\n",
    "\n",
    "- The IllustrisTNG simulations were undertaken with compute time awarded by \n",
    "    the Gauss Centre for Supercomputing (GCS) under GCS Large-Scale Projects GCS-ILLU \n",
    "    and GCS-DWAR on the GCS share of the supercomputer Hazel Hen \n",
    "    at the High Performance Computing Center Stuttgart (HLRS), \n",
    "    as well as on the machines of the Max Planck Computing and Data Facility (MPCDF) in Garching, Germany.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186b202-41e4-4e51-b426-9452ff296c0c",
   "metadata": {},
   "source": [
    "# Finding Galaxy Mergers within the IllustrisTNG Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d3e73-43f8-40d5-a66e-4b60c38ec7b5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffaf7935-2431-4286-ba29-0ddb848dfedd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Done\n"
     ]
    }
   ],
   "source": [
    "# ================================ IMPORTS ================================ #\n",
    "import os, argparse, h5py\n",
    "import numpy as np, pandas as pd, scipy.signal\n",
    "import matplotlib.pyplot as plt \n",
    "import illustris_python as il\n",
    "import tng_functions as tf\n",
    "\n",
    "print(\"Imports Done\")\n",
    "\n",
    "# Global variables\n",
    "SIM_DIR = '/home/tnguser/sims.TNG/TNG50-1/output/'\n",
    "\n",
    "# A useful fucntion I often use for indented printing\n",
    "def tabprint( printme, start = '\\t - ', end = '\\n' ):\n",
    "    print( start + str(printme), end = end )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cc9a5-46ed-44a4-9f2a-f818722fd23d",
   "metadata": {},
   "source": [
    "---\n",
    "## Command Line Arguments\n",
    "\n",
    "This is written in JupyterLab, and will be compiled and ran in python for faster execution.  This will define the possible input command line arguements.\n",
    "\n",
    "\n",
    "WARNING:  I have not been consistent with implementing and following arguments.  Code still in indevlopment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aad8ad-d829-43e3-81cb-950629bab0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: Defined\n"
     ]
    }
   ],
   "source": [
    "# This argument decides if code is in python or jupyter.\n",
    "buildEnv = False\n",
    "\n",
    "# Define argument parser function \n",
    "def initParser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument( '-s', '--simDir', default = '/home/tnguser/sims.TNG/TNG50-1/output/',  type=str, \\\n",
    "                        help=\"Base directory for a single simulation on the IllustrisTNG servers.\")   \n",
    "    \n",
    "    parser.add_argument( '-n', '--simName', default = 'TNG50-1',  type=str, \\\n",
    "                        help=\"Name for the simulation being worked on.\")\n",
    "    \n",
    "    parser.add_argument( '-o', '--overwrite', default = False,  type=bool, \\\n",
    "                        help=\"Overwrite output files?  If false, will check if output file exists before beginning time-consuming tasks.\")\n",
    "    \n",
    "    parser.add_argument( '-t', '--trim', default = -1,  type=int, \\\n",
    "                        help=\"Default number of subhalos to consider, sorted by highest mass first.\")\n",
    "    \n",
    "    parser.add_argument( '-f', '--function', default = 'None', type=str, \\\n",
    "                        help=\"Default function program will be executing.\")\n",
    "    \n",
    "    parser.add_argument( '-d', '--dataDir', default = 'data', type=str, \\\n",
    "                        help=\"Default location to store misc data files.\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = initParser()\n",
    "print(\"Args: Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28ddf0-65da-4a9f-950a-20619ed0afe7",
   "metadata": {},
   "source": [
    "## To Python? Or to JupyterLab? \n",
    "This will establish if this is being run in a JupyterLab environment or from Command Line in Python. \n",
    "\n",
    "NOTE:  If you're running this in Jupyter, modify the `cmdStr` below to whatever variables you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3bad9c-5048-4aaa-b254-fdb542bc0b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Building Environment\n",
      "CMD Line: \n",
      "\t$: python3 targets-working.py --trim 10 --dataDir tng-data\n",
      "Args: Read\n",
      "Namespace(simDir='/home/tnguser/sims.TNG/TNG50-1/output/', simName='TNG50-1', overwrite=False, trim=10, function='None', dataDir='tng-data')\n"
     ]
    }
   ],
   "source": [
    "# Am I in a jupyter notebook?\n",
    "try:\n",
    "    \n",
    "    # This command is NOT available in a python script\n",
    "    get_ipython().__class__.__name__\n",
    "    buildEnv = True\n",
    "    print (\"In Building Environment\")\n",
    "    \n",
    "    # Command Line Arguments\n",
    "    cmdStr  = 'python3 targets-working.py'\n",
    "    cmdStr += ' --trim 10'\n",
    "    cmdStr += ' --dataDir tng-data'\n",
    "    \n",
    "    # Read string as if command line\n",
    "    print( \"CMD Line: \\n\\t$:\", cmdStr)\n",
    "    \n",
    "    # This function doesn't like the 'python3 file.py' part.\n",
    "    args = parser.parse_args(cmdStr.split()[2:])\n",
    "\n",
    "# Or am I in a python script?\n",
    "except:\n",
    "    \n",
    "    # Read CMD arguments\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "print( \"Args: Read\")\n",
    "print( args )\n",
    "\n",
    "# Setup data directory if not found\n",
    "os.makedirs(args.dataDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd168c99-22b6-427d-9600-7c6ea4fb4bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this locational valid?\n",
      "Simulation data: True - /home/tnguser/sims.TNG/TNG50-1/output/\n"
     ]
    }
   ],
   "source": [
    "if buildEnv: \n",
    "    # Location of one simulation\n",
    "    print(\"Is this locational valid?\")\n",
    "    print( f\"Simulation data: {os.path.exists( args.simDir )} - {args.simDir}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28feb0bf-fed5-43f1-999a-854079cc1c20",
   "metadata": {},
   "source": [
    "---\n",
    "# Halos and SubHalos\n",
    "Within the simulation, Halos are the largest set of objects that are gravitationally bound to each other, I like to think of them as galaxy clusters.  Subhalos are also gravitationally bound objects but more dense, and I suspect has to do with potential energy.   I like to think of them as galaxies, globular clusters, blobs of gas, etc.  (That's my reasoning and I'm sticking to it)\n",
    "\n",
    "\n",
    "For more information, pleas visit the IllustrisTNG Data Specification Page.  https://www.tng-project.org/data/docs/specifications/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654a989-b038-469e-98de-fc5f693396a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Mass Filter\n",
    "\n",
    "So I am looking for larger galaxies that visualize well.  I will be choosing galaxies that are between masses of 1/10th and x10 the Milky Way galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b53a6f1-2773-4834-92a7-9a50cdd9b5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mass Mask: tng-data/TNG50-1-50-mask-mass-10.npy\n",
      "(6780233,) bool\n",
      "Reading Mass Mask: tng-data/TNG50-1-67-mask-mass-10.npy\n",
      "(6244619,) bool\n"
     ]
    }
   ],
   "source": [
    "def getMassFilter( args, snapNum, mScale = 10 ):\n",
    "    \n",
    "    # Define where file will be saved\n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-mass-{mScale}.npy'\n",
    "    \n",
    "    # Read from file if it exits\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Mass Mask: {mLoc}\")\n",
    "        mass_mask = np.load( mLoc )\n",
    "        return mass_mask\n",
    "    \n",
    "    # define mass limits\n",
    "    milky_way_mass = 150.0  # in (10^10 M_⊙) \n",
    "    upper_mass = milky_way_mass * mScale\n",
    "    lower_mass = milky_way_mass / mScale\n",
    "    \n",
    "    # Pull masses for all subhalos in snapshot\n",
    "    fields = ['SubhaloMass']\n",
    "    print(\"Pulling Masses for all Subhalos\")\n",
    "    print(\"WARNING: May take a while \")\n",
    "    SubhaloMass = il.groupcat.loadSubhalos( args.simDir, snapNum, fields=fields)\n",
    "    \n",
    "    # This is the first occasion where I wi\n",
    "    \n",
    "    # Find galaxies between upper and lower mass\n",
    "    mask_mass = ( SubhaloMass[:] <= upper_mass ) & ( SubhaloMass[:] >= lower_mass )\n",
    "    \n",
    "    # Save mass\n",
    "    np.save( mLoc, mask_mass )\n",
    "    \n",
    "    return mask_mass\n",
    "    \n",
    "if buildEnv and True:\n",
    "    args.overwrite = False\n",
    "    mask_mass = getMassFilter( args, 50 )\n",
    "    print( mask_mass.shape, mask_mass.dtype )\n",
    "    mask_mass = getMassFilter( args, 67 )\n",
    "    print( mask_mass.shape, mask_mass.dtype )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea4bb7-779c-4776-9b84-9dfd74e6c2ab",
   "metadata": {},
   "source": [
    "___\n",
    "## Centrals and Satellites\n",
    "Halo's often have a central galaxy that's the largest, with smaller subhalos orbiting it called satellites.  For convenience, let's create a mask of these central galaxies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca282d15-13f8-4c83-bb26-7c66ef93bd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_mask_from_list( true_list, snapnum ):    \n",
    "    n_subhalo = il.groupcat.loadHeader( args.simDir, snapnum)['Nsubgroups_Total']\n",
    "    mask = np.full( n_subhalo, False, dtype=bool )\n",
    "    mask[true_list] = True    \n",
    "    return mask\n",
    "    \n",
    "\n",
    "def getCentralFilter( args, snapnum = 99 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapnum}-mask-central.npy'\n",
    "\n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Central Galaxy file: {mLoc}\")\n",
    "        mask_central = np.load( mLoc )\n",
    "        return mask_central\n",
    "\n",
    "    print(f\"Getting Central SubHalo IDs for sim/snapshot: {args.simName} / {snapnum}\")\n",
    "\n",
    "    # The GroupFirstSub is the subhalo id for the largest subhalo in a halo.  \n",
    "    GroupFirstSub = il.groupcat.loadHalos( args.simDir, snapnum, fields=['GroupFirstSub'])\n",
    "\n",
    "    # Filter out groups that contain no subhalos.\n",
    "    w = np.where(GroupFirstSub >= 0) # value of -1 indicates no subhalo in this group\n",
    "    central_ids = GroupFirstSub[w]\n",
    "    \n",
    "    # Expand into a full array with a value for every subhalo\n",
    "    mask_central = expand_mask_from_list( central_ids, snapnum = snapnum )\n",
    "    \n",
    "    # Save mass\n",
    "    np.save( mLoc, mask_central )\n",
    "    \n",
    "    return mask_central\n",
    "\n",
    "if buildEnv and True: \n",
    "\n",
    "    args.overwrite = False\n",
    "    mask_central = getCentralFilter( args, snapnum = 50 )\n",
    "    print('Central Galaxies:', mask_central.shape, mask_central[:10] )\n",
    "    mask_central = getCentralFilter( args, snapnum = 67 )\n",
    "    print('Central Galaxies:', mask_central.shape, mask_central[:10] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fcaa5f8-59c9-4fdb-ad69-02bf02b67e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BoxSize': 35000.0, 'FlagDoubleprecision': 0, 'Git_commit': b'd203ec8b07c7e2bdda5f608aa0babea46d603699', 'Git_date': b'Thu Apr 7 14:14:27 2016 +0200', 'HubbleParam': 0.6774, 'Ngroups_ThisFile': 1, 'Ngroups_Total': 10638943, 'Nids_ThisFile': 11555101, 'Nids_Total': 8283281991, 'Nsubgroups_ThisFile': 54051, 'Nsubgroups_Total': 6244619, 'NumFiles': 680, 'Omega0': 0.3089, 'OmegaLambda': 0.6911, 'Redshift': 0.5030475232448832, 'Time': 0.6653149581332802}\n",
      "6244619\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "# print(tmp)\n",
    "tmp2 = il.groupcat.loadHeader(args.simDir, 67)\n",
    "print( tmp2 )\n",
    "print( tmp2['Nsubgroups_Total'] )\n",
    "print( type(tmp2['Nsubgroups_Total'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ba957-ce5c-42e0-8e05-3b998a5e1c77",
   "metadata": {},
   "source": [
    "# (r) Galaxy Morphologies (Deep Learning)\n",
    "\n",
    "Because our method relies on disks of galaxies, it might be useful for us to find mergers betweeen two disk galaxies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27267df1-15b8-4654-bfc3-b233a1821373",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file: subcatalogs/TNG50-1-morphologies_deeplearn.hdf5\n",
      "Top-level headers and sizes:\n",
      "\tGroup: Header, Number of items: 0\n",
      "\tGroup: Snapshot_25, Number of items: 4\n",
      "\tGroup: Snapshot_29, Number of items: 4\n",
      "\tGroup: Snapshot_33, Number of items: 4\n",
      "\tGroup: Snapshot_40, Number of items: 4\n",
      "\tGroup: Snapshot_50, Number of items: 4\n",
      "\tGroup: Snapshot_67, Number of items: 4\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-67-mask-disk-morphology.npy\n",
      "Disk Galaxies: (6244619,) [False  True  True  True  True False  True False  True  True]\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-50-mask-disk-morphology.npy\n",
      "Disk Galaxies: (6780233,) [ True  True  True False False False False  True False False]\n"
     ]
    }
   ],
   "source": [
    "# A function to print the upper level of an HDF5 file.\n",
    "def print_HDF5_info( file_path ):\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        print( f\"HDF5 file: {file_path}\" )\n",
    "        print(\"Top-level headers and sizes:\")\n",
    "\n",
    "        # Iterate over items in the root of the file\n",
    "        for key in file.keys():\n",
    "            # Get the object (could be a group or dataset)\n",
    "            item = file[key]\n",
    "\n",
    "            # Check if the item is a group or dataset and print its size\n",
    "            if isinstance(item, h5py.Group):\n",
    "                print(f\"\\tGroup: {key}, Number of items: {len(item)}\")\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                print(f\"\\tDataset: {key}, Shape: {item.shape}\")\n",
    "    # Close file\n",
    "\n",
    "def getDiskMorphologyFilter( args, snapNum = 99 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-disk-morphology.npy'\n",
    "\n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Disk Morphology Mask: {mLoc}\")\n",
    "        mask_disk = np.load( mLoc )\n",
    "        return mask_disk\n",
    "    \n",
    "    # Check if morphology file exists.  \n",
    "    hdf5Loc = f'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5'\n",
    "    if not os.path.exists( hdf5Loc ):\n",
    "        print(\"WARNING:!  Subcatalog File missing: \", hdf5Loc )\n",
    "        print(\"Please see IllustrisTNG Data Specs for info to download this file.\")\n",
    "        raise AssertionError\n",
    "    \n",
    "    # Read the deeplearning morphology file\n",
    "    with h5py.File(f'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5', 'r') as file:\n",
    "        \n",
    "        header = f'Snapshot_{snapNum}'\n",
    "        \n",
    "        # Verify snapshot header is in file\n",
    "        if header not in file.keys():\n",
    "            print(f\"Bad HDF5 header: {header} / {file.keys()}\" )\n",
    "            return None       \n",
    "        \n",
    "        subhaloIDs      = np.array( file[header]['SubhaloID'] )\n",
    "        subhaloDiskProb = np.array( file[header]['P_Disk'] )\n",
    "        \n",
    "        print( 'test morph: ', subhaloIDs.shape )\n",
    "        \n",
    "        # Iterate through and grab subhalos with a greater chance of being a disk galaxy\n",
    "        disk_list = []\n",
    "        for i in range( subhaloIDs.shape[0] ):\n",
    "            if subhaloDiskProb[i] > 0.5:\n",
    "                disk_list.append( subhaloIDs[i] )\n",
    "        \n",
    "    # Done reading file.\n",
    "    \n",
    "    # create mask \n",
    "    mask_disk = expand_mask_from_list( np.array( disk_list ), snapNum )\n",
    "    \n",
    "    # Save mass\n",
    "    print(f\"Saving Disk Morphology Mask: {mLoc}\")\n",
    "    np.save( mLoc, mask_disk )\n",
    "    \n",
    "    return mask_disk    \n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    print_HDF5_info( 'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5' )\n",
    "    args.overwrite=False\n",
    "    mask_disk = getDiskMorphologyFilter( args, 67 )  \n",
    "    print('Disk Galaxies:', mask_disk.shape, mask_disk[:10] )\n",
    "    mask_disk = getDiskMorphologyFilter( args, 50 )  \n",
    "    print('Disk Galaxies:', mask_disk.shape, mask_disk[:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df6eee-f1ae-461b-93a1-18bb8fd7cb1c",
   "metadata": {},
   "source": [
    "## (y) Merger History\n",
    "\n",
    "Because manually detecting major mergers in the merger tree is messy (trust me, I tried), I'll be using someone else's subcatalogs to detect major mergers between galaxies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea33f1ae-0c63-4884-9707-dff96eb333db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merger History Loc: subcatalogs/MergerHistory_050.hdf5\n",
      "Saving Major Merger Mask: tng-data/TNG50-1-50-mask-major-merger-18.npy\n",
      "Disk Galaxies: (6780233,) [ True False False False False False False False  True False]\n",
      "Merger History Loc: subcatalogs/MergerHistory_067.hdf5\n",
      "Saving Major Merger Mask: tng-data/TNG50-1-67-mask-major-merger-33.npy\n",
      "Disk Galaxies: (6244619,) [ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getMajorMergerMask( args, snapNum = 67, snapCutoff=13 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-major-merger-{snapCutoff}.npy'\n",
    "    \n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Upcoming Major Merger Mask: {mLoc}\")\n",
    "        mask_merger = np.load( mLoc )\n",
    "        return mask_merger\n",
    "    \n",
    "    file_loc = f'subcatalogs/MergerHistory_0{snapNum}.hdf5'\n",
    "    \n",
    "    print( f\"Merger History Loc: {file_loc}\" )\n",
    "    \n",
    "    # Return None if no file found for snap num.\n",
    "    if not os.path.exists( file_loc ):\n",
    "        print(f\"WARNING:  Could not find file: {file_loc}\")\n",
    "        raise ValueError(f\"Subcatalog File Missing: {file_loc}\")\n",
    "        return None\n",
    "    \n",
    "    # Read Merger History file.    \n",
    "    with h5py.File(file_loc, 'r') as file:\n",
    "            \n",
    "        # Get the object (could be a group or dataset)\n",
    "        dataset = file['SnapNumNextMajorMerger']\n",
    "        \n",
    "        # Create a boolean mask for values that are non-negative and below the upper limit\n",
    "        mask_merger = (dataset[:] >= 0) & (dataset[:] <= (snapNum + snapCutoff) )\n",
    "        \n",
    "    # Saving\n",
    "    print(f\"Saving Major Merger Mask: {mLoc}\")\n",
    "    np.save( mLoc, mask_merger )\n",
    "       \n",
    "    return mask_merger\n",
    "            \n",
    "    # Find merger\n",
    "    \n",
    "# Get merger tree catalog\n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    # print_HDF5_info( f'subcatalog/MergerHistory_0{snapNum}.hdf5' )\n",
    "    \n",
    "    mask_merger = getMajorMergerMask( args, 50, 18 )\n",
    "    print('Disk Galaxies:', mask_merger.shape, mask_merger[:10] )\n",
    "    mask_merger = getMajorMergerMask( args, 67, 33 )\n",
    "    \n",
    "    print('Disk Galaxies:', mask_merger.shape, mask_merger[:10] )\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd322f-6dca-456a-8ddc-5c672d5add91",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Combine Masks Together\n",
    "\n",
    "(Matt from the future here) \n",
    "This is step one of many to narrow down potential mergers so I'm going to establish some terminology as I'm rewriting this.  potential mergers will hence forth be furthered as Mergers-of-Interest or MOI for short.  And each step will have a number, so this will be moi_1.  \n",
    "\n",
    "### MOI_1: Parent galaxy that will collide soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4b4e295-6293-4539-b2f2-9211ffd4980d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mass Mask: tng-data/TNG50-1-50-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-50-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-50-mask-major-merger-17.npy\n",
      "Reading Mass Mask: tng-data/TNG50-1-67-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-67-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-67-mask-major-merger-32.npy\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def combine_masks(mask_list):\n",
    "    \n",
    "    # Verify that all masks have the same shape\n",
    "    if not all(mask.shape == mask_list[0].shape for mask in mask_list):\n",
    "        print(\"ERROR:  Masks do not have the same shape.\")\n",
    "        for mask in mask_list:\n",
    "            print( len(mask) )\n",
    "        raise ValueError(\"ERROR: Combine Masks: All masks must have the same shape\")\n",
    "\n",
    "    # Initialize the combined mask with the first mask\n",
    "    combined_mask = mask_list[0].copy()\n",
    "\n",
    "    # Perform logical AND operation with each subsequent mask\n",
    "    for mask in mask_list[1:]:\n",
    "        combined_mask &= mask\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "def generate_moi_1_mask( args, snapNum, mass = True, massScale = 10, central = False, disk = True, major = True, majorCutoff = 13 ):\n",
    "    \n",
    "    # Create list of mask to find goi\n",
    "    mask_list = []\n",
    "    if mass:     mask_list.append( getMassFilter           ( args, snapNum, mScale = massScale ) )\n",
    "    if central:  mask_list.append( getCentralFilter        ( args, snapNum ) )\n",
    "    if disk:     mask_list.append( getDiskMorphologyFilter ( args, snapNum ) )\n",
    "    if major:    mask_list.append( getMajorMergerMask      ( args, snapNum, majorCutoff ) )\n",
    "\n",
    "    try:\n",
    "        # Get \n",
    "        combined_mask = combine_masks( mask_list )\n",
    "        goi_ids = np.where( combined_mask )        \n",
    "        return goi_ids[0]\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    \n",
    "if buildEnv and True:\n",
    "    # snapnum of range of interest\n",
    "    snap_data = { \n",
    "        50: {'start_snap':50, 'to_snap':67, 'cutoff':17}, \n",
    "        67: {'start_snap':67, 'to_snap':99, 'cutoff':32}\n",
    "    }\n",
    "    \n",
    "    for snap in snap_data:\n",
    "        \n",
    "#         # Create list of mask to find moi_1\n",
    "#         mask_list = []\n",
    "#         mask_list.append( getMassFilter( args, snap ) )\n",
    "#         mask_list.append( getCentralFilter( args, snap ) )\n",
    "#         mask_list.append( getDiskMorphologyFilter( args, snap ) )\n",
    "#         mask_list.append( getMajorMergerMask( args, snap, snapCutoff=snap_data[snap]['cutoff'] ) )\n",
    "\n",
    "#         try:\n",
    "#             combined_mask = combine_masks( mask_list )\n",
    "#             print(combined_mask.shape)\n",
    "#             print( f\"Remaining Subhalos: {np.sum( combined_mask )}\" )\n",
    "#             moi_1_subhalo_ids = np.where( combined_mask )[0]\n",
    "#             snap_data[snap]['moi_1_subhalo'] = moi_1_subhalo_ids\n",
    "#             print( f\"GOIs: {moi_1_subhalo_ids.shape}\" )\n",
    "\n",
    "#         except ValueError as e:\n",
    "#             print(e)\n",
    "\n",
    "        # Initial GOIs of interest\n",
    "        snap_data[snap]['moi_1_subhalo'] = generate_moi_1_mask( args, snap, majorCutoff = snap_data[snap]['cutoff'] )\n",
    "    \n",
    "    print(\"done\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719a15d-e5a7-4e4d-b238-0424c4002e11",
   "metadata": {},
   "source": [
    "--- \n",
    "## Find MOI_2 in Future Merger Trees\n",
    "\n",
    "Since requesting a merger tree only returns it's tree for the current moment and backwards in time, I need to jump several snapshots forward, and identify which galaxies it belongs to there.  It's a long tedious process but I'll figure it out.\n",
    "\n",
    "### MOI_2:  Child galaxy that has already undergone a major merger event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c1d2632-d8e0-4a1c-9f7a-e02ebd186840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Merger-of-Interest List: tng-data/TNG50-1-moi_2-50-67.txt\n",
      "MOI_2 50 -> 67:  202\n",
      "Reading Merger-of-Interest List: tng-data/TNG50-1-moi_2-67-99.txt\n",
      "MOI_2 67 -> 99:  229\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def find_mois_in_tree( tree_id, snapNum, moi_1_list, args ):\n",
    "    \n",
    "    # Load only SubhaloIDRaw for effecient retrieval time\n",
    "    tree = il.sublink.loadTree( args.simDir, snapNum, tree_id, fields=['SubhaloIDRaw'] )\n",
    "\n",
    "    # See if any of my MOIs are in list\n",
    "    moi_1_mask = np.isin( moi_1_list, tree )\n",
    "    n_matches = np.sum(moi_1_mask)\n",
    "\n",
    "    # If none found\n",
    "    if n_matches == 0:\n",
    "        return []\n",
    "    # Else we have results\n",
    "\n",
    "    # Get the index locations where the mask is True\n",
    "    moi_1_loc = np.where(moi_1_mask)[0]\n",
    "    moi_1_ids_in_tree = moi_1_list[ moi_1_loc ]\n",
    "\n",
    "    moi_2_list = []\n",
    "\n",
    "    for moi_1 in moi_1_ids_in_tree:\n",
    "        # Append tuple ( moi_2, moi_1 )\n",
    "        moi_2_list.append( ( tf.generate_subhalo_id_raw( snapNum, tree_id, ), moi_1 ) )\n",
    "\n",
    "    return moi_2_list\n",
    "\n",
    "def getMOI_2( args, start_snapNum, stop_snapNum ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-moi_2-{start_snapNum}-{stop_snapNum}.txt'\n",
    "    \n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Merger-of-Interest List: {mLoc}\")\n",
    "        moi_list = np.loadtxt( mLoc, dtype=int )\n",
    "        return moi_list\n",
    "    \n",
    "    moi_1_subhalo = generate_moi_1_mask( args, start_snapNum, majorCutoff = stop_snapNum-start_snapNum+1 )\n",
    "    moi_1_ids = np.array([ tf.generate_subhalo_id_raw( start_snapNum, moi_1, ) for moi_1 in moi_1_subhalo ])\n",
    "    print( f\"Search for Galaxies of Interest: {moi_1_ids.shape}\")\n",
    "    \n",
    "    # Basically just get any id from final snapshot within mass 1/12 or x12 of milky way\n",
    "    potential_moi_2_list = generate_moi_1_mask( args, stop_snapNum, \\\n",
    "                                               mass = True, massScale = 50, \\\n",
    "                                               central = False, disk = False, \\\n",
    "                                               major=False )\n",
    "    print( f\"Searching within Merger Trees: {potential_moi_2_list.shape}\" )\n",
    "    \n",
    "    moi_2_list = []\n",
    "    for i, tree_id in enumerate(potential_moi_2_list):  \n",
    "        tabprint( f\" {i} / {potential_moi_2_list.shape[0]} - {tree_id}\", end='\\r' )\n",
    "        moi_2_list.extend( find_mois_in_tree( tree_id, stop_snapNum, moi_1_ids, args ) )\n",
    "    \n",
    "    print( f\"\\nFound MOI / Tree Matches: {len( moi_2_list) }\")\n",
    "    \n",
    "    # Save list for future reference\n",
    "    moi_2_list = np.array( moi_2_list, dtype=int )\n",
    "    np.savetxt( mLoc, moi_2_list, fmt='%i', header='moi_2 moi_1 ' )\n",
    "    return moi_2_list\n",
    "\n",
    "if buildEnv and True:  \n",
    "    \n",
    "    args.overwrite = False\n",
    "    \n",
    "    moi_2_50_67_list = getMOI_2( args, 50, 67 )\n",
    "    print(\"MOI_2 50 -> 67: \", len(moi_2_50_67_list))\n",
    "    # for i in range( moi_2_50_list.shape[0] ):\n",
    "    #     print( i, moi_2_50_list[i] )\n",
    "    \n",
    "    moi_2_67_99_list = getMOI_2( args, 67, 99 )\n",
    "    print(\"MOI_2 67 -> 99: \", len(moi_2_67_99_list))\n",
    "    \n",
    "    # for i in range( moi_2_67_list.shape[0] ):\n",
    "    #     print( i, moi_2_67_list[i] )\n",
    "    \n",
    "    print(\"Done.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e231d-2782-44d5-880d-890c6afe8690",
   "metadata": {},
   "source": [
    "## MOI_3:  MOI_2 with multiple MOI_1 in their tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf1ce961-c131-4912-bbf5-5935b852133a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOI 2 Repeats:  22 56\n"
     ]
    }
   ],
   "source": [
    "def find_moi_2_repeats( moi_list ):\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    # Step 1: Extract all moi_2's from the tuples\n",
    "    moi_2_list = [mois[0] for mois in moi_list]\n",
    "\n",
    "    # Step 2: Count the occurrences of each moi_2\n",
    "    moi_2_counts = Counter(moi_2_list)\n",
    "\n",
    "    # Step 3: Filter the tuples where moi_2 appears more than once\n",
    "    moi_2_repeats = [mois for mois in moi_list if moi_2_counts[mois[0]] > 1]\n",
    "    \n",
    "    return moi_2_repeats\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    # for i in range( moi_2_50_list.shape[0] ):\n",
    "    #     print( i, moi_2_50_list[i] )\n",
    "    \n",
    "    moi_2_50_67_repeats = find_moi_2_repeats( moi_2_50_67_list )\n",
    "    moi_2_67_99_repeats = find_moi_2_repeats( moi_2_67_99_list )\n",
    "    \n",
    "    print( 'MOI 2 Repeats: ', len( moi_2_50_67_repeats ), len( moi_2_67_99_repeats ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70597fcc-01dc-4155-ad52-e929b3d5821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define print fucntion for a row\n",
    "def printRow( tree, i, fields ):\n",
    "    # if i == -1:\n",
    "    #     print(\"Invalid index\")\n",
    "    #     return\n",
    "    \n",
    "    print( \" - \".join( [ f\"{key}:{tree[key][i]}\" for key in fields ]) )\n",
    "    \n",
    "def createVisLink( subhaloIDRaw, projection = 'face', simulation='TNG50-1' ):\n",
    "        tmp = tf.deconstruct_subhalo_id_raw( subhaloIDRaw )    \n",
    "        \n",
    "        if projection == 'face':\n",
    "            link = f\"https://www.tng-project.org/api/{simulation}/snapshots/{tmp[0]}/subhalos/{tmp[1]}/vis.png?partType=stars&partField=stellarComp-jwst_f200w-jwst_f115w-jwst_f070w&size=1&method=histo&rotation=face-on&plotStyle=edged\"\n",
    "        # Else, project x,y plane\n",
    "        else:\n",
    "            link = f\"https://www.tng-project.org/api/{simulation}/snapshots/{tmp[0]}/subhalos/{tmp[1]}/vis.png?partType=stars&partField=stellarComp-jwst_f200w-jwst_f115w-jwst_f070w&size=1&method=histo&nPixels=256%2C256&axes=0%2C1&plotStyle=edged\"\n",
    "        return link\n",
    "    \n",
    "# WARNING:  CURRENTLY NOT FUNCTIONING\n",
    "def analyze_MOI( args, gois ):\n",
    "    \n",
    "    mGOI = gois[0]\n",
    "    tGOI = gois[1]\n",
    "    \n",
    "    print( 'Final Viz: ', createVisLink( tGOI ) )\n",
    "    \n",
    "    tabprint( f'Merger GOI: {mGOI}' )\n",
    "    tabprint( f'Tree   GOI: {tGOI}' )    \n",
    "    \n",
    "    # If matches found, load more info\n",
    "    fields = ['SubhaloID','NextProgenitorID','MainLeafProgenitorID','FirstProgenitorID','SubhaloGrNr',\\\n",
    "              'SubhaloIDRaw','SubhaloMass', 'RootDescendantID', 'SnapNum', 'DescendantID',\\\n",
    "              'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', ]\n",
    "    \n",
    "    tree_snapNum, tree_subhaloID = tf.deconstruct_subhalo_id_raw( tGOI )\n",
    "    goi_snapNum, goi_subhaloID = tf.deconstruct_subhalo_id_raw( mGOI )\n",
    "    \n",
    "    \n",
    "    # Load Tree with desired info\n",
    "    tree = il.sublink.loadTree( args.simDir, tree_snapNum, tree_subhaloID, fields=fields)\n",
    "    \n",
    "    # Create a dictionary to map Subhalo IDs to their index in the list\n",
    "    subhalo_index = {subhalo_id: index for index, subhalo_id in enumerate(tree['SubhaloID'])}  \n",
    "    ci = 0  # Starting index of requested subhalo/snapshot\n",
    "    \n",
    "    # Print some start info for familiarization\n",
    "    if False: \n",
    "        print(\"Printing basic info for familization\")\n",
    "        print(\"\\nChild Info\")\n",
    "        printRow( tree, ci, fields )\n",
    "\n",
    "        print(\"\\nPrimary Info\")\n",
    "        pi = subhalo_index.get( tree['FirstProgenitorID'][ci], -1 )\n",
    "        printRow( tree, pi, fields )\n",
    "\n",
    "        print(\"\\nSecondary Info\")\n",
    "        si = subhalo_index.get( tree['NextProgenitorID'][pi], -1 )\n",
    "        if si == -1:\n",
    "            print(\"No Secondary Parent\")\n",
    "        else:\n",
    "            printRow( tree, si, fields )\n",
    "    \n",
    "    \n",
    "    # Grab ids and velocity arrays of the primary galaxies throughout time. \n",
    "    pVel = np.ones( (tree_snapNum+1, 3) ) * np.nan    # Velocities\n",
    "    pIDRaw = np.zeros( ( tree_snapNum+1), dtype=int)  # SubhaloIDRaw\n",
    "        \n",
    "    while ci != -1:\n",
    "        \n",
    "        i, tmp = tf.deconstruct_subhalo_id_raw( tree['SubhaloIDRaw'][ci] )\n",
    "        \n",
    "        # Grab array values\n",
    "        pVel[i,:] = tree['SubhaloVel'][ci][:]\n",
    "        pIDRaw[i] = tree['SubhaloIDRaw'][ci]\n",
    "        \n",
    "        # Update to primary parent in previous snapshot\n",
    "        ci = subhalo_index.get( tree['FirstProgenitorID'][ci], -1 )\n",
    "\n",
    "    # Calculate the change in velocity (Δv)\n",
    "    dVel = np.diff(pVel, axis=0)\n",
    "    \n",
    "    # Calculate magnitude of acceleration at each time step (assumption Δt=1)\n",
    "    pAcc = np.sqrt( np.sum( dVel**2, axis=-1 ) )\n",
    "    snapshots = [ tf.deconstruct_subhalo_id_raw( pid )[0] for pid in pIDRaw ]\n",
    "    \n",
    "    plt.xlim( 50, 75 )\n",
    "    plt.plot( snapshots[1:], pAcc )\n",
    "    \n",
    "    # Grab peaks in acceleration after snapshot\n",
    "    cSnapshot = 55 \n",
    "    \n",
    "    # Find peaks and their prominences\n",
    "    peaks, properties = scipy.signal.find_peaks(pAcc[cSnapshot:], prominence=True)\n",
    "    peak_snapshots = peaks + cSnapshot + 1\n",
    "    prominences = properties['prominences']\n",
    "    \n",
    "    # Print or use the sorted peaks and their prominences\n",
    "    idList = []\n",
    "    print(\"\")\n",
    "    for i, sn in enumerate(peak_snapshots):\n",
    "        if prominences[i] < 5: continue\n",
    "        plt.axvline(x=peak_snapshots[i], color='r', linestyle='--', label=f'{peak_snapshots[i]} - {prominences[i]:.2f}')\n",
    "        print(f\"Peak at index {peak_snapshots[i]} with prominence {prominences[i]}: {pIDRaw[peak_snapshots[i]]} - {pIDRaw[peak_snapshots[i]+1]}\")\n",
    "        \n",
    "        \n",
    "        for j in range( peak_snapshots[i]-1, tree_snapNum, 1):\n",
    "            link = createVisLink( pIDRaw[j] ) \n",
    "            tabprint( link )\n",
    "        \n",
    "    data = {}\n",
    "    \n",
    "    plt.axvline( x=75, color='k', linestyle='--' )\n",
    "    \n",
    "    plt.xlabel('Snap Shots')\n",
    "    plt.ylabel('Acceleration Magnitude')\n",
    "    plt.title(f\"Fly-by Detection for SubhaloIDRaw: {tGOI}\")\n",
    "    plt.legend()\n",
    "    \n",
    "    return tree\n",
    "    \n",
    "\n",
    "# WARNING:  CODE HAS BEEN CHANGED AND THIS IS NOT EXPECTED TO WORK AS INTENDED ATM.\n",
    "if buildEnv and False:  \n",
    "    moi_list = getMOI_v1( args, 67, 75 )\n",
    "    \n",
    "    # tmp_tree = analyze_MOI( args, moi_list[51] )\n",
    "    \n",
    "    print( tmp_tree.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e4edaf8-8f6e-496a-8da3-e259162e23e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Snap:  57\n"
     ]
    }
   ],
   "source": [
    "def get_merger_snapshot( id_raw ):\n",
    "    \n",
    "    snap, subhalo_id = tf.deconstruct_subhalo_id_raw( id_raw )\n",
    "    \n",
    "    merger_loc = f'subcatalogs/MergerHistory_0{snap}.hdf5'\n",
    "    \n",
    "    # Return None if no file found for snap num.\n",
    "    if not os.path.exists( merger_loc ):\n",
    "        print(f\"WARNING:  Could not find file: {merger_loc}\")\n",
    "        raise ValueError(f\"Subcatalog File Missing: {merger_loc}\")\n",
    "        return None\n",
    "    \n",
    "    # Read Merger History file.    \n",
    "    with h5py.File(merger_loc, 'r') as file:\n",
    "            \n",
    "        # Get the object (could be a group or dataset)\n",
    "        dataset = file['SnapNumNextMajorMerger']\n",
    "        \n",
    "        merger_snap = dataset[subhalo_id]\n",
    "        \n",
    "    \n",
    "    return merger_snap\n",
    "\n",
    "if buildEnv:\n",
    "    tmp = get_merger_snapshot( moi_2_50_67_list[0][1] )\n",
    "    print(' Snap: ', tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8becaaf2-a6f3-4726-955d-fcef85ce1032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21  /  22\n",
      "55  /  56\n",
      "201  /  202\n",
      "228  /  229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_moi_info( args, moi_info ):\n",
    "    \n",
    "    m2_id = moi_info[0]\n",
    "    m1_id = moi_info[1]\n",
    "        \n",
    "    # Fields to load from Tree\n",
    "    fields = ['SubhaloID','NextProgenitorID','MainLeafProgenitorID','FirstProgenitorID','SubhaloGrNr',\\\n",
    "              'SubhaloIDRaw','SubhaloMass', 'RootDescendantID', 'SnapNum', 'DescendantID',\\\n",
    "              'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', ]\n",
    "    \n",
    "    m2_snap, m2_subhalo = tf.deconstruct_subhalo_id_raw( m2_id )\n",
    "    m1_snap, m1_subhalo = tf.deconstruct_subhalo_id_raw( m1_id )   \n",
    "    \n",
    "    # Get snapshot when moi is supposed to occur\n",
    "    merger_snap = get_merger_snapshot( m1_id )\n",
    "    \n",
    "    # Load Tree with desired fields\n",
    "    tree = il.sublink.loadTree( args.simDir, m2_snap, m2_subhalo, fields=fields)\n",
    "    \n",
    "    # Create a dictionary to map Subhalo IDs to their index in the list\n",
    "    tree_index = {subhalo_id: index for index, subhalo_id in enumerate(tree['SubhaloID'])}  \n",
    "    ci = 0  # Starting index of requested subhalo/snapshot\n",
    "    \n",
    "    # Grab ids and velocity arrays of the primary galaxies throughout time. \n",
    "    pVel = np.ones( (m2_snap+1, 3) ) * np.nan    # Velocities\n",
    "    pIDRaw = np.zeros( ( m2_snap+1), dtype=int)  # SubhaloIDRaw\n",
    "        \n",
    "    while ci != -1:        \n",
    "        i, tmp = tf.deconstruct_subhalo_id_raw( tree['SubhaloIDRaw'][ci] )\n",
    "        \n",
    "        # Grab array values\n",
    "        pVel[i,:] = tree['SubhaloVel'][ci][:]\n",
    "        pIDRaw[i] = tree['SubhaloIDRaw'][ci]\n",
    "        \n",
    "        # Update to primary parent in previous snapshot\n",
    "        ci = tree_index.get( tree['FirstProgenitorID'][ci], -1 )\n",
    "\n",
    "    # Calculate the change in velocity (Δv)\n",
    "    dVel = np.diff(pVel, axis=0)    \n",
    "    # Calculate magnitude of acceleration at each time step (assumption Δt=1)\n",
    "    pAcc = np.sqrt( np.sum( dVel**2, axis=-1 ) )\n",
    "    snapshots = [ tf.deconstruct_subhalo_id_raw( pid )[0] for pid in pIDRaw ]\n",
    "    \n",
    "    snaploc = 1000000000000\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Iterate through snapshots to get info\n",
    "    for snap in range( m1_snap, m2_snap ):\n",
    "        \n",
    "        # Find rows for current snapnum\n",
    "        snap_mask = (tree['SubhaloIDRaw'] // snaploc) % snaploc == snap\n",
    "        snap_index = np.where( snap_mask )\n",
    "\n",
    "        # Gather masses for snap\n",
    "        snap_masses = tree['SubhaloMass'][snap_index]\n",
    "\n",
    "        # Find n highest masses\n",
    "        n = 2\n",
    "        top_index = np.argsort(snap_masses)[-n:][::-1]\n",
    "        \n",
    "        if len(top_index) <= 1: continue\n",
    "       \n",
    "        # Treat 1st and 2nd most massive galaxies as *potential* primary and secondary galaxies\n",
    "        pid = snap_index[0][top_index[0]]\n",
    "        sid = snap_index[0][top_index[1]]\n",
    "        \n",
    "        keys =  [ 'SubhaloIDRaw', 'SubhaloMass', 'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', ]\n",
    "        \n",
    "        data[snap] = {}\n",
    "        data[snap]['p_acceleration'] = pAcc[snap]\n",
    "        \n",
    "        for k in keys:\n",
    "            for c, ii in [ ('p',pid), ('s',sid) ]:\n",
    "                #print( k, c, ii )\n",
    "                data[snap]['%s_%s'%(c,k)] = tree[k][ii]\n",
    "\n",
    "        data[snap]['merger_snap'] = merger_snap\n",
    "        data[snap]['xy_projection'] = createVisLink( tree['SubhaloIDRaw'][pid], projection = 'xy' )\n",
    "        data[snap]['p_face_projection'] = createVisLink( tree['SubhaloIDRaw'][pid], projection = 'face' )\n",
    "        data[snap]['s_face_projection'] = createVisLink( tree['SubhaloIDRaw'][sid], projection = 'face' )\n",
    "    \n",
    "    return data\n",
    "        \n",
    "\n",
    "def save_moi_info( args, moi_list, moi_file ):\n",
    "        \n",
    "    # If file exists, read and return.\n",
    "    if os.path.exists( moi_file ) and args.overwrite == False:\n",
    "        df = pd.read_csv( moi_file )\n",
    "        return df\n",
    "    \n",
    "    # Else, create file by getting info via merger trees.\n",
    "    data = {}\n",
    "    n = len( moi_list )\n",
    "    for i in range( n ):\n",
    "        print( i, ' / ', n, end='\\r'  )\n",
    "        data[moi_list[i][0]] = get_moi_info( args, moi_list[i] )\n",
    "    print('')\n",
    "    # Convert the nested dictionary to a list of records\n",
    "    records = [{'moi_SubhaloIDRaw': subhalo_id, 'snapnum': snapnum, **props}\n",
    "               for subhalo_id, snaps in data.items()\n",
    "               for snapnum, props in snaps.items()]\n",
    "\n",
    "    df = pd.json_normalize(records, sep='_')\n",
    "    # print( df )\n",
    "\n",
    "    df.to_csv( moi_file , index=False )\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    args.overwrite = True\n",
    "    \n",
    "    moi_2_50_67_repeats_df = save_moi_info( args, moi_2_50_67_repeats, f'{args.dataDir}/{args.simName}-moi_2-50_67_repeats.csv' )\n",
    "    moi_2_67_99_repeats_df = save_moi_info( args, moi_2_67_99_repeats, f'{args.dataDir}/{args.simName}-moi_2-67_99_repeats.csv' )\n",
    "    moi_2_50_67_df = save_moi_info( args, moi_2_50_67_list, f'{args.dataDir}/{args.simName}-moi_2-50_67.csv' )\n",
    "    moi_2_67_99_df = save_moi_info( args, moi_2_67_99_list, f'{args.dataDir}/{args.simName}-moi_2-67_99.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ccb25-f069-4e91-918b-705c3c0c3469",
   "metadata": {},
   "source": [
    "# Manual Search\n",
    "\n",
    "While it would be nice if there was an automatic way to review these potential targets, I need to find a handful of visually nice targets now.  So I loaded the CSV as an Excel spreadsheet, and manually opened the links to visuallize the galaxies.  For any that show  hints of tidal features, I'm pasting the snapnum and subhalo id down below.\n",
    "\n",
    "### MOI_3:  Galaxies that show hints of tidal features.\n",
    "\n",
    "NOTE to myself.  The following is only from reviewing moi_2_50_67_repeats.   More are sure to be found in moi_2_67_99_repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "603af899-b38d-4cbc-8119-ca8a592b4858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "moi_4_great = {\n",
    "    54:45004,\n",
    "    61:186579,\n",
    "    62:187573,\n",
    "}\n",
    "\n",
    "moi_3_good = {\n",
    "    54:45004,\n",
    "    55:46172,\n",
    "    56:46470,\n",
    "    62:187573,\n",
    "    63:189019,\n",
    "    64:205294,\n",
    "    61:186579,   # AMAZING\n",
    "    62:190717,   # AMAZING\n",
    "    63:192216,\n",
    "    64:153811, \n",
    "    56:230959,\n",
    "}\n",
    "\n",
    "moi_3_maybe {\n",
    "    58:136813,\n",
    "    59:138897,\n",
    "    56:192189,\n",
    "    59:210457,\n",
    "    62:338866,\n",
    "    63:336934,\n",
    "    64:338830,\n",
    "    65:345132,\n",
    "    55:330164,\n",
    "    56:333739,\n",
    "    57:336505,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587505cd-99a9-45cb-b60f-b55d602baa13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
