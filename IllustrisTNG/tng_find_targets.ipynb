{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41903b7-c553-4b32-8059-c5040dbd0304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFile: tng_find_targets.py\\nAuthor: Matthew Ogden\\nEmail: ogdenm12@gmail.com\\nGithub: mbogden\\nCreated: 2023-Nov-09\\n\\nDescription: \\n    This code is designed to interact with the IllustrisTNG Simulation Data and related catalogs. \\n    It's goal is to identify close interactions/mergers between two galaxies.\\n\\nReferences:  \\n- TNG50 Simulation Data\\n    - Nelson, D. et al. (2015). The Illustris Simulation: Public Data Release. Astronomy and Computing, 13, 12–37. https://doi.org/10.1016/j.ascom.2015.09.003\\n    - Nelson, D. et al. (2019). First results from the TNG50 simulation: Galactic outflows driven by supernovae and black hole feedback. Monthly Notices of the Royal Astronomical Society, 490(3), 3234–3261. https://doi.org/10.1093/mnras/stz2306\\n    - Pillepich, A. et al. (2019). First results from the TNG50 simulation: The evolution of stellar and gaseous discs across cosmic time. Monthly Notices of the Royal Astronomical Society, 490(3), 3196–3233. https://doi.org/10.1093/mnras/stz2338\\n    \\n- Subhalo Morphology (Deep Learning)\\n    - Huertas-Company, M.et al. (2019). The Hubble Sequence at $z\\\\sim0$ in the IllustrisTNG simulation with deep learning. Monthly Notices of the Royal Astronomical Society, 489(2), 1859–1879. https://doi.org/10.1093/mnras/stz2191\\n    - Varma, S., Huertas-Company, M., Pillepich, A., Nelson, D., Rodriguez-Gomez, V., Dekel, A., Faber, S. M., Iglesias-Navarro, P., Koo, D. C., & Primack, J. (2021). The building up of observed stellar scaling relations of massive galaxies and the connection to black hole growth in the TNG50 simulation. Monthly Notices of the Royal Astronomical Society, 509(2), 2654–2673. https://doi.org/10.1093/mnras/stab3149\\n    \\nAcknowledgements:    \\n- Sections of this code were written with the assistance of ChatGPT made by OpenAI.\\n\\n- The IllustrisTNG simulations were undertaken with compute time awarded by \\n    the Gauss Centre for Supercomputing (GCS) under GCS Large-Scale Projects GCS-ILLU \\n    and GCS-DWAR on the GCS share of the supercomputer Hazel Hen \\n    at the High Performance Computing Center Stuttgart (HLRS), \\n    as well as on the machines of the Max Planck Computing and Data Facility (MPCDF) in Garching, Germany.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "File: tng_find_targets.py\n",
    "Author: Matthew Ogden\n",
    "Email: ogdenm12@gmail.com\n",
    "Github: mbogden\n",
    "Created: 2023-Nov-09\n",
    "\n",
    "Description: \n",
    "    This code is designed to interact with the IllustrisTNG Simulation Data and related catalogs. \n",
    "    It's goal is to identify close interactions/mergers between two galaxies.\n",
    "\n",
    "References:  \n",
    "- TNG50 Simulation Data\n",
    "    - Nelson, D. et al. (2015). The Illustris Simulation: Public Data Release. Astronomy and Computing, 13, 12–37. https://doi.org/10.1016/j.ascom.2015.09.003\n",
    "    - Nelson, D. et al. (2019). First results from the TNG50 simulation: Galactic outflows driven by supernovae and black hole feedback. Monthly Notices of the Royal Astronomical Society, 490(3), 3234–3261. https://doi.org/10.1093/mnras/stz2306\n",
    "    - Pillepich, A. et al. (2019). First results from the TNG50 simulation: The evolution of stellar and gaseous discs across cosmic time. Monthly Notices of the Royal Astronomical Society, 490(3), 3196–3233. https://doi.org/10.1093/mnras/stz2338\n",
    "    \n",
    "- Subhalo Morphology (Deep Learning)\n",
    "    - Huertas-Company, M.et al. (2019). The Hubble Sequence at $z\\sim0$ in the IllustrisTNG simulation with deep learning. Monthly Notices of the Royal Astronomical Society, 489(2), 1859–1879. https://doi.org/10.1093/mnras/stz2191\n",
    "    - Varma, S., Huertas-Company, M., Pillepich, A., Nelson, D., Rodriguez-Gomez, V., Dekel, A., Faber, S. M., Iglesias-Navarro, P., Koo, D. C., & Primack, J. (2021). The building up of observed stellar scaling relations of massive galaxies and the connection to black hole growth in the TNG50 simulation. Monthly Notices of the Royal Astronomical Society, 509(2), 2654–2673. https://doi.org/10.1093/mnras/stab3149\n",
    "    \n",
    "Acknowledgements:    \n",
    "- Sections of this code were written with the assistance of ChatGPT made by OpenAI.\n",
    "\n",
    "- The IllustrisTNG simulations were undertaken with compute time awarded by \n",
    "    the Gauss Centre for Supercomputing (GCS) under GCS Large-Scale Projects GCS-ILLU \n",
    "    and GCS-DWAR on the GCS share of the supercomputer Hazel Hen \n",
    "    at the High Performance Computing Center Stuttgart (HLRS), \n",
    "    as well as on the machines of the Max Planck Computing and Data Facility (MPCDF) in Garching, Germany.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186b202-41e4-4e51-b426-9452ff296c0c",
   "metadata": {},
   "source": [
    "# Finding Galaxy Mergers within the IllustrisTNG Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d3e73-43f8-40d5-a66e-4b60c38ec7b5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffaf7935-2431-4286-ba29-0ddb848dfedd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Done\n"
     ]
    }
   ],
   "source": [
    "# ================================ IMPORTS ================================ #\n",
    "import os, argparse, h5py\n",
    "import numpy as np, pandas as pd, scipy.signal\n",
    "import matplotlib.pyplot as plt \n",
    "import illustris_python as il\n",
    "import tng_functions as tf\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Imports Done\")\n",
    "\n",
    "# Global variables\n",
    "SIM_DIR = '/home/tnguser/sims.TNG/TNG50-1/output/'\n",
    "\n",
    "# A useful fucntion I often use for indented printing\n",
    "def tabprint( printme, start = '\\t - ', end = '\\n' ):\n",
    "    print( start + str(printme), end = end )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cc9a5-46ed-44a4-9f2a-f818722fd23d",
   "metadata": {},
   "source": [
    "---\n",
    "## Command Line Arguments\n",
    "\n",
    "This is written in JupyterLab, and will be compiled and ran in python for faster execution.  This will define the possible input command line arguements.\n",
    "\n",
    "\n",
    "WARNING:  I have not been consistent with implementing and following arguments.  Code still in indevlopment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aad8ad-d829-43e3-81cb-950629bab0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: Defined\n"
     ]
    }
   ],
   "source": [
    "# This argument decides if code is in python or jupyter.\n",
    "buildEnv = False\n",
    "\n",
    "# Define argument parser function \n",
    "def initParser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument( '-s', '--simDir', default = '/home/tnguser/sims.TNG/TNG50-1/output/',  type=str, \\\n",
    "                        help=\"Base directory for a single simulation on the IllustrisTNG servers.\")   \n",
    "    \n",
    "    parser.add_argument( '-n', '--simName', default = 'TNG50-1',  type=str, \\\n",
    "                        help=\"Name for the simulation being worked on.\")\n",
    "    \n",
    "    parser.add_argument( '-o', '--overwrite', default = False,  type=bool, \\\n",
    "                        help=\"Overwrite output files?  If false, will check if output file exists before beginning time-consuming tasks.\")\n",
    "    \n",
    "    parser.add_argument( '-t', '--trim', default = -1,  type=int, \\\n",
    "                        help=\"Default number of subhalos to consider, sorted by highest mass first.\")\n",
    "    \n",
    "    parser.add_argument( '-f', '--function', default = 'None', type=str, \\\n",
    "                        help=\"Default function program will be executing.\")\n",
    "    \n",
    "    parser.add_argument( '-d', '--dataDir', default = 'data', type=str, \\\n",
    "                        help=\"Default location to store misc data files.\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = initParser()\n",
    "print(\"Args: Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28ddf0-65da-4a9f-950a-20619ed0afe7",
   "metadata": {},
   "source": [
    "## To Python? Or to JupyterLab? \n",
    "This will establish if this is being run in a JupyterLab environment or from Command Line in Python. \n",
    "\n",
    "NOTE:  If you're running this in Jupyter, modify the `cmdStr` below to whatever variables you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3bad9c-5048-4aaa-b254-fdb542bc0b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Building Environment\n",
      "CMD Line: \n",
      "\t$: python3 targets-working.py --trim 10 --dataDir tng-data\n",
      "Args: Read\n",
      "Namespace(simDir='/home/tnguser/sims.TNG/TNG50-1/output/', simName='TNG50-1', overwrite=False, trim=10, function='None', dataDir='tng-data')\n"
     ]
    }
   ],
   "source": [
    "# Am I in a jupyter notebook?\n",
    "try:\n",
    "    \n",
    "    # This command is NOT available in a python script\n",
    "    get_ipython().__class__.__name__\n",
    "    buildEnv = True\n",
    "    print (\"In Building Environment\")\n",
    "    \n",
    "    # Command Line Arguments\n",
    "    cmdStr  = 'python3 targets-working.py'\n",
    "    cmdStr += ' --trim 10'\n",
    "    cmdStr += ' --dataDir tng-data'\n",
    "    \n",
    "    # Read string as if command line\n",
    "    print( \"CMD Line: \\n\\t$:\", cmdStr)\n",
    "    \n",
    "    # This function doesn't like the 'python3 file.py' part.\n",
    "    args = parser.parse_args(cmdStr.split()[2:])\n",
    "\n",
    "# Or am I in a python script?\n",
    "except:\n",
    "    \n",
    "    # Read CMD arguments\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "print( \"Args: Read\")\n",
    "print( args )\n",
    "\n",
    "# Setup data directory if not found\n",
    "os.makedirs(args.dataDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd168c99-22b6-427d-9600-7c6ea4fb4bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this locational valid?\n",
      "Simulation data: True - /home/tnguser/sims.TNG/TNG50-1/output/\n"
     ]
    }
   ],
   "source": [
    "if buildEnv: \n",
    "    # Location of one simulation\n",
    "    print(\"Is this locational valid?\")\n",
    "    print( f\"Simulation data: {os.path.exists( args.simDir )} - {args.simDir}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28feb0bf-fed5-43f1-999a-854079cc1c20",
   "metadata": {},
   "source": [
    "---\n",
    "# Halos and SubHalos\n",
    "Within the simulation, Halos are the largest set of objects that are gravitationally bound to each other, I like to think of them as galaxy clusters.  Subhalos are also gravitationally bound objects but more dense, and I suspect has to do with potential energy.   I like to think of them as galaxies, globular clusters, blobs of intergalactic gas, etc.  (That's my reasoning and I'm sticking to it)\n",
    "\n",
    "\n",
    "For more information, pleas visit the IllustrisTNG Data Specification Page.  https://www.tng-project.org/data/docs/specifications/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654a989-b038-469e-98de-fc5f693396a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Mass Filter\n",
    "\n",
    "So I am looking for galaxies that have enough mass to visualize well.  Thus I will be choosing galaxies that are a scale multiple above and below the mass of the Milky Way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b53a6f1-2773-4834-92a7-9a50cdd9b5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mass Mask: tng-data/TNG50-1-50-mask-mass-10.npy\n",
      "(6780233,) bool\n",
      "Reading Mass Mask: tng-data/TNG50-1-67-mask-mass-10.npy\n",
      "(6244619,) bool\n"
     ]
    }
   ],
   "source": [
    "def getMassFilter( args, snapNum, mScale = 10 ):\n",
    "    \n",
    "    # Define where file will be saved\n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-mass-{mScale}.npy'\n",
    "    \n",
    "    # Read from file if it exits\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Mass Mask: {mLoc}\")\n",
    "        mass_mask = np.load( mLoc )\n",
    "        return mass_mask\n",
    "    \n",
    "    # define mass limits\n",
    "    milky_way_mass = 150.0  # in (10^10 M_⊙) \n",
    "    upper_mass = milky_way_mass * mScale\n",
    "    lower_mass = milky_way_mass / mScale\n",
    "    \n",
    "    # Pull masses for all subhalos in snapshot\n",
    "    fields = ['SubhaloMass']\n",
    "    print(\"Pulling Masses for all Subhalos\")\n",
    "    print(\"WARNING: May take a while \")\n",
    "    SubhaloMass = il.groupcat.loadSubhalos( args.simDir, snapNum, fields=fields)\n",
    "    \n",
    "    # This is the first occasion where I wi\n",
    "    \n",
    "    # Find galaxies between upper and lower mass\n",
    "    mask_mass = ( SubhaloMass[:] <= upper_mass ) & ( SubhaloMass[:] >= lower_mass )\n",
    "    \n",
    "    # Save mass\n",
    "    np.save( mLoc, mask_mass )\n",
    "    \n",
    "    return mask_mass\n",
    "    \n",
    "if buildEnv and True:\n",
    "    args.overwrite = False\n",
    "    mask_mass = getMassFilter( args, 50 )\n",
    "    print( mask_mass.shape, mask_mass.dtype )\n",
    "    mask_mass = getMassFilter( args, 67 )\n",
    "    print( mask_mass.shape, mask_mass.dtype )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea4bb7-779c-4776-9b84-9dfd74e6c2ab",
   "metadata": {},
   "source": [
    "___\n",
    "## Centrals and Satellites\n",
    "Halo's often have a central largest galaxy, with smaller subhalos in orbit around the halo called satellites.  For convenience, let's create a mask of these central galaxies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca282d15-13f8-4c83-bb26-7c66ef93bd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Central Galaxy file: tng-data/TNG50-1-50-mask-central.npy\n",
      "Central Galaxies: (6780233,) [ True False False False False False False False False False]\n",
      "Reading Central Galaxy file: tng-data/TNG50-1-67-mask-central.npy\n",
      "Central Galaxies: (6244619,) [ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "def expand_mask_from_list( true_list, snapnum ):    \n",
    "    n_subhalo = il.groupcat.loadHeader( args.simDir, snapnum)['Nsubgroups_Total']\n",
    "    mask = np.full( n_subhalo, False, dtype=bool )\n",
    "    mask[true_list] = True    \n",
    "    return mask\n",
    "    \n",
    "\n",
    "def getCentralFilter( args, snapnum = 99 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapnum}-mask-central.npy'\n",
    "\n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Central Galaxy file: {mLoc}\")\n",
    "        mask_central = np.load( mLoc )\n",
    "        return mask_central\n",
    "\n",
    "    print(f\"Getting Central SubHalo IDs for sim/snapshot: {args.simName} / {snapnum}\")\n",
    "\n",
    "    # The GroupFirstSub is the subhalo id for the largest subhalo in a halo.  \n",
    "    GroupFirstSub = il.groupcat.loadHalos( args.simDir, snapnum, fields=['GroupFirstSub'])\n",
    "\n",
    "    # Filter out groups that contain no subhalos.\n",
    "    w = np.where(GroupFirstSub >= 0) # value of -1 indicates no subhalo in this group\n",
    "    central_ids = GroupFirstSub[w]\n",
    "    \n",
    "    # Expand into a full array with a value for every subhalo\n",
    "    mask_central = expand_mask_from_list( central_ids, snapnum = snapnum )\n",
    "    \n",
    "    # Save mass\n",
    "    np.save( mLoc, mask_central )\n",
    "    \n",
    "    return mask_central\n",
    "\n",
    "if buildEnv and True: \n",
    "\n",
    "    args.overwrite = False\n",
    "    mask_central = getCentralFilter( args, snapnum = 50 )\n",
    "    print('Central Galaxies:', mask_central.shape, mask_central[:10] )\n",
    "    mask_central = getCentralFilter( args, snapnum = 67 )\n",
    "    print('Central Galaxies:', mask_central.shape, mask_central[:10] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fcaa5f8-59c9-4fdb-ad69-02bf02b67e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BoxSize': 35000.0, 'FlagDoubleprecision': 0, 'Git_commit': b'd203ec8b07c7e2bdda5f608aa0babea46d603699', 'Git_date': b'Thu Apr 7 14:14:27 2016 +0200', 'HubbleParam': 0.6774, 'Ngroups_ThisFile': 1, 'Ngroups_Total': 10638943, 'Nids_ThisFile': 11555101, 'Nids_Total': 8283281991, 'Nsubgroups_ThisFile': 54051, 'Nsubgroups_Total': 6244619, 'NumFiles': 680, 'Omega0': 0.3089, 'OmegaLambda': 0.6911, 'Redshift': 0.5030475232448832, 'Time': 0.6653149581332802}\n",
      "6244619\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "if buildEnv:\n",
    "    tmp2 = il.groupcat.loadHeader(args.simDir, 67)\n",
    "    print( tmp2 )\n",
    "    print( tmp2['Nsubgroups_Total'] )\n",
    "    print( type(tmp2['Nsubgroups_Total'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ba957-ce5c-42e0-8e05-3b998a5e1c77",
   "metadata": {},
   "source": [
    "# (r) Galaxy Morphologies (Deep Learning)\n",
    "\n",
    "Because our method relies on disks of galaxies, it might be useful for us to find mergers betweeen two disk galaxies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27267df1-15b8-4654-bfc3-b233a1821373",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_25': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_29': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_33': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_40': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_50': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_67': {'contents': {}, 'type': 'Group'}}\n"
     ]
    }
   ],
   "source": [
    "def getDiskMorphologyFilter( args, snapNum = 99 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-disk-morphology.npy'\n",
    "\n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Disk Morphology Mask: {mLoc}\")\n",
    "        mask_disk = np.load( mLoc )\n",
    "        return mask_disk\n",
    "    \n",
    "    # Check if morphology file exists.  \n",
    "    hdf5Loc = f'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5'\n",
    "    if not os.path.exists( hdf5Loc ):\n",
    "        print(\"WARNING:!  Subcatalog File missing: \", hdf5Loc )\n",
    "        print(\"Please see IllustrisTNG Data Specs for info to download this file.\")\n",
    "        raise AssertionError\n",
    "    \n",
    "    # Read the deeplearning morphology file\n",
    "    with h5py.File(f'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5', 'r') as file:\n",
    "        \n",
    "        header = f'Snapshot_{snapNum}'\n",
    "        \n",
    "        # Verify snapshot header is in file\n",
    "        if header not in file.keys():\n",
    "            print(f\"Bad HDF5 header: {header} / {file.keys()}\" )\n",
    "            return None       \n",
    "        \n",
    "        subhaloIDs      = np.array( file[header]['SubhaloID'] )\n",
    "        subhaloDiskProb = np.array( file[header]['P_Disk'] )\n",
    "        \n",
    "        print( 'test morph: ', subhaloIDs.shape )\n",
    "        \n",
    "        # Iterate through and grab subhalos with a greater chance of being a disk galaxy\n",
    "        disk_list = []\n",
    "        for i in range( subhaloIDs.shape[0] ):\n",
    "            if subhaloDiskProb[i] > 0.5:\n",
    "                disk_list.append( subhaloIDs[i] )\n",
    "        \n",
    "    # Done reading file.\n",
    "    \n",
    "    # create mask \n",
    "    mask_disk = expand_mask_from_list( np.array( disk_list ), snapNum )\n",
    "    \n",
    "    # Save mass\n",
    "    print(f\"Saving Disk Morphology Mask: {mLoc}\")\n",
    "    np.save( mLoc, mask_disk )\n",
    "    \n",
    "    return mask_disk    \n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    tf.explore_hdf5( 'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5', max_depth = 0 )\n",
    "    # args.overwrite=False\n",
    "    # mask_disk = getDiskMorphologyFilter( args, 67 )  \n",
    "    # print('Disk Galaxies:', mask_disk.shape, mask_disk[:10] )\n",
    "    # mask_disk = getDiskMorphologyFilter( args, 50 )  \n",
    "    # print('Disk Galaxies:', mask_disk.shape, mask_disk[:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df6eee-f1ae-461b-93a1-18bb8fd7cb1c",
   "metadata": {},
   "source": [
    "## (y) Merger History\n",
    "\n",
    "Because manually detecting major mergers in the merger tree is messy (trust me, I tried), I'll be using someone else's subcatalogs to detect major mergers between galaxies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea33f1ae-0c63-4884-9707-dff96eb333db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-50-mask-major-merger-18.npy\n",
      "Disk Galaxies: (6780233,) [ True False False False False False False False  True False]\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-67-mask-major-merger-33.npy\n",
      "Disk Galaxies: (6244619,) [ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getMajorMergerMask( args, snapNum = 67, snapCutoff=13 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-{snapNum}-mask-major-merger-{snapCutoff}.npy'\n",
    "    \n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Upcoming Major Merger Mask: {mLoc}\")\n",
    "        mask_merger = np.load( mLoc )\n",
    "        return mask_merger\n",
    "    \n",
    "    file_loc = f'subcatalogs/MergerHistory_0{snapNum}.hdf5'\n",
    "    \n",
    "    print( f\"Merger History Loc: {file_loc}\" )\n",
    "    \n",
    "    # Return None if no file found for snap num.\n",
    "    if not os.path.exists( file_loc ):\n",
    "        print(f\"WARNING:  Could not find file: {file_loc}\")\n",
    "        raise ValueError(f\"Subcatalog File Missing: {file_loc}\")\n",
    "        return None\n",
    "    \n",
    "    # Read Merger History file.    \n",
    "    with h5py.File(file_loc, 'r') as file:\n",
    "            \n",
    "        # Get the object (could be a group or dataset)\n",
    "        dataset = file['SnapNumNextMajorMerger']\n",
    "        \n",
    "        # Create a boolean mask for values that are non-negative and below the upper limit\n",
    "        mask_merger = (dataset[:] >= 0) & (dataset[:] <= (snapNum + snapCutoff) )\n",
    "        \n",
    "    # Saving\n",
    "    print(f\"Saving Major Merger Mask: {mLoc}\")\n",
    "    np.save( mLoc, mask_merger )\n",
    "       \n",
    "    return mask_merger\n",
    "            \n",
    "    # Find merger\n",
    "    \n",
    "# Get merger tree catalog\n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    # print_HDF5_info( f'subcatalog/MergerHistory_0{snapNum}.hdf5' )\n",
    "    \n",
    "    mask_merger = getMajorMergerMask( args, 50, 18 )\n",
    "    print('Disk Galaxies:', mask_merger.shape, mask_merger[:10] )\n",
    "    mask_merger = getMajorMergerMask( args, 67, 33 )\n",
    "    \n",
    "    print('Disk Galaxies:', mask_merger.shape, mask_merger[:10] )\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd322f-6dca-456a-8ddc-5c672d5add91",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Combine Masks and Generate MOI_1 IDs\n",
    "\n",
    "(Matt from the future here) \n",
    "This is step one of many to narrow down potential mergers so I'm going to establish some terminology as I'm rewriting this.  potential mergers will hence forth be furthered as Mergers-of-Interest or MOI for short.  And each step will have a number, so this will be moi_1.  \n",
    "\n",
    "### MOI_1: Parent galaxy that will collide soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b4e295-6293-4539-b2f2-9211ffd4980d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mass Mask: tng-data/TNG50-1-25-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-25-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-25-mask-major-merger-4.npy\n",
      "Reading Mass Mask: tng-data/TNG50-1-29-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-29-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-29-mask-major-merger-4.npy\n",
      "Reading Mass Mask: tng-data/TNG50-1-33-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-33-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-33-mask-major-merger-7.npy\n",
      "Reading Mass Mask: tng-data/TNG50-1-40-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-40-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-40-mask-major-merger-15.npy\n",
      "Reading Mass Mask: tng-data/TNG50-1-50-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-50-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-50-mask-major-merger-17.npy\n",
      "Reading Mass Mask: tng-data/TNG50-1-67-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data/TNG50-1-67-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data/TNG50-1-67-mask-major-merger-32.npy\n",
      "done\n",
      "MOI_1 Found:  788\n"
     ]
    }
   ],
   "source": [
    "def combine_masks(mask_list):\n",
    "    \n",
    "    # Verify that all masks have the same shape\n",
    "    if not all(mask.shape == mask_list[0].shape for mask in mask_list):\n",
    "        print(\"ERROR:  Masks do not have the same shape.\")\n",
    "        for mask in mask_list:\n",
    "            print( len(mask) )\n",
    "        raise ValueError(\"ERROR: Combine Masks: All masks must have the same shape\")\n",
    "\n",
    "    # Initialize the combined mask with the first mask\n",
    "    combined_mask = mask_list[0].copy()\n",
    "\n",
    "    # Perform logical AND operation with each subsequent mask\n",
    "    for mask in mask_list[1:]:\n",
    "        combined_mask &= mask\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "def generate_moi_1_ids( args, snapNum, mass = True, massScale = 10, central = False, disk = True, major = True, majorCutoff = 13 ):\n",
    "    \n",
    "    # Create list of mask to find goi\n",
    "    mask_list = []\n",
    "    if mass:     mask_list.append( getMassFilter           ( args, snapNum, mScale = massScale ) )\n",
    "    if central:  mask_list.append( getCentralFilter        ( args, snapNum ) )\n",
    "    if disk:     mask_list.append( getDiskMorphologyFilter ( args, snapNum ) )\n",
    "    if major:    mask_list.append( getMajorMergerMask      ( args, snapNum, majorCutoff ) )\n",
    "\n",
    "    try:\n",
    "        # Get \n",
    "        combined_mask = combine_masks( mask_list )\n",
    "        subhalo_ids = np.where( combined_mask )[0] \n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    # Loop through subhalo ids and combine with snapnum to create SubhaloIDRaw\n",
    "    moi_1_ids = []\n",
    "    \n",
    "    for mid in subhalo_ids:\n",
    "        moi_1_ids.append( tf.generate_subhalo_id_raw( snapNum, mid, )  )\n",
    "    \n",
    "    moi_1_ids = np.array( moi_1_ids )\n",
    "    \n",
    "    return moi_1_ids\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "if buildEnv and True:\n",
    "    \n",
    "\n",
    "    # Starting snaps and their ranges for looking\n",
    "    #  (based on merger history hdf5 file)\n",
    "    snap_info = { \n",
    "        25: {'start_snap':25, 'stop_snap':29}, \n",
    "        29: {'start_snap':29, 'stop_snap':33}, \n",
    "        33: {'start_snap':33, 'stop_snap':40},\n",
    "        40: {'start_snap':40, 'stop_snap':55}, \n",
    "        50: {'start_snap':50, 'stop_snap':67}, \n",
    "        67: {'start_snap':67, 'stop_snap':99}\n",
    "    }\n",
    "    \n",
    "    moi_1_ids = []\n",
    "    args.overwrite = False\n",
    "        \n",
    "    for snap in snap_info:\n",
    "\n",
    "        # Initial GOIs of interest\n",
    "        snap_info[snap]['moi_1'] = generate_moi_1_ids( args, snap, majorCutoff = snap_info[snap]['stop_snap']- snap_info[snap]['start_snap']) \n",
    "    \n",
    "    c = 0\n",
    "    for s in snap_info:\n",
    "        c += len( snap_info[s]['moi_1'] )\n",
    "    \n",
    "    print(\"done\")\n",
    "    print( \"MOI_1 Found: \", c )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719a15d-e5a7-4e4d-b238-0424c4002e11",
   "metadata": {},
   "source": [
    "--- \n",
    "## Find MOI_2 in Future Merger Trees\n",
    "\n",
    "Since requesting a merger tree only returns it's tree for the current moment and backwards in time, I need to jump several snapshots forward, and identify which galaxies it belongs to there.  It's a long tedious process but I'll figure it out.\n",
    "\n",
    "### MOI_2:  Child galaxy that has already undergone a major merger event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c1d2632-d8e0-4a1c-9f7a-e02ebd186840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snap 25 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Mass Mask: tng-data/TNG50-1-29-mask-mass-50.npy\n",
      "\t -  2990 / 2991 - 29000000414949 - Found: 22\n",
      "Found MOI_2 / Tree Matches: 22\n",
      "Snap 29 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Mass Mask: tng-data/TNG50-1-33-mask-mass-50.npy\n",
      "\t -  3379 / 3380 - 33000000482584 - Found: 46\n",
      "Found MOI_2 / Tree Matches: 46\n",
      "Snap 33 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Pulling Masses for all Subhalos\n",
      "WARNING: May take a while \n",
      "\t -  3805 / 3806 - 40000000554961 - Found: 95\n",
      "Found MOI_2 / Tree Matches: 95\n",
      "Snap 40 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Pulling Masses for all Subhalos\n",
      "WARNING: May take a while \n",
      "\t -  4096 / 4097 - 55000000642360 - Found: 188\n",
      "Found MOI_2 / Tree Matches: 188\n",
      "Snap 50 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Mass Mask: tng-data/TNG50-1-67-mask-mass-50.npy\n",
      "\t -  4137 / 4138 - 67000000686265 - Found: 196\n",
      "Found MOI_2 / Tree Matches: 196\n",
      "Snap 67 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Mass Mask: tng-data/TNG50-1-99-mask-mass-50.npy\n",
      "\t -  4095 / 4096 - 99000000791758 - Found: 229\n",
      "Found MOI_2 / Tree Matches: 229\n",
      "<class 'dict'>\n",
      "[(99000000000002, 67000000285388), (99000000000019, 67000000521654), (99000000096765, 67000000424745), (99000000117255, 67000000365040), (99000000117255, 67000000365042), (99000000117263, 67000000506299), (99000000143881, 67000000261417), (99000000143885, 67000000153307), (99000000143886, 67000000153309), (99000000143886, 67000000153313), (99000000143890, 67000000519227), (99000000167392, 67000000131717), (99000000167394, 67000000295828), (99000000167394, 67000000295829), (99000000167399, 67000000131718), (99000000184932, 67000000323513), (99000000208812, 67000000363215), (99000000220601, 67000000241012), (99000000229933, 67000000146970), (99000000229933, 67000000146972), (99000000242788, 67000000216072), (99000000242789, 67000000225712), (99000000242789, 67000000225713), (99000000242791, 67000000225714), (99000000264886, 67000000345761), (99000000275545, 67000000160813), (99000000275545, 67000000160815), (99000000282779, 67000000190043), (99000000294867, 67000000435040), (99000000300903, 67000000184179), (99000000300903, 67000000184180), (99000000300903, 67000000184182), (99000000300903, 67000000184183), (99000000324123, 67000000218606), (99000000324123, 67000000218607), (99000000324124, 67000000280993), (99000000329508, 67000000211992), (99000000333425, 67000000391637), (99000000342447, 67000000367142), (99000000352426, 67000000234935), (99000000352426, 67000000234936), (99000000355724, 67000000282211), (99000000355725, 67000000329197), (99000000358608, 67000000269552), (99000000360923, 67000000244815), (99000000360924, 67000000516984), (99000000371126, 67000000242560), (99000000371126, 67000000242561), (99000000372754, 67000000274916), (99000000372755, 67000000396987), (99000000377656, 67000000277959), (99000000382215, 67000000271076), (99000000383976, 67000000445246), (99000000383976, 67000000447266), (99000000388544, 67000000307256), (99000000392278, 67000000445393), (99000000396628, 67000000314577), (99000000398784, 67000000276422), (99000000400973, 67000000287780), (99000000400973, 67000000287781), (99000000402556, 67000000447047), (99000000408534, 67000000293092), (99000000410037, 67000000292102), (99000000418335, 67000000296868), (99000000418335, 67000000296870), (99000000419618, 67000000316152), (99000000422755, 67000000498432), (99000000424288, 67000000330081), (99000000427211, 67000000302085), (99000000430865, 67000000436238), (99000000432106, 67000000308503), (99000000434356, 67000000337656), (99000000434356, 67000000337657), (99000000434357, 67000000512642), (99000000435752, 67000000321039), (99000000435752, 67000000321040), (99000000438148, 67000000299414), (99000000438148, 67000000299415), (99000000440407, 67000000361836), (99000000440407, 67000000361837), (99000000440408, 67000000438729), (99000000441709, 67000000374765), (99000000441709, 67000000423209), (99000000446666, 67000000342045), (99000000448830, 67000000297999), (99000000449658, 67000000359921), (99000000452978, 67000000356635), (99000000455291, 67000000409596), (99000000456327, 67000000474796), (99000000460722, 67000000350284), (99000000464163, 67000000325904), (99000000464163, 67000000325905), (99000000469488, 67000000327094), (99000000471996, 67000000376070), (99000000474008, 67000000353206), (99000000477328, 67000000389081), (99000000479290, 67000000363906), (99000000479938, 67000000385766), (99000000480802, 67000000348011), (99000000482889, 67000000351283), (99000000484448, 67000000355453), (99000000486046, 67000000402361), (99000000486046, 67000000462591), (99000000487743, 67000000400274), (99000000488530, 67000000377799), (99000000489206, 67000000369178), (99000000491426, 67000000373825), (99000000493433, 67000000373217), (99000000493433, 67000000373218), (99000000494011, 67000000378394), (99000000497557, 67000000421627), (99000000499021, 67000000443111), (99000000499704, 67000000399353), (99000000499704, 67000000399354), (99000000500576, 67000000440536), (99000000503987, 67000000393423), (99000000505100, 67000000404128), (99000000505100, 67000000404129), (99000000505101, 67000000484783), (99000000506720, 67000000384936), (99000000507292, 67000000415897), (99000000509091, 67000000412331), (99000000510273, 67000000386589), (99000000510585, 67000000421199), (99000000519721, 67000000443678), (99000000522983, 67000000406810), (99000000522983, 67000000406811), (99000000523548, 67000000381323), (99000000525002, 67000000459478), (99000000525002, 67000000481640), (99000000525533, 67000000405304), (99000000526478, 67000000396305), (99000000526879, 67000000465124), (99000000526880, 67000000484567), (99000000527839, 67000000429589), (99000000528322, 67000000423574), (99000000528322, 67000000423575), (99000000531910, 67000000414327), (99000000532301, 67000000413570), (99000000532301, 67000000413571), (99000000533998, 67000000491550), (99000000536147, 67000000519800), (99000000538370, 67000000411599), (99000000540452, 67000000441808), (99000000540920, 67000000399865), (99000000544697, 67000000422759), (99000000545004, 67000000514475), (99000000545437, 67000000417693), (99000000546114, 67000000416299), (99000000546474, 67000000428958), (99000000550149, 67000000423982), (99000000550475, 67000000433484), (99000000550934, 67000000424348), (99000000552581, 67000000427901), (99000000558410, 67000000426538), (99000000559036, 67000000449562), (99000000559712, 67000000522703), (99000000560082, 67000000446735), (99000000560421, 67000000471346), (99000000561325, 67000000443998), (99000000564268, 67000000464081), (99000000565251, 67000000441134), (99000000565947, 67000000479990), (99000000569653, 67000000444945), (99000000570841, 67000000448785), (99000000571633, 67000000462087), (99000000572457, 67000000511452), (99000000573062, 67000000463802), (99000000573738, 67000000493336), (99000000575863, 67000000459245), (99000000578294, 67000000489908), (99000000579232, 67000000462756), (99000000579509, 67000000469424), (99000000579759, 67000000476450), (99000000582137, 67000000480833), (99000000584724, 67000000472438), (99000000586066, 67000000476656), (99000000586670, 67000000464803), (99000000587256, 67000000506955), (99000000587500, 67000000467140), (99000000590385, 67000000492045), (99000000592840, 67000000456491), (99000000594556, 67000000485083), (99000000594731, 67000000483109), (99000000596986, 67000000482532), (99000000597311, 67000000500228), (99000000598112, 67000000489023), (99000000598840, 67000000471646), (99000000599865, 67000000488251), (99000000601123, 67000000491964), (99000000601693, 67000000485357), (99000000601997, 67000000487422), (99000000602330, 67000000480204), (99000000603826, 67000000523597), (99000000604371, 67000000495260), (99000000605279, 67000000503861), (99000000606405, 67000000486798), (99000000606820, 67000000486254), (99000000609891, 67000000495543), (99000000611152, 67000000499549), (99000000611385, 67000000500545), (99000000613209, 67000000495789), (99000000615607, 67000000505694), (99000000616825, 67000000506009), (99000000619381, 67000000507560), (99000000620348, 67000000503549), (99000000621167, 67000000501261), (99000000622298, 67000000519140), (99000000622751, 67000000506399), (99000000623122, 67000000510751), (99000000623456, 67000000498214), (99000000624756, 67000000506693), (99000000626408, 67000000508867), (99000000626697, 67000000511593), (99000000627572, 67000000512912), (99000000628717, 67000000507703), (99000000630155, 67000000516766), (99000000630870, 67000000513317), (99000000631677, 67000000524596), (99000000633843, 67000000526774), (99000000633963, 67000000519328), (99000000635905, 67000000477343), (99000000636201, 67000000508235), (99000000636640, 67000000520068), (99000000637695, 67000000529772), (99000000638374, 67000000528728), (99000000639991, 67000000519492), (99000000641964, 67000000536490), (99000000645517, 67000000515098)]\n"
     ]
    }
   ],
   "source": [
    "def find_mois_in_tree( tree_id_raw, moi_1_list, args, ):\n",
    "    \n",
    "    tree_snap, tree_id = tf.deconstruct_subhalo_id_raw( tree_id_raw )\n",
    "    \n",
    "    # Load only SubhaloIDRaw for effecient retrieval time\n",
    "    tree = il.sublink.loadTree( args.simDir, tree_snap, tree_id, fields=['SubhaloIDRaw'] )\n",
    "\n",
    "    # See if any of my MOIs are in list\n",
    "    moi_1_mask = np.isin( moi_1_list, tree )\n",
    "    n_matches = np.sum(moi_1_mask) # Presuming True = 1 and False = 0\n",
    "\n",
    "    # If none found\n",
    "    if n_matches == 0:\n",
    "        return []\n",
    "    # Else we have results\n",
    "\n",
    "    # Get the index locations where the mask is True\n",
    "    moi_1_loc = np.where(moi_1_mask)[0]\n",
    "    moi_1_ids_in_tree = moi_1_list[ moi_1_loc ]\n",
    "\n",
    "    moi_2_list = []\n",
    "\n",
    "    for moi_1 in moi_1_ids_in_tree:\n",
    "        moi_2_list.append( ( tree_id_raw, moi_1 ) )\n",
    "\n",
    "    return moi_2_list\n",
    "\n",
    "\n",
    "def getMOI_2( args, snap_info, func_overwrite = False ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/{args.simName}-moi_2.txt'\n",
    "    \n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite and not func_overwrite:\n",
    "        print(f\"Reading Merger-of-Interest List: {mLoc}\")\n",
    "        moi_list = np.loadtxt( mLoc, dtype=int )\n",
    "        return moi_list\n",
    "    \n",
    "    # Loop through snaps of interest, collect moi_2 value\n",
    "    \n",
    "    for snap in snap_info:\n",
    "        \n",
    "        print(f\"Snap {snap} of {snap_info.keys()}\")\n",
    "        stop = snap_info[snap]['stop_snap']\n",
    "        moi_1_list = snap_info[snap]['moi_1']\n",
    "    \n",
    "        # Basically just get lots of galaxies above mass range in final snapshot\n",
    "        tree_id_list = generate_moi_1_ids( args, stop, \\\n",
    "                                               mass = True, massScale = 50, \\\n",
    "                                               central = False, disk = False, \\\n",
    "                                               major=False )\n",
    "        # Find moi_2 containing moi_1\n",
    "        moi_2_list = []\n",
    "        for i, tree_id_raw in enumerate(tree_id_list):  \n",
    "            tree_snap, tree_id = tf.deconstruct_subhalo_id_raw( tree_id_raw )\n",
    "            tabprint( f\" {i} / {tree_id_list.shape[0]} - {tree_id_raw} - Found: {len(moi_2_list)}\", end='\\r' )\n",
    "\n",
    "            found_ids = find_mois_in_tree( tree_id_raw, moi_1_list, args )\n",
    "            moi_2_list.extend( found_ids )\n",
    "\n",
    "        print( f\"\\nFound MOI_2 / Tree Matches: {len( moi_2_list) }\")\n",
    "        \n",
    "        snap_info[snap]['moi_2'] = moi_2_list\n",
    "        \n",
    "    np.savetxt( mLoc, moi_2_list )\n",
    "    \n",
    "    return snap_info\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    args.overwrite=False\n",
    "    snap_info = getMOI_2( args, snap_info, True )\n",
    "    \n",
    "    print( type( snap_info ) )\n",
    "    print( snap_info[67]['moi_2'] )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e231d-2782-44d5-880d-890c6afe8690",
   "metadata": {},
   "source": [
    "## MOI_3:  MOI_2 with multiple MOI_1 in their tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1ce961-c131-4912-bbf5-5935b852133a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "29\n",
      "33\n",
      "40\n",
      "50\n",
      "67\n",
      "MOI 2 Duos:  126\n"
     ]
    }
   ],
   "source": [
    "def find_moi_2_repeats( moi_list ):\n",
    "    \n",
    "    # Step 1: Extract all moi_2's from the tuples\n",
    "    moi_2_list = [mois[0] for mois in moi_list]\n",
    "\n",
    "    # Step 2: Count the occurrences of each moi_2\n",
    "    moi_2_counts = Counter(moi_2_list)\n",
    "\n",
    "    # Step 3: Filter the tuples where moi_2 appears more than once\n",
    "    moi_2_repeats = [mois for mois in moi_list if moi_2_counts[mois[0]] > 1]\n",
    "    \n",
    "    return moi_2_repeats\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    m2_duos = []\n",
    "    \n",
    "    for snap in snap_info:\n",
    "        \n",
    "        print(snap )\n",
    "    \n",
    "        m2_duos.extend( find_moi_2_repeats( snap_info[snap]['moi_2'] ) )\n",
    "\n",
    "    print( 'MOI 2 Duos: ', len( m2_duos ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e4edaf8-8f6e-496a-8da3-e259162e23e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Snap:  28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define print fucntion for a row\n",
    "def printRow( tree, i, fields ):\n",
    "    # if i == -1:\n",
    "    #     print(\"Invalid index\")\n",
    "    #     return\n",
    "    \n",
    "    print( \" - \".join( [ f\"{key}:{tree[key][i]}\" for key in fields ]) )\n",
    "    \n",
    "def createVisLink( subhaloIDRaw, projection = 'face', simulation='TNG50-1' ):\n",
    "        tmp = tf.deconstruct_subhalo_id_raw( subhaloIDRaw )    \n",
    "        \n",
    "        if projection == 'face':\n",
    "            link = f\"https://www.tng-project.org/api/{simulation}/snapshots/{tmp[0]}/subhalos/{tmp[1]}/vis.png?partType=stars&partField=stellarComp-jwst_f200w-jwst_f115w-jwst_f070w&size=1&method=histo&rotation=face-on&plotStyle=edged\"\n",
    "        # Else, project x,y plane\n",
    "        else:\n",
    "            link = f\"https://www.tng-project.org/api/{simulation}/snapshots/{tmp[0]}/subhalos/{tmp[1]}/vis.png?partType=stars&partField=stellarComp-jwst_f200w-jwst_f115w-jwst_f070w&size=1&method=histo&nPixels=256%2C256&axes=0%2C1&plotStyle=edged\"\n",
    "        return link\n",
    "\n",
    "def get_merger_snapshot( id_raw ):\n",
    "    \n",
    "    snap, subhalo_id = tf.deconstruct_subhalo_id_raw( id_raw )\n",
    "    \n",
    "    merger_loc = f'subcatalogs/MergerHistory_0{snap}.hdf5'\n",
    "    \n",
    "    # Return None if no file found for snap num.\n",
    "    if not os.path.exists( merger_loc ):\n",
    "        print(f\"WARNING:  Could not find file: {merger_loc}\")\n",
    "        raise ValueError(f\"Subcatalog File Missing: {merger_loc}\")\n",
    "        return None\n",
    "    \n",
    "    # Read Merger History file.    \n",
    "    with h5py.File(merger_loc, 'r') as file:\n",
    "            \n",
    "        # Get the object (could be a group or dataset)\n",
    "        dataset = file['SnapNumNextMajorMerger']\n",
    "        \n",
    "        merger_snap = dataset[subhalo_id]\n",
    "        \n",
    "    \n",
    "    return merger_snap\n",
    "\n",
    "if buildEnv:\n",
    "    tmp = get_merger_snapshot( m2_duos[0][1] )\n",
    "    print(' Snap: ', tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8becaaf2-a6f3-4726-955d-fcef85ce1032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125  /  126\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_moi_info( args, moi_info ):\n",
    "    \n",
    "    m2_id = moi_info[0]\n",
    "    m1_id = moi_info[1]\n",
    "        \n",
    "    # Fields to load from Tree\n",
    "    fields = ['SubhaloID','NextProgenitorID','MainLeafProgenitorID','FirstProgenitorID','SubhaloGrNr',\\\n",
    "              'SubhaloIDRaw','SubhaloMass', 'RootDescendantID', 'SnapNum', 'DescendantID',\\\n",
    "              'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', ]\n",
    "    \n",
    "    m2_snap, m2_subhalo = tf.deconstruct_subhalo_id_raw( m2_id )\n",
    "    m1_snap, m1_subhalo = tf.deconstruct_subhalo_id_raw( m1_id )   \n",
    "    \n",
    "    # Get snapshot when moi is supposed to occur\n",
    "    merger_snap = get_merger_snapshot( m1_id )\n",
    "    \n",
    "    # Load Tree with desired fields\n",
    "    tree = il.sublink.loadTree( args.simDir, m2_snap, m2_subhalo, fields=fields)\n",
    "    \n",
    "    # Create a dictionary to map Subhalo IDs to their index in the list\n",
    "    tree_index = {subhalo_id: index for index, subhalo_id in enumerate(tree['SubhaloID'])}  \n",
    "    ci = 0  # Starting index of requested subhalo/snapshot\n",
    "    \n",
    "    # Grab ids and velocity arrays of the primary galaxies throughout time. \n",
    "    pVel = np.ones( (m2_snap+1, 3) ) * np.nan    # Velocities\n",
    "    pIDRaw = np.zeros( ( m2_snap+1), dtype=int)  # SubhaloIDRaw\n",
    "        \n",
    "    while ci != -1:        \n",
    "        i, tmp = tf.deconstruct_subhalo_id_raw( tree['SubhaloIDRaw'][ci] )\n",
    "        \n",
    "        # Grab array values\n",
    "        pVel[i,:] = tree['SubhaloVel'][ci][:]\n",
    "        pIDRaw[i] = tree['SubhaloIDRaw'][ci]\n",
    "        \n",
    "        # Update to primary parent in previous snapshot\n",
    "        ci = tree_index.get( tree['FirstProgenitorID'][ci], -1 )\n",
    "\n",
    "    # Calculate the change in velocity (Δv)\n",
    "    dVel = np.diff(pVel, axis=0)    \n",
    "    # Calculate magnitude of acceleration at each time step (assumption Δt=1)\n",
    "    pAcc = np.sqrt( np.sum( dVel**2, axis=-1 ) )\n",
    "    snapshots = [ tf.deconstruct_subhalo_id_raw( pid )[0] for pid in pIDRaw ]\n",
    "    \n",
    "    snaploc = 1000000000000\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    # Iterate through snapshots to get info\n",
    "    for snap in range( m1_snap-5, m2_snap ):\n",
    "        \n",
    "        # Find rows for current snapnum\n",
    "        snap_mask = (tree['SubhaloIDRaw'] // snaploc) % snaploc == snap\n",
    "        snap_index = np.where( snap_mask )\n",
    "\n",
    "        # Gather masses for snap\n",
    "        snap_masses = tree['SubhaloMass'][snap_index]\n",
    "\n",
    "        # Find n highest masses\n",
    "        n = 2\n",
    "        top_index = np.argsort(snap_masses)[-n:][::-1]\n",
    "        \n",
    "        if len(top_index) <= 1: continue\n",
    "       \n",
    "        # Treat 1st and 2nd most massive galaxies as *potential* primary and secondary galaxies\n",
    "        pid = snap_index[0][top_index[0]]\n",
    "        sid = snap_index[0][top_index[1]]\n",
    "        \n",
    "        keys =  [ 'SubhaloIDRaw', 'SubhaloMass', 'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', ]\n",
    "        \n",
    "        # Store values in a flat dictionary\n",
    "        flat_dict = {}\n",
    "        flat_dict['snap'] = snap\n",
    "        flat_dict['moi_1_id'] = m1_id\n",
    "        flat_dict['moi_2_id'] = m2_id\n",
    "        flat_dict['merger_snap'] = merger_snap\n",
    "        flat_dict['p_acceleration'] = pAcc[snap]\n",
    "        flat_dict['xy_projection'] = createVisLink( tree['SubhaloIDRaw'][pid], projection = 'xy' )\n",
    "        flat_dict['p_face_projection'] = createVisLink( tree['SubhaloIDRaw'][pid], projection = 'face' )\n",
    "        flat_dict['s_face_projection'] = createVisLink( tree['SubhaloIDRaw'][sid], projection = 'face' )\n",
    "        \n",
    "        # Loop through catalog data and store\n",
    "        for k in keys:\n",
    "            for c, ii in [ ('p',pid), ('s',sid) ]:\n",
    "                # print( k, c, ii )\n",
    "                flat_dict[f'{c}_{k}'] = tree[k][ii]\n",
    "        \n",
    "        data_list.append(flat_dict)\n",
    "    \n",
    "    return data_list\n",
    "        \n",
    "\n",
    "def save_moi_info( args, moi_list, moi_file, func_overwrite=False ):\n",
    "        \n",
    "    # If file exists, read and return.\n",
    "    if os.path.exists( moi_file ) and args.overwrite == False and func_overwrite == False:\n",
    "        df = pd.read_csv( moi_file )\n",
    "        return df\n",
    "    \n",
    "    # Else, create file by getting info via merger trees.\n",
    "    data_list = []\n",
    "    n = len( moi_list )\n",
    "    for i in range( n ):\n",
    "        print( i, ' / ', n, end='\\r'  )\n",
    "        moi_info = get_moi_info( args, moi_list[i] )\n",
    "        data_list.extend( moi_info )\n",
    "        \n",
    "    print('')\n",
    "    # Convert list into dataframe\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    df.to_csv( moi_file , index=False )\n",
    "    \n",
    "    # return df\n",
    "    \n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    args.overwrite = False\n",
    "      \n",
    "    m2_duos_df = save_moi_info( args, m2_duos, f'{args.dataDir}/moi_2-duos.csv', func_overwrite=True )\n",
    "    \n",
    "    print( m2_duos_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ccb25-f069-4e91-918b-705c3c0c3469",
   "metadata": {},
   "source": [
    "# MOI_3:  Manual Search\n",
    "\n",
    "While it would be nice if there was an automatic way to review these potential targets, I need to find a handful of visually nice targets now.  So I loaded the CSV as an Excel spreadsheet, and manually opened the links to visuallize the galaxies.  For any that show  hints of tidal features, I'm pasting the snapnum and subhalo id down below.\n",
    "\n",
    "### MOI_3:  Galaxies that show hints of tidal features.\n",
    "\n",
    "NOTE to myself.  The following is only from reviewing moi_2_50_67_repeats.   More are sure to be found in newer file m2_duos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603af899-b38d-4cbc-8119-ca8a592b4858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "moi_4_great = {\n",
    "    54:45004,\n",
    "    61:186579,\n",
    "    62:190717,   # AMAZING\n",
    "    63:192216,\n",
    "    64:153811, \n",
    "    62:187573,\n",
    "}\n",
    "\n",
    "moi_3_good = [\n",
    "    (54, 45004),\n",
    "    (55, 46172),\n",
    "    (56, 46470),\n",
    "    (62, 187573),\n",
    "    (63, 189019),\n",
    "    (64, 205294),\n",
    "    (61, 186579),\n",
    "    (62, 190717),\n",
    "    (63, 192216),\n",
    "    (64, 153811),\n",
    "    (56, 230959),\n",
    "]\n",
    "\n",
    "\n",
    "moi_3_maybe = [\n",
    "    (58, 136813),\n",
    "    (59, 138897),\n",
    "    (56, 192189),\n",
    "    (59, 210457),\n",
    "    (62, 338866),\n",
    "    (63, 336934),\n",
    "    (64, 338830),\n",
    "    (65, 345132),\n",
    "    (55, 330164),\n",
    "    (56, 333739),\n",
    "    (57, 336505),\n",
    "]\n",
    "\n",
    "if buildEnv:\n",
    "    \n",
    "    # Let's save some of these test moi-3s somewhere.\n",
    "    m3_file = 'tng-data/moi_3-test.csv'\n",
    "    \n",
    "    # Identify MOI_2 id's that contain our MOI_3 id.\n",
    "    # Get rows containing moi_3 ids\n",
    "    pid_list = [ tf.generate_subhalo_id_raw( snap, subhalo_id ) for snap, subhalo_id in moi_3_good ]\n",
    "    print( pid_list )\n",
    "    \n",
    "    # Extract moi_2 ids associated with p_ids from moi_3 list.\n",
    "    # print( m2_duos_df['p_SubhaloIDRaw'] )\n",
    "    moi2_ids = m2_duos_df[ m2_duos_df['p_SubhaloIDRaw'].isin(pid_list) ]['moi_2_id'].unique()\n",
    "    print( moi2_ids )\n",
    "    \n",
    "    # Get all rows with moi_2 values, thus getting all info before and after moi_3 snap\n",
    "    moi_3_df = m2_duos_df[ m2_duos_df['moi_2_id'].isin(moi2_ids) ]\n",
    "    \n",
    "    # Add new columns indicating row of MOI_3 candidates\n",
    "    moi_3_df['moi_3'] = False\n",
    "    moi_3_df.loc[moi_3_df['p_SubhaloIDRaw'].isin(pid_list), 'moi_3'] = True\n",
    "    \n",
    "    moi_3_df.to_csv( m3_file , index=False )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414d64b-10fd-4269-aa1a-d2937e94259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
