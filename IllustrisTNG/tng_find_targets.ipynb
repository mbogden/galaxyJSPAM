{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41903b7-c553-4b32-8059-c5040dbd0304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFile: tng_find_targets.py\\nAuthor: Matthew Ogden\\nEmail: ogdenm12@gmail.com\\nGithub: mbogden\\nCreated: 2023-Nov-09\\n\\nDescription: \\n    This code is designed to interact with the IllustrisTNG Simulation Data and related catalogs. \\n    It's goal is to identify close interactions/mergers between two galaxies.\\n\\nReferences:  \\n- TNG50 Simulation Data\\n    - Nelson, D. et al. (2015). The Illustris Simulation: Public Data Release. Astronomy and Computing, 13, 12–37. https://doi.org/10.1016/j.ascom.2015.09.003\\n    - Nelson, D. et al. (2019). First results from the TNG50 simulation: Galactic outflows driven by supernovae and black hole feedback. Monthly Notices of the Royal Astronomical Society, 490(3), 3234–3261. https://doi.org/10.1093/mnras/stz2306\\n    - Pillepich, A. et al. (2019). First results from the TNG50 simulation: The evolution of stellar and gaseous discs across cosmic time. Monthly Notices of the Royal Astronomical Society, 490(3), 3196–3233. https://doi.org/10.1093/mnras/stz2338\\n    \\n- Subhalo Morphology (Deep Learning)\\n    - Huertas-Company, M.et al. (2019). The Hubble Sequence at $z\\\\sim0$ in the IllustrisTNG simulation with deep learning. Monthly Notices of the Royal Astronomical Society, 489(2), 1859–1879. https://doi.org/10.1093/mnras/stz2191\\n    - Varma, S., Huertas-Company, M., Pillepich, A., Nelson, D., Rodriguez-Gomez, V., Dekel, A., Faber, S. M., Iglesias-Navarro, P., Koo, D. C., & Primack, J. (2021). The building up of observed stellar scaling relations of massive galaxies and the connection to black hole growth in the TNG50 simulation. Monthly Notices of the Royal Astronomical Society, 509(2), 2654–2673. https://doi.org/10.1093/mnras/stab3149\\n    \\nAcknowledgements:    \\n- Sections of this code were written with the assistance of ChatGPT made by OpenAI.\\n\\n- The IllustrisTNG simulations were undertaken with compute time awarded by \\n    the Gauss Centre for Supercomputing (GCS) under GCS Large-Scale Projects GCS-ILLU \\n    and GCS-DWAR on the GCS share of the supercomputer Hazel Hen \\n    at the High Performance Computing Center Stuttgart (HLRS), \\n    as well as on the machines of the Max Planck Computing and Data Facility (MPCDF) in Garching, Germany.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "File: tng_find_targets.py\n",
    "Author: Matthew Ogden\n",
    "Email: ogdenm12@gmail.com\n",
    "Github: mbogden\n",
    "Created: 2023-Nov-09\n",
    "\n",
    "Description: \n",
    "    This code is designed to interact with the IllustrisTNG Simulation Data and Subhalo catalogs. \n",
    "    It's goal is to identify close interactions/mergers between two galaxies.\n",
    "\n",
    "References:  \n",
    "- TNG50 Simulation Data\n",
    "    - Nelson, D. et al. (2015). The Illustris Simulation: Public Data Release. Astronomy and Computing, 13, 12–37. https://doi.org/10.1016/j.ascom.2015.09.003\n",
    "    - Nelson, D. et al. (2019). First results from the TNG50 simulation: Galactic outflows driven by supernovae and black hole feedback. Monthly Notices of the Royal Astronomical Society, 490(3), 3234–3261. https://doi.org/10.1093/mnras/stz2306\n",
    "    - Pillepich, A. et al. (2019). First results from the TNG50 simulation: The evolution of stellar and gaseous discs across cosmic time. Monthly Notices of the Royal Astronomical Society, 490(3), 3196–3233. https://doi.org/10.1093/mnras/stz2338\n",
    "    \n",
    "- Subhalo Morphology (Deep Learning)\n",
    "    - Huertas-Company, M.et al. (2019). The Hubble Sequence at $z\\sim0$ in the IllustrisTNG simulation with deep learning. Monthly Notices of the Royal Astronomical Society, 489(2), 1859–1879. https://doi.org/10.1093/mnras/stz2191\n",
    "    - Varma, S., Huertas-Company, M., Pillepich, A., Nelson, D., Rodriguez-Gomez, V., Dekel, A., Faber, S. M., Iglesias-Navarro, P., Koo, D. C., & Primack, J. (2021). The building up of observed stellar scaling relations of massive galaxies and the connection to black hole growth in the TNG50 simulation. Monthly Notices of the Royal Astronomical Society, 509(2), 2654–2673. https://doi.org/10.1093/mnras/stab3149\n",
    "    \n",
    "Acknowledgements:    \n",
    "- Sections of this code were written with the assistance of ChatGPT made by OpenAI.\n",
    "\n",
    "- The IllustrisTNG simulations were undertaken with compute time awarded by \n",
    "    the Gauss Centre for Supercomputing (GCS) under GCS Large-Scale Projects GCS-ILLU \n",
    "    and GCS-DWAR on the GCS share of the supercomputer Hazel Hen \n",
    "    at the High Performance Computing Center Stuttgart (HLRS), \n",
    "    as well as on the machines of the Max Planck Computing and Data Facility (MPCDF) in Garching, Germany.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186b202-41e4-4e51-b426-9452ff296c0c",
   "metadata": {},
   "source": [
    "# Finding Galaxy Mergers within the IllustrisTNG Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d3e73-43f8-40d5-a66e-4b60c38ec7b5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffaf7935-2431-4286-ba29-0ddb848dfedd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Done\n"
     ]
    }
   ],
   "source": [
    "# ================================ IMPORTS ================================ #\n",
    "import os, argparse, h5py\n",
    "import numpy as np, pandas as pd, scipy.signal\n",
    "import matplotlib.pyplot as plt \n",
    "import illustris_python as il\n",
    "import tng_functions as tf\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Imports Done\")\n",
    "\n",
    "# Global variables\n",
    "SIM_DIR = '/home/tnguser/sims.TNG/TNG50-1/output/'\n",
    "\n",
    "# A useful fucntion I often use for indented printing\n",
    "def tabprint( printme, start = '\\t - ', end = '\\n' ):\n",
    "    print( start + str(printme), end = end )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cc9a5-46ed-44a4-9f2a-f818722fd23d",
   "metadata": {},
   "source": [
    "---\n",
    "## Command Line Arguments\n",
    "\n",
    "This is written in JupyterLab, and will be compiled and ran in python for faster execution.  This will define the possible input command line arguements.\n",
    "\n",
    "\n",
    "WARNING:  I have not been consistent with implementing and following arguments.  Code still in indevlopment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aad8ad-d829-43e3-81cb-950629bab0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: Defined\n"
     ]
    }
   ],
   "source": [
    "# This argument decides if code is in python or jupyter.\n",
    "buildEnv = False\n",
    "\n",
    "# Define argument parser function \n",
    "def initParser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument( '-s', '--simDir', default = '/home/tnguser/sims.TNG/TNG50-1/output/',  type=str, \\\n",
    "                        help=\"Base directory for a single simulation on the IllustrisTNG servers.\")   \n",
    "    \n",
    "    parser.add_argument( '-n', '--simName', default = 'TNG50-1',  type=str, \\\n",
    "                        help=\"Name for the simulation being worked on.\")\n",
    "    \n",
    "    parser.add_argument( '-o', '--overwrite', default = False,  type=bool, \\\n",
    "                        help=\"Overwrite output files?  If false, will check if output file exists before beginning time-consuming tasks.\")\n",
    "    \n",
    "    parser.add_argument( '-t', '--trim', default = -1,  type=int, \\\n",
    "                        help=\"Default number of subhalos to consider, sorted by highest mass first.\")\n",
    "    \n",
    "    parser.add_argument( '-f', '--function', default = 'None', type=str, \\\n",
    "                        help=\"Default function program will be executing.\")\n",
    "    \n",
    "    parser.add_argument( '-d', '--dataDir', default = 'data', type=str, \\\n",
    "                        help=\"Default location to store misc data files.\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = initParser()\n",
    "print(\"Args: Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28ddf0-65da-4a9f-950a-20619ed0afe7",
   "metadata": {},
   "source": [
    "## To Python? Or to JupyterLab? \n",
    "This will establish if this is being run in a JupyterLab environment or from Command Line in Python. \n",
    "\n",
    "NOTE:  If you're running this in Jupyter, modify the `cmdStr` below to whatever variables you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3bad9c-5048-4aaa-b254-fdb542bc0b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Building Environment\n",
      "CMD Line: \n",
      "\t$: python3 targets-working.py --trim 10 --dataDir tng-data\n",
      "Args: Read\n",
      "Namespace(simDir='/home/tnguser/sims.TNG/TNG50-1/output/', simName='TNG50-1', overwrite=False, trim=10, function='None', dataDir='tng-data')\n"
     ]
    }
   ],
   "source": [
    "# Am I in a jupyter notebook?\n",
    "try:\n",
    "    \n",
    "    # This command is NOT available in a python script\n",
    "    get_ipython().__class__.__name__\n",
    "    buildEnv = True\n",
    "    print (\"In Building Environment\")\n",
    "    \n",
    "    # Command Line Arguments\n",
    "    cmdStr  = 'python3 targets-working.py'\n",
    "    cmdStr += ' --trim 10'\n",
    "    cmdStr += ' --dataDir tng-data'\n",
    "    \n",
    "    # Read string as if command line\n",
    "    print( \"CMD Line: \\n\\t$:\", cmdStr)\n",
    "    \n",
    "    # This function doesn't like the 'python3 file.py' part.\n",
    "    args = parser.parse_args(cmdStr.split()[2:])\n",
    "\n",
    "# Or am I in a python script?\n",
    "except:\n",
    "    \n",
    "    # Read CMD arguments\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "print( \"Args: Read\")\n",
    "print( args )\n",
    "\n",
    "if args.dataDir[-1] != '/': args.dataDir += '/'\n",
    "\n",
    "# Setup data directory if not found\n",
    "os.makedirs(args.dataDir, exist_ok=True)\n",
    "os.makedirs(args.dataDir + 'tmp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd168c99-22b6-427d-9600-7c6ea4fb4bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this locational valid?\n",
      "Simulation data: True - /home/tnguser/sims.TNG/TNG50-1/output/\n"
     ]
    }
   ],
   "source": [
    "if buildEnv: \n",
    "    # Location of one simulation\n",
    "    print(\"Is this locational valid?\")\n",
    "    print( f\"Simulation data: {os.path.exists( args.simDir )} - {args.simDir}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28feb0bf-fed5-43f1-999a-854079cc1c20",
   "metadata": {},
   "source": [
    "---\n",
    "# Halos and SubHalos\n",
    "Within the simulation, Halos are the largest set of objects that are gravitationally bound to each other, I like to think of them as galaxy clusters.  Subhalos are also gravitationally bound objects but more dense, and I suspect has to do with potential energy.   I like to think of them as galaxies, globular clusters, blobs of intergalactic gas, etc.  (That's my reasoning and I'm sticking to it)\n",
    "\n",
    "\n",
    "For more information, pleas visit the IllustrisTNG Data Specification Page.  https://www.tng-project.org/data/docs/specifications/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654a989-b038-469e-98de-fc5f693396a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Mass Filter\n",
    "\n",
    "So I am looking for galaxies that have enough mass to visualize well.  Thus I will be choosing galaxies that are a scale multiple above and below the mass of the Milky Way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b53a6f1-2773-4834-92a7-9a50cdd9b5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mass Mask: tng-data//tmp/TNG50-1-50-mask-mass-10.npy\n",
      "(6780233,) bool\n",
      "Reading Mass Mask: tng-data//tmp/TNG50-1-67-mask-mass-10.npy\n",
      "(6244619,) bool\n"
     ]
    }
   ],
   "source": [
    "def getMassFilter( args, snapNum, mScale = 10 ):\n",
    "    \n",
    "    # Define where file will be saved\n",
    "    mLoc = f'{args.dataDir}/tmp/{args.simName}-{snapNum}-mask-mass-{mScale}.npy'\n",
    "    \n",
    "    # Read from file if it exits\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Mass Mask: {mLoc}\")\n",
    "        mass_mask = np.load( mLoc )\n",
    "        return mass_mask\n",
    "    \n",
    "    # define mass limits\n",
    "    milky_way_mass = 150.0  # in (10^10 M_⊙) \n",
    "    upper_mass = milky_way_mass * mScale\n",
    "    lower_mass = milky_way_mass / mScale\n",
    "    \n",
    "    # Pull masses for all subhalos in snapshot\n",
    "    fields = ['SubhaloMass']\n",
    "    print(\"Pulling Masses for all Subhalos\")\n",
    "    print(\"WARNING: May take a while \")\n",
    "    SubhaloMass = il.groupcat.loadSubhalos( args.simDir, snapNum, fields=fields)\n",
    "    \n",
    "    # This is the first occasion where I wi\n",
    "    \n",
    "    # Find galaxies between upper and lower mass\n",
    "    mask_mass = ( SubhaloMass[:] <= upper_mass ) & ( SubhaloMass[:] >= lower_mass )\n",
    "    \n",
    "    # Save mass\n",
    "    np.save( mLoc, mask_mass )\n",
    "    \n",
    "    return mask_mass\n",
    "    \n",
    "if buildEnv and True:\n",
    "    args.overwrite = False\n",
    "    mask_mass = getMassFilter( args, 50 )\n",
    "    print( mask_mass.shape, mask_mass.dtype )\n",
    "    mask_mass = getMassFilter( args, 67 )\n",
    "    print( mask_mass.shape, mask_mass.dtype )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea4bb7-779c-4776-9b84-9dfd74e6c2ab",
   "metadata": {},
   "source": [
    "___\n",
    "## Centrals and Satellites\n",
    "Halo's often have a central largest galaxy, with smaller subhalos in orbit around the halo called satellites.  For convenience, let's create a mask of these central galaxies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca282d15-13f8-4c83-bb26-7c66ef93bd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Central Galaxy file: tng-data//tmp/TNG50-1-50-mask-central.npy\n",
      "Central Galaxies: (6780233,) [ True False False False False False False False False False]\n",
      "Reading Central Galaxy file: tng-data//tmp/TNG50-1-67-mask-central.npy\n",
      "Central Galaxies: (6244619,) [ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "def expand_mask_from_list( true_list, snapnum ):    \n",
    "    n_subhalo = il.groupcat.loadHeader( args.simDir, snapnum)['Nsubgroups_Total']\n",
    "    mask = np.full( n_subhalo, False, dtype=bool )\n",
    "    mask[true_list] = True    \n",
    "    return mask\n",
    "    \n",
    "\n",
    "def getCentralFilter( args, snapnum = 99 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/tmp/{args.simName}-{snapnum}-mask-central.npy'\n",
    "\n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Central Galaxy file: {mLoc}\")\n",
    "        mask_central = np.load( mLoc )\n",
    "        return mask_central\n",
    "\n",
    "    print(f\"Getting Central SubHalo IDs for sim/snapshot: {args.simName} / {snapnum}\")\n",
    "\n",
    "    # The GroupFirstSub is the subhalo id for the largest subhalo in a halo.  \n",
    "    GroupFirstSub = il.groupcat.loadHalos( args.simDir, snapnum, fields=['GroupFirstSub'])\n",
    "\n",
    "    # Filter out groups that contain no subhalos.\n",
    "    w = np.where(GroupFirstSub >= 0) # value of -1 indicates no subhalo in this group\n",
    "    central_ids = GroupFirstSub[w]\n",
    "    \n",
    "    # Expand into a full array with a value for every subhalo\n",
    "    mask_central = expand_mask_from_list( central_ids, snapnum = snapnum )\n",
    "    \n",
    "    # Save mass\n",
    "    np.save( mLoc, mask_central )\n",
    "    \n",
    "    return mask_central\n",
    "\n",
    "if buildEnv and True: \n",
    "\n",
    "    args.overwrite = False\n",
    "    mask_central = getCentralFilter( args, snapnum = 50 )\n",
    "    print('Central Galaxies:', mask_central.shape, mask_central[:10] )\n",
    "    mask_central = getCentralFilter( args, snapnum = 67 )\n",
    "    print('Central Galaxies:', mask_central.shape, mask_central[:10] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fcaa5f8-59c9-4fdb-ad69-02bf02b67e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BoxSize': 35000.0, 'FlagDoubleprecision': 0, 'Git_commit': b'd203ec8b07c7e2bdda5f608aa0babea46d603699', 'Git_date': b'Thu Apr 7 14:14:27 2016 +0200', 'HubbleParam': 0.6774, 'Ngroups_ThisFile': 1, 'Ngroups_Total': 10638943, 'Nids_ThisFile': 11555101, 'Nids_Total': 8283281991, 'Nsubgroups_ThisFile': 54051, 'Nsubgroups_Total': 6244619, 'NumFiles': 680, 'Omega0': 0.3089, 'OmegaLambda': 0.6911, 'Redshift': 0.5030475232448832, 'Time': 0.6653149581332802}\n",
      "6244619\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "if buildEnv:\n",
    "    tmp2 = il.groupcat.loadHeader(args.simDir, 67)\n",
    "    print( tmp2 )\n",
    "    print( tmp2['Nsubgroups_Total'] )\n",
    "    print( type(tmp2['Nsubgroups_Total'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ba957-ce5c-42e0-8e05-3b998a5e1c77",
   "metadata": {},
   "source": [
    "# (r) Galaxy Morphologies (Deep Learning)\n",
    "\n",
    "Because our method relies on disks of galaxies, it might be useful for us to find mergers betweeen two disk galaxies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27267df1-15b8-4654-bfc3-b233a1821373",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_25': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_29': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_33': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_40': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_50': {'contents': {}, 'type': 'Group'},\n",
      " 'Snapshot_67': {'contents': {}, 'type': 'Group'}}\n"
     ]
    }
   ],
   "source": [
    "def getDiskMorphologyFilter( args, snapNum = 99 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/tmp/{args.simName}-{snapNum}-mask-disk-morphology.npy'\n",
    "\n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Disk Morphology Mask: {mLoc}\")\n",
    "        mask_disk = np.load( mLoc )\n",
    "        return mask_disk\n",
    "    \n",
    "    # Check if morphology file exists.  \n",
    "    hdf5Loc = f'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5'\n",
    "    if not os.path.exists( hdf5Loc ):\n",
    "        print(\"WARNING:!  Subcatalog File missing: \", hdf5Loc )\n",
    "        print(\"Please see IllustrisTNG Data Specs for info to download this file.\")\n",
    "        raise AssertionError\n",
    "    \n",
    "    # Read the deeplearning morphology file\n",
    "    with h5py.File(f'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5', 'r') as file:\n",
    "        \n",
    "        header = f'Snapshot_{snapNum}'\n",
    "        \n",
    "        # Verify snapshot header is in file\n",
    "        if header not in file.keys():\n",
    "            print(f\"Bad HDF5 header: {header} / {file.keys()}\" )\n",
    "            return None       \n",
    "        \n",
    "        subhaloIDs      = np.array( file[header]['SubhaloID'] )\n",
    "        subhaloDiskProb = np.array( file[header]['P_Disk'] )\n",
    "        \n",
    "        print( 'test morph: ', subhaloIDs.shape )\n",
    "        \n",
    "        # Iterate through and grab subhalos with a greater chance of being a disk galaxy\n",
    "        disk_list = []\n",
    "        for i in range( subhaloIDs.shape[0] ):\n",
    "            if subhaloDiskProb[i] > 0.5:\n",
    "                disk_list.append( subhaloIDs[i] )\n",
    "        \n",
    "    # Done reading file.\n",
    "    \n",
    "    # create mask \n",
    "    mask_disk = expand_mask_from_list( np.array( disk_list ), snapNum )\n",
    "    \n",
    "    # Save mass\n",
    "    print(f\"Saving Disk Morphology Mask: {mLoc}\")\n",
    "    np.save( mLoc, mask_disk )\n",
    "    \n",
    "    return mask_disk    \n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    tf.explore_hdf5( 'subcatalogs/TNG50-1-morphologies_deeplearn.hdf5', max_depth = 0 )\n",
    "    # args.overwrite=False\n",
    "    # mask_disk = getDiskMorphologyFilter( args, 67 )  \n",
    "    # print('Disk Galaxies:', mask_disk.shape, mask_disk[:10] )\n",
    "    # mask_disk = getDiskMorphologyFilter( args, 50 )  \n",
    "    # print('Disk Galaxies:', mask_disk.shape, mask_disk[:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df6eee-f1ae-461b-93a1-18bb8fd7cb1c",
   "metadata": {},
   "source": [
    "## (y) Merger History\n",
    "\n",
    "Because manually detecting major mergers in the merger tree is messy (trust me, I tried), I'll be using someone else's subcatalogs to detect major mergers between galaxies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea33f1ae-0c63-4884-9707-dff96eb333db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Upcoming Major Merger Mask: tng-data//tmp/TNG50-1-50-mask-major-merger-18.npy\n",
      "Disk Galaxies: (6780233,) [ True False False False False False False False  True False]\n",
      "Reading Upcoming Major Merger Mask: tng-data//tmp/TNG50-1-67-mask-major-merger-33.npy\n",
      "Disk Galaxies: (6244619,) [ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getMajorMergerMask( args, snapNum = 67, snapCutoff=13 ):\n",
    "    \n",
    "    mLoc = f'{args.dataDir}/tmp/{args.simName}-{snapNum}-mask-major-merger-{snapCutoff}.npy'\n",
    "    \n",
    "    # If already obtained, read from file\n",
    "    if os.path.exists( mLoc ) and not args.overwrite:\n",
    "        print(f\"Reading Upcoming Major Merger Mask: {mLoc}\")\n",
    "        mask_merger = np.load( mLoc )\n",
    "        return mask_merger\n",
    "    \n",
    "    file_loc = f'subcatalogs/MergerHistory_0{snapNum}.hdf5'\n",
    "    \n",
    "    print( f\"Merger History Loc: {file_loc}\" )\n",
    "    \n",
    "    # Return None if no file found for snap num.\n",
    "    if not os.path.exists( file_loc ):\n",
    "        print(f\"WARNING:  Could not find file: {file_loc}\")\n",
    "        raise ValueError(f\"Subcatalog File Missing: {file_loc}\")\n",
    "        return None\n",
    "    \n",
    "    # Read Merger History file.    \n",
    "    with h5py.File(file_loc, 'r') as file:\n",
    "            \n",
    "        # Get the object (could be a group or dataset)\n",
    "        dataset = file['SnapNumNextMajorMerger']\n",
    "        \n",
    "        # Create a boolean mask for values that are non-negative and below the upper limit\n",
    "        mask_merger = (dataset[:] >= 0) & (dataset[:] <= (snapNum + snapCutoff) )\n",
    "        \n",
    "    # Saving\n",
    "    print(f\"Saving Major Merger Mask: {mLoc}\")\n",
    "    np.save( mLoc, mask_merger )\n",
    "       \n",
    "    return mask_merger\n",
    "            \n",
    "    # Find merger\n",
    "    \n",
    "# Get merger tree catalog\n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    # print_HDF5_info( f'subcatalog/MergerHistory_0{snapNum}.hdf5' )\n",
    "    \n",
    "    mask_merger = getMajorMergerMask( args, 50, 18 )\n",
    "    print('Disk Galaxies:', mask_merger.shape, mask_merger[:10] )\n",
    "    mask_merger = getMajorMergerMask( args, 67, 33 )\n",
    "    \n",
    "    print('Disk Galaxies:', mask_merger.shape, mask_merger[:10] )\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd322f-6dca-456a-8ddc-5c672d5add91",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Combine Masks and Generate MOI_1 IDs\n",
    "\n",
    "(Matt from the future here) \n",
    "This is step one of many to narrow down potential mergers so I'm going to establish some terminology as I'm rewriting this.  potential mergers will hence forth be furthered as Mergers-of-Interest or MOI for short.  And each step will have a number, so this will be moi_1.  \n",
    "\n",
    "### MOI_1: Parent galaxy that will collide soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b4e295-6293-4539-b2f2-9211ffd4980d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Mass Mask: tng-data//tmp/TNG50-1-25-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data//tmp/TNG50-1-25-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data//tmp/TNG50-1-25-mask-major-merger-4.npy\n",
      "Reading Mass Mask: tng-data//tmp/TNG50-1-29-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data//tmp/TNG50-1-29-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data//tmp/TNG50-1-29-mask-major-merger-4.npy\n",
      "Reading Mass Mask: tng-data//tmp/TNG50-1-33-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data//tmp/TNG50-1-33-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data//tmp/TNG50-1-33-mask-major-merger-7.npy\n",
      "Reading Mass Mask: tng-data//tmp/TNG50-1-40-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data//tmp/TNG50-1-40-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data//tmp/TNG50-1-40-mask-major-merger-15.npy\n",
      "Reading Mass Mask: tng-data//tmp/TNG50-1-50-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data//tmp/TNG50-1-50-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data//tmp/TNG50-1-50-mask-major-merger-17.npy\n",
      "Reading Mass Mask: tng-data//tmp/TNG50-1-67-mask-mass-10.npy\n",
      "Reading Disk Morphology Mask: tng-data//tmp/TNG50-1-67-mask-disk-morphology.npy\n",
      "Reading Upcoming Major Merger Mask: tng-data//tmp/TNG50-1-67-mask-major-merger-32.npy\n",
      "done\n",
      "MOI_1 Found:  788\n"
     ]
    }
   ],
   "source": [
    "def combine_masks(mask_list):\n",
    "    \n",
    "    # Verify that all masks have the same shape\n",
    "    if not all(mask.shape == mask_list[0].shape for mask in mask_list):\n",
    "        print(\"ERROR:  Masks do not have the same shape.\")\n",
    "        for mask in mask_list:\n",
    "            print( len(mask) )\n",
    "        raise ValueError(\"ERROR: Combine Masks: All masks must have the same shape\")\n",
    "\n",
    "    # Initialize the combined mask with the first mask\n",
    "    combined_mask = mask_list[0].copy()\n",
    "\n",
    "    # Perform logical AND operation with each subsequent mask\n",
    "    for mask in mask_list[1:]:\n",
    "        combined_mask &= mask\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "def generate_moi_1_ids( args, snapNum, mass = True, massScale = 10, central = False, disk = True, major = True, majorCutoff = 13 ):\n",
    "    \n",
    "    # Create list of mask to find goi\n",
    "    mask_list = []\n",
    "    if mass:     mask_list.append( getMassFilter           ( args, snapNum, mScale = massScale ) )\n",
    "    if central:  mask_list.append( getCentralFilter        ( args, snapNum ) )\n",
    "    if disk:     mask_list.append( getDiskMorphologyFilter ( args, snapNum ) )\n",
    "    if major:    mask_list.append( getMajorMergerMask      ( args, snapNum, majorCutoff ) )\n",
    "\n",
    "    try:\n",
    "        # Get \n",
    "        combined_mask = combine_masks( mask_list )\n",
    "        subhalo_ids = np.where( combined_mask )[0] \n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    # Loop through subhalo ids and combine with snapnum to create SubhaloIDRaw\n",
    "    moi_1_ids = []\n",
    "    \n",
    "    for mid in subhalo_ids:\n",
    "        moi_1_ids.append( tf.generate_subhalo_id_raw( snapNum, mid, )  )\n",
    "    \n",
    "    moi_1_ids = np.array( moi_1_ids )\n",
    "    \n",
    "    return moi_1_ids\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "if buildEnv and True:\n",
    "    \n",
    "\n",
    "    # Starting snaps and their ranges for looking\n",
    "    #  (based on merger history hdf5 file)\n",
    "    snap_info = { \n",
    "        25: {'start_snap':25, 'stop_snap':29}, \n",
    "        29: {'start_snap':29, 'stop_snap':33}, \n",
    "        33: {'start_snap':33, 'stop_snap':40},\n",
    "        40: {'start_snap':40, 'stop_snap':55}, \n",
    "        50: {'start_snap':50, 'stop_snap':67}, \n",
    "        67: {'start_snap':67, 'stop_snap':99}\n",
    "    }\n",
    "    \n",
    "    moi_1_ids = []\n",
    "    args.overwrite = False\n",
    "        \n",
    "    for snap in snap_info:\n",
    "\n",
    "        # Initial GOIs of interest\n",
    "        snap_info[snap]['moi_1'] = generate_moi_1_ids( args, snap, majorCutoff = snap_info[snap]['stop_snap']- snap_info[snap]['start_snap']) \n",
    "    \n",
    "    c = 0\n",
    "    for s in snap_info:\n",
    "        c += len( snap_info[s]['moi_1'] )\n",
    "    \n",
    "    print(\"done\")\n",
    "    print( \"MOI_1 Found: \", c )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719a15d-e5a7-4e4d-b238-0424c4002e11",
   "metadata": {},
   "source": [
    "--- \n",
    "## Find MOI_2 in Future Merger Trees\n",
    "\n",
    "Since requesting a merger tree only returns it's tree for the current moment and backwards in time, I need to jump several snapshots forward, and identify which galaxies it belongs to there.  It's a long tedious process but I'll figure it out.\n",
    "\n",
    "### MOI_2:  Child galaxy that has already undergone a major merger event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1d2632-d8e0-4a1c-9f7a-e02ebd186840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snap 25 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Moi_2 results: tng-data//tmp/TNG50-1-moi_2-29.npy\n",
      "Snap 29 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Moi_2 results: tng-data//tmp/TNG50-1-moi_2-33.npy\n",
      "Snap 33 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Moi_2 results: tng-data//tmp/TNG50-1-moi_2-40.npy\n",
      "Snap 40 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Moi_2 results: tng-data//tmp/TNG50-1-moi_2-55.npy\n",
      "Snap 50 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Moi_2 results: tng-data//tmp/TNG50-1-moi_2-67.npy\n",
      "Snap 67 of dict_keys([25, 29, 33, 40, 50, 67])\n",
      "Reading Moi_2 results: tng-data//tmp/TNG50-1-moi_2-99.npy\n",
      "<class 'dict'>\n",
      "[[99000000000002 67000000285388]\n",
      " [99000000000019 67000000521654]\n",
      " [99000000096765 67000000424745]\n",
      " [99000000117255 67000000365040]\n",
      " [99000000117255 67000000365042]\n",
      " [99000000117263 67000000506299]\n",
      " [99000000143881 67000000261417]\n",
      " [99000000143885 67000000153307]\n",
      " [99000000143886 67000000153309]\n",
      " [99000000143886 67000000153313]]\n"
     ]
    }
   ],
   "source": [
    "def find_mois_in_tree( tree_id_raw, moi_1_list, args, ):\n",
    "    \n",
    "    tree_snap, tree_id = tf.deconstruct_subhalo_id_raw( tree_id_raw )\n",
    "\n",
    "    # Load only SubhaloIDRaw for effecient retrieval time\n",
    "    tree = il.sublink.loadTree( args.simDir, tree_snap, tree_id, fields=['SubhaloIDRaw'] )\n",
    "\n",
    "    # See if any of my MOIs are in list\n",
    "    moi_1_mask = np.isin( moi_1_list, tree )\n",
    "    n_matches = np.sum(moi_1_mask) # Presuming True = 1 and False = 0\n",
    "\n",
    "    # If none found\n",
    "    if n_matches == 0:\n",
    "        return []\n",
    "    # Else we have results\n",
    "\n",
    "    # Get the index locations where the mask is True\n",
    "    moi_1_loc = np.where(moi_1_mask)[0]\n",
    "    moi_1_ids_in_tree = moi_1_list[ moi_1_loc ]\n",
    "\n",
    "    moi_2_list = []\n",
    "\n",
    "    for moi_1 in moi_1_ids_in_tree:\n",
    "        moi_2_list.append( ( tree_id_raw, moi_1 ) )\n",
    "\n",
    "    return moi_2_list\n",
    "\n",
    "\n",
    "def getMOI_2( args, snap_info, func_overwrite = False):\n",
    "    \n",
    "    # Loop through snaps of interest, collect moi_2 value\n",
    "    \n",
    "    for snap in snap_info:\n",
    "        \n",
    "        print(f\"Snap {snap} of {snap_info.keys()}\")\n",
    "        stop = snap_info[snap]['stop_snap']\n",
    "        moi_1_list = snap_info[snap]['moi_1']\n",
    "        \n",
    "            \n",
    "        # Was tree already exist on disk?  (It takes a while to load 1000s)\n",
    "        m2Loc = f'{args.dataDir}/tmp/{args.simName}-moi_2-{stop}.npy'\n",
    "\n",
    "        # If already obtained, read from file\n",
    "        if os.path.exists( m2Loc ) and not args.overwrite and not func_overwrite:\n",
    "            print(f\"Reading Moi_2 results: {m2Loc}\")\n",
    "            snap_info[snap]['moi_2'] = np.load( m2Loc )\n",
    "            continue\n",
    "    \n",
    "        # Basically just get lots of galaxies above mass range in final snapshot\n",
    "        tree_id_list = generate_moi_1_ids( args, stop, \\\n",
    "                                               mass = True, massScale = 20, \\\n",
    "                                               central = False, disk = False, \\\n",
    "                                               major=False )\n",
    "        # Find moi_2 containing moi_1\n",
    "        moi_2_list = []\n",
    "        for i, tree_id_raw in enumerate(tree_id_list):  \n",
    "            tree_snap, tree_id = tf.deconstruct_subhalo_id_raw( tree_id_raw )\n",
    "            tabprint( f\" {i} / {tree_id_list.shape[0]} - {tree_id_raw} - Found: {len(moi_2_list)}\", end='\\r' )\n",
    "\n",
    "            found_ids = find_mois_in_tree( tree_id_raw, moi_1_list, args )\n",
    "            moi_2_list.extend( found_ids )\n",
    "\n",
    "        print( f\"\\nFound MOI_2 / Tree Matches: {len( moi_2_list) }\")\n",
    "        \n",
    "        np.save( m2Loc, moi_2_list )        \n",
    "        snap_info[snap]['moi_2'] = moi_2_list\n",
    "\n",
    "    return snap_info\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    args.overwrite = False\n",
    "    snap_info = getMOI_2( args, snap_info )\n",
    "    test_m2_list = snap_info[67]['moi_2'][:10]\n",
    "    \n",
    "    print( type( snap_info ) )\n",
    "    print( test_m2_list )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4edaf8-8f6e-496a-8da3-e259162e23e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Snap:  88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define print fucntion for a row\n",
    "def printRow( tree, i, fields ):\n",
    "    # if i == -1:\n",
    "    #     print(\"Invalid index\")\n",
    "    #     return\n",
    "    \n",
    "    print( \" - \".join( [ f\"{key}:{tree[key][i]}\" for key in fields ]) )\n",
    "    \n",
    "def createVisLink( subhaloIDRaw, projection = 'face', simulation='TNG50-1' ):\n",
    "        tmp = tf.deconstruct_subhalo_id_raw( subhaloIDRaw )    \n",
    "        \n",
    "        if projection == 'face':\n",
    "            link = f\"https://www.tng-project.org/api/{simulation}/snapshots/{tmp[0]}/subhalos/{tmp[1]}/vis.png?partType=stars&partField=stellarComp-jwst_f200w-jwst_f115w-jwst_f070w&size=1&method=histo&rotation=face-on&plotStyle=edged\"\n",
    "        # Else, project x,y plane\n",
    "        else:\n",
    "            link = f\"https://www.tng-project.org/api/{simulation}/snapshots/{tmp[0]}/subhalos/{tmp[1]}/vis.png?partType=stars&partField=stellarComp-jwst_f200w-jwst_f115w-jwst_f070w&size=1&method=histo&nPixels=256%2C256&axes=0%2C1&plotStyle=edged\"\n",
    "        return link\n",
    "\n",
    "def get_merger_snapshot( id_raw ):\n",
    "    \n",
    "    snap, subhalo_id = tf.deconstruct_subhalo_id_raw( id_raw )\n",
    "    \n",
    "    merger_loc = f'subcatalogs/MergerHistory_0{snap}.hdf5'\n",
    "    \n",
    "    # Return None if no file found for snap num.\n",
    "    if not os.path.exists( merger_loc ):\n",
    "        print(f\"WARNING:  Could not find file: {merger_loc}\")\n",
    "        raise ValueError(f\"Subcatalog File Missing: {merger_loc}\")\n",
    "        return None\n",
    "    \n",
    "    # Read Merger History file.    \n",
    "    with h5py.File(merger_loc, 'r') as file:\n",
    "            \n",
    "        # Get the object (could be a group or dataset)\n",
    "        dataset = file['SnapNumNextMajorMerger']\n",
    "        \n",
    "        merger_snap = dataset[subhalo_id]\n",
    "        \n",
    "    \n",
    "    return merger_snap\n",
    "\n",
    "if buildEnv:\n",
    "    tmp = get_merger_snapshot( test_m2_list[0][1] )\n",
    "    print(' Snap: ', tmp )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e231d-2782-44d5-880d-890c6afe8690",
   "metadata": {},
   "source": [
    "## MOI_3:  MOI_2 with multiple MOI_1 in their tree\n",
    "\n",
    "Given I have so many targets to choose from, let's narrow it down autmomatically again.  If a tree contains 2+ MOI_1 ids, then there's a good chance both parents of the major merger will meet our desired MOI_1 requirements.  Whereas non-repeats, will only have one parent meeting our requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e86b0193-4a8d-4c7f-8a89-2b6203a86cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "29\n",
      "33\n",
      "40\n",
      "50\n",
      "67\n",
      "These Mergers have more than 2 repeats, thus they may encounter multiple mergers in the timeframe. Ignoring for now...\n",
      "[array([99000000300903, 67000000184179]), array([99000000300903, 67000000184180]), array([99000000300903, 67000000184182]), array([99000000300903, 67000000184183])]\n",
      "M2 Repeats: (61, 3) \n",
      "            moi_2         p_moi_1         s_moi_1\n",
      "0  29000000007534  25000000035928  25000000035929\n",
      "1  40000000066106  33000000059075  33000000059076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_moi_2_repeats( moi_list ):\n",
    "    \n",
    "    # Step 1: Extract all moi_2's from the tuples\n",
    "    moi_2_list = [mois[0] for mois in moi_list]\n",
    "\n",
    "    # Step 2: Count the occurrences of each moi_2\n",
    "    moi_2_counts = Counter(moi_2_list)\n",
    "    \n",
    "    # Let's see if any contain 3 or more....\n",
    "    moi_2_3plus = [mois for mois in moi_list if moi_2_counts[mois[0]] >= 3]\n",
    "    print( \"These Mergers have more than 2 repeats, thus they may encounter multiple mergers in the timeframe. Ignoring for now...\" )\n",
    "    print( moi_2_3plus )\n",
    "    \n",
    "    # Step 3: Combine MOI_1's into a single row.  \n",
    "    # SubhaloIDs were given based on descending mass, thus lower numbers should have more mass.  \n",
    "    # Greater mass parents will be labels as primary, and the other secondary. \n",
    "    # The following is magic written by ChatGPT.\n",
    "    \n",
    "    # Creating a DataFrame from the list of tuples\n",
    "    df = pd.DataFrame(moi_list, columns=['moi_2', 'moi_1'])\n",
    "\n",
    "    # Grouping by 'moi_2' and filtering for pairs\n",
    "    pairs = df.groupby('moi_2').filter(lambda x: len(x) == 2)\n",
    "\n",
    "    # Function to assign labels based on the smaller value\n",
    "    def assign_labels(group):\n",
    "        smaller_row = group.loc[group['moi_1'].idxmin()]\n",
    "        larger_row = group.loc[group['moi_1'].idxmax()]\n",
    "\n",
    "        return pd.Series({\n",
    "            'moi_2': smaller_row['moi_2'],\n",
    "            'p_moi_1': smaller_row['moi_1'],\n",
    "            's_moi_1': larger_row['moi_1']\n",
    "        })\n",
    "\n",
    "    # Applying the function to each group\n",
    "    moi_3 = pairs.groupby('moi_2').apply(assign_labels).reset_index(drop=True)\n",
    "        \n",
    "    return moi_3\n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    args.overwrite = False\n",
    "    \n",
    "    # Combine all moi_1/moi_2 lists\n",
    "    m2_all_list = []\n",
    "    \n",
    "    for s in snap_info:\n",
    "        print( s )\n",
    "        m2_all_list.extend( snap_info[s]['moi_2'] )\n",
    "    \n",
    "    m3_id_df = find_moi_2_repeats( m2_all_list )\n",
    "    \n",
    "    print( f\"M2 Repeats: {m3_id_df.shape} \")\n",
    "    print(m3_id_df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8becaaf2-a6f3-4726-955d-fcef85ce1032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Tree:  (15, 38)\n",
      "<class 'numpy.ndarray'>\n",
      "            moi_2  snap  merger_snap         p_moi_1         s_moi_1  \\\n",
      "0  29000000007534    24           28  25000000035928  25000000035929   \n",
      "1  29000000007534    23           28  25000000035928  25000000035929   \n",
      "\n",
      "                                       xy_projection  \\\n",
      "0  https://www.tng-project.org/api/TNG50-1/snapsh...   \n",
      "1  https://www.tng-project.org/api/TNG50-1/snapsh...   \n",
      "\n",
      "                                   p_face_projection  \\\n",
      "0  https://www.tng-project.org/api/TNG50-1/snapsh...   \n",
      "1  https://www.tng-project.org/api/TNG50-1/snapsh...   \n",
      "\n",
      "                                   s_face_projection  p_SubhaloID  \\\n",
      "0  https://www.tng-project.org/api/TNG50-1/snapsh...     21001277   \n",
      "1  https://www.tng-project.org/api/TNG50-1/snapsh...     21001278   \n",
      "\n",
      "   s_SubhaloID  ...                      p_SubhaloPos  \\\n",
      "0     21033413  ...   [21505.63, 15993.273, 2533.538]   \n",
      "1     21033414  ...  [21423.52, 16020.759, 2424.4734]   \n",
      "\n",
      "                        s_SubhaloPos                       p_SubhaloVel  \\\n",
      "0  [21393.527, 16187.839, 3019.1362]   [187.62364, -69.7606, 283.31952]   \n",
      "1   [21292.72, 16272.698, 3014.4285]  [190.01091, -63.87294, 285.05435]   \n",
      "\n",
      "                         s_SubhaloVel                        p_SubhaloSpin  \\\n",
      "0  [287.81644, -189.27324, 38.480217]  [395.68765, 400.23273, -121.345474]   \n",
      "1  [263.34476, -191.15932, 40.662643]   [311.85614, 79.679375, -125.07007]   \n",
      "\n",
      "                        s_SubhaloSpin  p_SubhaloHalfmassRad  \\\n",
      "0  [500.30865, -473.30206, -144.1621]             98.003609   \n",
      "1  [559.80145, -359.9568, -152.19844]             70.214867   \n",
      "\n",
      "   s_SubhaloHalfmassRad                        p_SubhaloCM  \\\n",
      "0             72.694550  [21536.832, 15989.496, 2549.0603]   \n",
      "1             86.093391  [21423.479, 16018.679, 2415.2688]   \n",
      "\n",
      "                        s_SubhaloCM  \n",
      "0  [21384.266, 16200.33, 3004.1829]  \n",
      "1  [21279.965, 16267.33, 2990.4094]  \n",
      "\n",
      "[2 rows x 38 columns]\n",
      "Loading Trees: 60 of 61\n",
      "Loading Trees Complete: 61\n",
      "\n",
      "Final MOI 3 DataFrame\n",
      "(1537, 38)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This loads the MOI_2 Tree for moi_3's.\n",
    "def travel_tree( args, m3 ):\n",
    "    \n",
    "    # Rename common variables\n",
    "    m2 = m3['moi_2']\n",
    "    p_m1 = m3['p_moi_1']\n",
    "    s_m1 = m3['s_moi_1']\n",
    "    m2_snap, m2_subhalo = tf.deconstruct_subhalo_id_raw( m2 )\n",
    "    m1_snap, p_m1_subhalo = tf.deconstruct_subhalo_id_raw( p_m1 )\n",
    "    m1_snap, s_m1_subhalo = tf.deconstruct_subhalo_id_raw( s_m1 )\n",
    "    \n",
    "    # Get snapshot when moi is supposed to occur\n",
    "    merger_snap = get_merger_snapshot( p_m1 )\n",
    "        \n",
    "    # Fields to load from Tree\n",
    "    fields = ['SubhaloID','NextProgenitorID','MainLeafProgenitorID','FirstProgenitorID','SubhaloGrNr',\\\n",
    "              'SubhaloIDRaw','SubhaloMass', 'RootDescendantID', 'SnapNum', 'DescendantID',\\\n",
    "              'SubhaloPos', 'SubhaloVel', 'SubhaloSpin', 'SubhaloHalfmassRad', 'SubhaloCM', ]\n",
    "    \n",
    "    # NOTE: Descendent ID is children id in future.\n",
    "    # NOTE: First ProgenitorID is ID of primary Parent from past. \n",
    "    \n",
    "    \n",
    "    # Load Merger Tree with desired fields\n",
    "    tree = il.sublink.loadTree( args.simDir, m2_snap, m2_subhalo, fields=fields)\n",
    "    \n",
    "    # Create a dictionary to map Subhalo IDs to their index in the list\n",
    "    tree_index = {subhalo_id: index for index, subhalo_id in enumerate(tree['SubhaloID'])}  \n",
    "    \n",
    "            \n",
    "    # List to store data in.\n",
    "    data_list = []\n",
    "    \n",
    "    # Define function for extracting row of tree\n",
    "    def extract_data( snap, p_id, s_id ):\n",
    "        \n",
    "        # Store values in a flat dictionary\n",
    "        flat_dict = {}\n",
    "        flat_dict['moi_2'] = m2\n",
    "        flat_dict['snap'] = snap\n",
    "        flat_dict['merger_snap'] = merger_snap\n",
    "        flat_dict['p_moi_1'] = p_m1\n",
    "        flat_dict['s_moi_1'] = s_m1\n",
    "        flat_dict['xy_projection'] = createVisLink( tree['SubhaloIDRaw'][p_id], projection = 'xy' )\n",
    "        flat_dict['p_face_projection'] = createVisLink( tree['SubhaloIDRaw'][p_id], projection = 'face' )\n",
    "        flat_dict['s_face_projection'] = createVisLink( tree['SubhaloIDRaw'][s_id], projection = 'face' )\n",
    "        \n",
    "        # Loop through tree fields and store\n",
    "        for k in fields:\n",
    "            for c, ii in [ ('p',p_id), ('s',s_id) ]:\n",
    "                flat_dict[f'{c}_{k}'] = tree[k][ii]\n",
    "        \n",
    "        return flat_dict\n",
    "    \n",
    "    # Grab index values for both moi_1.\n",
    "    p_m1_id = np.where( tree['SubhaloIDRaw'] == p_m1  )[0][0]\n",
    "    s_m1_id = np.where( tree['SubhaloIDRaw'] == s_m1  )[0][0]\n",
    "    \n",
    "    # Go backwards a few snapshots first just in case of immidiate merger\n",
    "    back_snaps = 10\n",
    "    snap = m1_snap\n",
    "    p_id = p_m1_id\n",
    "    s_id = s_m1_id\n",
    "        \n",
    "    while snap >= m1_snap - back_snaps:\n",
    "        \n",
    "        # Values needed for other fields\n",
    "        snap -= 1\n",
    "        p_id = tree_index[tree['FirstProgenitorID'][p_id]]\n",
    "        s_id = tree_index[tree['FirstProgenitorID'][s_id]]\n",
    "        \n",
    "        # Verify the snaps all match\n",
    "        assert snap == tree['SnapNum'][p_id]\n",
    "        assert snap == tree['SnapNum'][s_id]\n",
    "        \n",
    "        data_list.append( extract_data( snap, p_id, s_id ) )\n",
    "        \n",
    "    # Now go forwards in time to grab info\n",
    "    snap = m1_snap\n",
    "    p_id = p_m1_id\n",
    "    s_id = s_m1_id\n",
    "    \n",
    "    # Loop until primary and secondary merger, or you've reached the origin of the tree.\n",
    "    while (p_id != s_id) and snap < m2_snap:\n",
    "        \n",
    "        # Add data of interest\n",
    "        data_list.append( extract_data( snap, p_id, s_id ) )\n",
    "        \n",
    "        # Iterate forwards in time\n",
    "        snap += 1\n",
    "        p_id = tree_index[tree['DescendantID'][p_id]]\n",
    "        s_id = tree_index[tree['DescendantID'][s_id]]\n",
    "        \n",
    "        # Verify the snaps all match\n",
    "        if snap != tree['SnapNum'][p_id]: break\n",
    "        if snap != tree['SnapNum'][s_id]: break\n",
    "        \n",
    "    # Convert to dataframe and return    \n",
    "    return pd.DataFrame(data_list)\n",
    "        \n",
    "\n",
    "def create_moi_3_file( args, m3_id_df, file_name_loc = 'tng-data/moi_3', func_overwrite=False ):\n",
    "        \n",
    "    # If file exists, read and return.\n",
    "    if os.path.exists( file_name_loc + '.pkl' ) and args.overwrite == False and func_overwrite == False:\n",
    "        m3_df = pd.read_pickle( file_name_loc + '.pkl' )\n",
    "        return m3_df\n",
    "    \n",
    "    # Else, create file by getting info via merger trees.\n",
    "    m3_df_list = []\n",
    "    for i, row in m3_id_df.iterrows():\n",
    "        \n",
    "        print(f\"Loading Trees: {i} of {m3_id_df.shape[0]}\", end='\\r')\n",
    "        m3_df_list.append( travel_tree(args, row) )\n",
    "        \n",
    "        # if i > 2: break\n",
    "    \n",
    "    print(f\"\\nLoading Trees Complete: {m3_id_df.shape[0]}\" )\n",
    "        \n",
    "    print('')\n",
    "    print(\"Final MOI 3 DataFrame\")\n",
    "    # Combine all the dataframes\n",
    "    m3_df = pd.concat( m3_df_list, ignore_index=True )\n",
    "    \n",
    "    # Sort by Moi_2, then by snap\n",
    "    m3_df = m3_df.sort_values(by=['moi_2', 'snap'], ascending=[True, True])\n",
    "    \n",
    "    m3_df.to_csv( file_name_loc + '.csv', index=False )\n",
    "    m3_df.to_pickle( file_name_loc + '.pkl' )\n",
    "    \n",
    "    return m3_df\n",
    "\n",
    "\n",
    "if buildEnv and True:\n",
    "    \n",
    "    args.overwrite = False\n",
    "    \n",
    "    ex_df = travel_tree( args, m3_id_df.loc[0] )\n",
    "    # m3_df = save_moi_info( args, m2_repeats, 'tng-data/moi-2-repeats.csv' )\n",
    "    print( 'Example Tree: ', ex_df.shape ) \n",
    "    print( type( ex_df.loc[0]['p_SubhaloPos'] ) )\n",
    "    print( ex_df[:2] )\n",
    "    \n",
    "    m3_df = create_moi_3_file( args, m3_id_df, file_name_loc = 'tng-data/moi-3', func_overwrite=True )\n",
    "\n",
    "    print( m3_df.shape )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ccb25-f069-4e91-918b-705c3c0c3469",
   "metadata": {},
   "source": [
    "# MOI_4:   Galaxies that show hints of tidal features.\n",
    "\n",
    "While it would be nice if there was an automatic way to review these potential targets for tidal features, I need to find a handful of visually nice targets now.  So I manually opened the links to visuallize the galaxies.  Any that showed hints of tidal features, I'm copying the snapnum and subhalo id down below.\n",
    "\n",
    "NOTE to myself.  The following is only from reviewing moi_2_50_67_repeats.   More are sure to be found in newer file moi_3 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "603af899-b38d-4cbc-8119-ca8a592b4858",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54000000045004, 55000000046172, 56000000046470, 62000000187573, 63000000189019, 64000000205294, 61000000186579, 62000000190717, 63000000192216, 64000000153811, 56000000230959]\n",
      "[67000000146970 67000000184181 67000000258162 99000000229933]\n",
      "MOI_4 Test Shape: (102, 38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14268/2694747778.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  moi_4_df['moi_4'] = False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "moi_4_great = {\n",
    "    54:45004,\n",
    "    61:186579,\n",
    "    62:190717,   # AMAZING\n",
    "    63:192216,\n",
    "    64:153811, \n",
    "    62:187573,\n",
    "}\n",
    "\n",
    "moi_4_good = [\n",
    "    (54, 45004),\n",
    "    (55, 46172),\n",
    "    (56, 46470),\n",
    "    (62, 187573),\n",
    "    (63, 189019),\n",
    "    (64, 205294),\n",
    "    (61, 186579),\n",
    "    (62, 190717),   # AMAZING\n",
    "    (63, 192216),\n",
    "    (64, 153811),\n",
    "    (56, 230959),\n",
    "]\n",
    "\n",
    "\n",
    "moi_4_maybe = [\n",
    "    (58, 136813),\n",
    "    (59, 138897),\n",
    "    (56, 192189),\n",
    "    (59, 210457),\n",
    "    (62, 338866),\n",
    "    (63, 336934),\n",
    "    (64, 338830),\n",
    "    (65, 345132),\n",
    "    (55, 330164),\n",
    "    (56, 333739),\n",
    "    (57, 336505),\n",
    "]\n",
    "\n",
    "if buildEnv:\n",
    "    \n",
    "    # Let's save some of these test moi-3s somewhere.\n",
    "    m4_loc = 'tng-data/moi_4-test.pkl'\n",
    "    m4_csv_loc = m4_loc[:-4] + '.csv'\n",
    "    \n",
    "    # Identify MOI_3 id's that contain our MOI_4 id.\n",
    "    pid_list = [ tf.generate_subhalo_id_raw( snap, subhalo_id ) for snap, subhalo_id in moi_4_good ]\n",
    "    print( pid_list )\n",
    "    \n",
    "    # Extract data associated with moi_3 above.\n",
    "    moi3_ids = m3_df[ m3_df['p_SubhaloIDRaw'].isin(pid_list) ]['moi_2'].unique()\n",
    "    print( moi3_ids )\n",
    "    \n",
    "    # Get all rows with moi_4 values, thus getting all info before and after moi_4 snap\n",
    "    moi_4_df = m3_df[ m3_df['moi_2'].isin(moi3_ids) ]\n",
    "    \n",
    "    # Look for duplicates\n",
    "    print(f\"MOI_4 Test Shape: {moi_4_df.shape}\")\n",
    "    \n",
    "    # Add new columns indicating row of MOI_4 candidates\n",
    "    moi_4_df['moi_4'] = False\n",
    "    moi_4_df.loc[moi_4_df['p_SubhaloIDRaw'].isin(pid_list), 'moi_4'] = True\n",
    "    \n",
    "    moi_4_df.to_pickle( m4_loc )\n",
    "    moi_4_df.to_csv( m4_csv_loc , index=False )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414d64b-10fd-4269-aa1a-d2937e94259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a4d58-c249-4693-baaa-e1227380f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
